using device: cpu
Loading pretrained GPT-2 model: gpt2
loaded 338025 tokens
1 epoch = 165 batches
Using OneCycleLR scheduler with max_lr=0.0007 and total_steps=20000
step 0, loss: 4.9285, current_lr: 0.000028
Saved best model with loss: 4.9285 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.9285.pt
step 1, loss: 4.4606, current_lr: 0.000028
Saved best model with loss: 4.4606 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.4606.pt
step 2, loss: 4.0646, current_lr: 0.000028
Saved best model with loss: 4.0646 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.0646.pt
step 3, loss: 4.3851, current_lr: 0.000028
step 4, loss: 4.3394, current_lr: 0.000028
step 5, loss: 4.1568, current_lr: 0.000028
step 6, loss: 4.1171, current_lr: 0.000028
step 7, loss: 4.1457, current_lr: 0.000028
step 8, loss: 3.8273, current_lr: 0.000028
Saved best model with loss: 3.8273 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.8273.pt
step 9, loss: 3.9955, current_lr: 0.000028
step 10, loss: 3.9946, current_lr: 0.000028
step 11, loss: 3.5211, current_lr: 0.000028
Saved best model with loss: 3.5211 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.5211.pt
step 12, loss: 3.9006, current_lr: 0.000028
step 13, loss: 3.8258, current_lr: 0.000028
step 14, loss: 3.8449, current_lr: 0.000028
step 15, loss: 3.5076, current_lr: 0.000028
Saved best model with loss: 3.5076 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.5076.pt
step 16, loss: 3.7530, current_lr: 0.000028
step 17, loss: 3.7007, current_lr: 0.000028
step 18, loss: 3.8214, current_lr: 0.000028
step 19, loss: 3.7428, current_lr: 0.000028
step 20, loss: 4.0523, current_lr: 0.000028
step 21, loss: 3.8357, current_lr: 0.000028
step 22, loss: 3.8522, current_lr: 0.000028
step 23, loss: 3.9134, current_lr: 0.000028
step 24, loss: 3.6694, current_lr: 0.000028
step 25, loss: 3.5051, current_lr: 0.000028
Saved best model with loss: 3.5051 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.5051.pt
step 26, loss: 3.8200, current_lr: 0.000028
step 27, loss: 3.8112, current_lr: 0.000028
step 28, loss: 3.8489, current_lr: 0.000028
step 29, loss: 3.3721, current_lr: 0.000028
Saved best model with loss: 3.3721 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3721.pt
step 30, loss: 3.7661, current_lr: 0.000028
step 31, loss: 3.7904, current_lr: 0.000028
step 32, loss: 3.5456, current_lr: 0.000028
step 33, loss: 3.3806, current_lr: 0.000028
step 34, loss: 3.6107, current_lr: 0.000028
step 35, loss: 3.5519, current_lr: 0.000028
step 36, loss: 3.9313, current_lr: 0.000028
step 37, loss: 3.8784, current_lr: 0.000028
step 38, loss: 3.7909, current_lr: 0.000028
step 39, loss: 3.4273, current_lr: 0.000028
step 40, loss: 4.0313, current_lr: 0.000028
step 41, loss: 3.5450, current_lr: 0.000028
step 42, loss: 3.3735, current_lr: 0.000028
step 43, loss: 3.4538, current_lr: 0.000028
step 44, loss: 3.8303, current_lr: 0.000028
step 45, loss: 3.7364, current_lr: 0.000028
step 46, loss: 3.7734, current_lr: 0.000028
step 47, loss: 4.1469, current_lr: 0.000028
step 48, loss: 3.9950, current_lr: 0.000028
step 49, loss: 4.0320, current_lr: 0.000028
step 50, loss: 3.9914, current_lr: 0.000028
step 51, loss: 4.0522, current_lr: 0.000028
step 52, loss: 3.9408, current_lr: 0.000028
step 53, loss: 3.6930, current_lr: 0.000028
step 54, loss: 3.8109, current_lr: 0.000028
step 55, loss: 4.0477, current_lr: 0.000028
step 56, loss: 3.9560, current_lr: 0.000028
step 57, loss: 3.8179, current_lr: 0.000028
step 58, loss: 3.8375, current_lr: 0.000028
step 59, loss: 3.7855, current_lr: 0.000028
step 60, loss: 3.6592, current_lr: 0.000028
step 61, loss: 3.7050, current_lr: 0.000028
step 62, loss: 3.3509, current_lr: 0.000028
Saved best model with loss: 3.3509 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3509.pt
step 63, loss: 3.8139, current_lr: 0.000028
step 64, loss: 3.3901, current_lr: 0.000028
step 65, loss: 3.8421, current_lr: 0.000028
step 66, loss: 4.0219, current_lr: 0.000028
step 67, loss: 3.9386, current_lr: 0.000028
step 68, loss: 4.0974, current_lr: 0.000028
step 69, loss: 3.7924, current_lr: 0.000028
step 70, loss: 3.5735, current_lr: 0.000028
step 71, loss: 4.0797, current_lr: 0.000028
step 72, loss: 3.5122, current_lr: 0.000028
step 73, loss: 3.8025, current_lr: 0.000028
step 74, loss: 3.6451, current_lr: 0.000028
step 75, loss: 3.9181, current_lr: 0.000028
step 76, loss: 3.6841, current_lr: 0.000028
step 77, loss: 3.6696, current_lr: 0.000028
step 78, loss: 3.5078, current_lr: 0.000028
step 79, loss: 3.5267, current_lr: 0.000028
step 80, loss: 3.6676, current_lr: 0.000028
step 81, loss: 3.5871, current_lr: 0.000028
step 82, loss: 3.9570, current_lr: 0.000028
step 83, loss: 3.7610, current_lr: 0.000028
step 84, loss: 3.7454, current_lr: 0.000028
step 85, loss: 3.3397, current_lr: 0.000028
Saved best model with loss: 3.3397 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3397.pt
step 86, loss: 3.3253, current_lr: 0.000028
Saved best model with loss: 3.3253 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3253.pt
step 87, loss: 3.7141, current_lr: 0.000028
step 88, loss: 3.8626, current_lr: 0.000028
step 89, loss: 3.9640, current_lr: 0.000028
step 90, loss: 3.6068, current_lr: 0.000028
step 91, loss: 3.8788, current_lr: 0.000028
step 92, loss: 3.8226, current_lr: 0.000028
step 93, loss: 3.5308, current_lr: 0.000028
step 94, loss: 3.0308, current_lr: 0.000028
Saved best model with loss: 3.0308 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.0308.pt
step 95, loss: 3.6709, current_lr: 0.000028
step 96, loss: 3.6094, current_lr: 0.000028
step 97, loss: 3.7062, current_lr: 0.000028
step 98, loss: 3.5230, current_lr: 0.000028
step 99, loss: 3.4624, current_lr: 0.000028
step 100, loss: 3.4790, current_lr: 0.000028
step 101, loss: 3.7808, current_lr: 0.000028
step 102, loss: 3.3708, current_lr: 0.000028
step 103, loss: 3.6789, current_lr: 0.000028
step 104, loss: 4.1509, current_lr: 0.000029
step 105, loss: 4.2369, current_lr: 0.000029
step 106, loss: 4.1452, current_lr: 0.000029
step 107, loss: 4.0376, current_lr: 0.000029
step 108, loss: 4.1068, current_lr: 0.000029
step 109, loss: 3.9167, current_lr: 0.000029
step 110, loss: 4.1111, current_lr: 0.000029
step 111, loss: 4.1689, current_lr: 0.000029
step 112, loss: 4.1243, current_lr: 0.000029
step 113, loss: 4.2035, current_lr: 0.000029
step 114, loss: 3.9710, current_lr: 0.000029
step 115, loss: 4.1723, current_lr: 0.000029
step 116, loss: 3.7742, current_lr: 0.000029
step 117, loss: 3.9851, current_lr: 0.000029
step 118, loss: 3.8877, current_lr: 0.000029
step 119, loss: 3.7701, current_lr: 0.000029
step 120, loss: 3.9832, current_lr: 0.000029
step 121, loss: 4.0545, current_lr: 0.000029
step 122, loss: 3.8928, current_lr: 0.000029
step 123, loss: 3.9911, current_lr: 0.000029
step 124, loss: 3.4634, current_lr: 0.000029
step 125, loss: 3.9610, current_lr: 0.000029
step 126, loss: 3.6393, current_lr: 0.000029
step 127, loss: 3.2794, current_lr: 0.000029
step 128, loss: 3.4800, current_lr: 0.000029
step 129, loss: 3.6198, current_lr: 0.000029
step 130, loss: 3.7376, current_lr: 0.000029
step 131, loss: 3.8231, current_lr: 0.000029
step 132, loss: 3.7869, current_lr: 0.000029
step 133, loss: 3.4127, current_lr: 0.000029
step 134, loss: 3.9168, current_lr: 0.000029
step 135, loss: 3.4220, current_lr: 0.000029
step 136, loss: 3.5626, current_lr: 0.000029
step 137, loss: 3.6122, current_lr: 0.000029
step 138, loss: 3.4596, current_lr: 0.000029
step 139, loss: 3.4893, current_lr: 0.000029
step 140, loss: 3.4452, current_lr: 0.000029
step 141, loss: 3.5974, current_lr: 0.000029
step 142, loss: 4.0493, current_lr: 0.000029
step 143, loss: 3.9201, current_lr: 0.000029
step 144, loss: 3.8923, current_lr: 0.000029
step 145, loss: 3.9007, current_lr: 0.000029
step 146, loss: 3.6738, current_lr: 0.000029
step 147, loss: 3.5985, current_lr: 0.000029
step 148, loss: 3.2442, current_lr: 0.000029
step 149, loss: 3.7863, current_lr: 0.000029
step 150, loss: 3.7987, current_lr: 0.000029
step 151, loss: 3.6950, current_lr: 0.000029
step 152, loss: 3.4631, current_lr: 0.000029
step 153, loss: 3.5591, current_lr: 0.000029
step 154, loss: 3.3320, current_lr: 0.000029
step 155, loss: 3.3373, current_lr: 0.000029
step 156, loss: 3.4074, current_lr: 0.000029
step 157, loss: 3.2863, current_lr: 0.000029
step 158, loss: 3.0607, current_lr: 0.000029
step 159, loss: 3.6223, current_lr: 0.000029
step 160, loss: 3.9134, current_lr: 0.000029
step 161, loss: 3.9584, current_lr: 0.000029
step 162, loss: 4.0652, current_lr: 0.000029
step 163, loss: 3.4931, current_lr: 0.000029
step 164, loss: 3.4717, current_lr: 0.000029
step 165, loss: 3.7151, current_lr: 0.000029
step 166, loss: 3.6518, current_lr: 0.000029
step 167, loss: 3.3791, current_lr: 0.000029
step 168, loss: 3.7161, current_lr: 0.000029
step 169, loss: 3.7333, current_lr: 0.000029
step 170, loss: 3.5829, current_lr: 0.000029
step 171, loss: 3.5607, current_lr: 0.000029
step 172, loss: 3.6760, current_lr: 0.000029
step 173, loss: 3.3784, current_lr: 0.000029
step 174, loss: 3.5214, current_lr: 0.000029
step 175, loss: 3.5144, current_lr: 0.000029
step 176, loss: 3.0656, current_lr: 0.000029
step 177, loss: 3.4792, current_lr: 0.000029
step 178, loss: 3.3972, current_lr: 0.000029
step 179, loss: 3.4573, current_lr: 0.000029
step 180, loss: 3.1520, current_lr: 0.000030
step 181, loss: 3.3793, current_lr: 0.000030
step 182, loss: 3.3249, current_lr: 0.000030
step 183, loss: 3.5107, current_lr: 0.000030
step 184, loss: 3.4595, current_lr: 0.000030
step 185, loss: 3.7230, current_lr: 0.000030
step 186, loss: 3.5211, current_lr: 0.000030
step 187, loss: 3.4830, current_lr: 0.000030
step 188, loss: 3.5540, current_lr: 0.000030
step 189, loss: 3.3248, current_lr: 0.000030
step 190, loss: 3.2000, current_lr: 0.000030
step 191, loss: 3.3983, current_lr: 0.000030
step 192, loss: 3.4356, current_lr: 0.000030
step 193, loss: 3.5009, current_lr: 0.000030
step 194, loss: 3.1138, current_lr: 0.000030
step 195, loss: 3.4543, current_lr: 0.000030
step 196, loss: 3.4298, current_lr: 0.000030
step 197, loss: 3.1542, current_lr: 0.000030
step 198, loss: 3.0556, current_lr: 0.000030
step 199, loss: 3.2827, current_lr: 0.000030
step 200, loss: 3.2286, current_lr: 0.000030
step 201, loss: 3.6327, current_lr: 0.000030
step 202, loss: 3.6031, current_lr: 0.000030
step 203, loss: 3.4705, current_lr: 0.000030
step 204, loss: 3.1088, current_lr: 0.000030
step 205, loss: 3.6862, current_lr: 0.000030
step 206, loss: 3.2103, current_lr: 0.000030
step 207, loss: 3.1044, current_lr: 0.000030
step 208, loss: 3.1615, current_lr: 0.000030
step 209, loss: 3.5486, current_lr: 0.000030
step 210, loss: 3.4628, current_lr: 0.000030
step 211, loss: 3.5028, current_lr: 0.000030
step 212, loss: 3.8474, current_lr: 0.000030
step 213, loss: 3.6808, current_lr: 0.000030
step 214, loss: 3.7366, current_lr: 0.000030
step 215, loss: 3.6919, current_lr: 0.000030
step 216, loss: 3.7983, current_lr: 0.000030
step 217, loss: 3.6638, current_lr: 0.000030
step 218, loss: 3.4079, current_lr: 0.000030
step 219, loss: 3.4575, current_lr: 0.000030
step 220, loss: 3.7876, current_lr: 0.000030
step 221, loss: 3.6701, current_lr: 0.000030
step 222, loss: 3.5741, current_lr: 0.000030
step 223, loss: 3.5837, current_lr: 0.000030
step 224, loss: 3.5088, current_lr: 0.000030
step 225, loss: 3.4193, current_lr: 0.000030
step 226, loss: 3.4573, current_lr: 0.000030
step 227, loss: 3.0041, current_lr: 0.000030
Saved best model with loss: 3.0041 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.0041.pt
step 228, loss: 3.5580, current_lr: 0.000030
step 229, loss: 3.1808, current_lr: 0.000030
step 230, loss: 3.4951, current_lr: 0.000030
step 231, loss: 3.6310, current_lr: 0.000030
step 232, loss: 3.5949, current_lr: 0.000030
step 233, loss: 3.7570, current_lr: 0.000031
step 234, loss: 3.4768, current_lr: 0.000031
step 235, loss: 3.3025, current_lr: 0.000031
step 236, loss: 3.7347, current_lr: 0.000031
step 237, loss: 3.2402, current_lr: 0.000031
step 238, loss: 3.4950, current_lr: 0.000031
step 239, loss: 3.3224, current_lr: 0.000031
step 240, loss: 3.6541, current_lr: 0.000031
step 241, loss: 3.3832, current_lr: 0.000031
step 242, loss: 3.4112, current_lr: 0.000031
step 243, loss: 3.2498, current_lr: 0.000031
step 244, loss: 3.2789, current_lr: 0.000031
step 245, loss: 3.3887, current_lr: 0.000031
step 246, loss: 3.3468, current_lr: 0.000031
step 247, loss: 3.7114, current_lr: 0.000031
step 248, loss: 3.5003, current_lr: 0.000031
step 249, loss: 3.5112, current_lr: 0.000031
step 250, loss: 3.0511, current_lr: 0.000031
step 251, loss: 3.0974, current_lr: 0.000031
step 252, loss: 3.4927, current_lr: 0.000031
step 253, loss: 3.6562, current_lr: 0.000031
step 254, loss: 3.7405, current_lr: 0.000031
step 255, loss: 3.3600, current_lr: 0.000031
step 256, loss: 3.6625, current_lr: 0.000031
step 257, loss: 3.5987, current_lr: 0.000031
step 258, loss: 3.2865, current_lr: 0.000031
step 259, loss: 2.8390, current_lr: 0.000031
Saved best model with loss: 2.8390 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.8390.pt
step 260, loss: 3.4338, current_lr: 0.000031
step 261, loss: 3.3609, current_lr: 0.000031
step 262, loss: 3.4728, current_lr: 0.000031
step 263, loss: 3.3094, current_lr: 0.000031
step 264, loss: 3.2776, current_lr: 0.000031
step 265, loss: 3.2599, current_lr: 0.000031
step 266, loss: 3.5442, current_lr: 0.000031
step 267, loss: 3.1772, current_lr: 0.000031
step 268, loss: 3.4784, current_lr: 0.000031
step 269, loss: 3.8124, current_lr: 0.000031
step 270, loss: 3.9361, current_lr: 0.000031
step 271, loss: 3.8076, current_lr: 0.000031
step 272, loss: 3.7603, current_lr: 0.000031
step 273, loss: 3.8445, current_lr: 0.000031
step 274, loss: 3.6293, current_lr: 0.000031
step 275, loss: 3.8654, current_lr: 0.000032
step 276, loss: 3.9015, current_lr: 0.000032
step 277, loss: 3.9172, current_lr: 0.000032
step 278, loss: 3.9440, current_lr: 0.000032
step 279, loss: 3.6699, current_lr: 0.000032
step 280, loss: 3.9159, current_lr: 0.000032
step 281, loss: 3.4856, current_lr: 0.000032
step 282, loss: 3.7094, current_lr: 0.000032
step 283, loss: 3.5920, current_lr: 0.000032
step 284, loss: 3.5019, current_lr: 0.000032
step 285, loss: 3.7581, current_lr: 0.000032
step 286, loss: 3.8057, current_lr: 0.000032
step 287, loss: 3.6606, current_lr: 0.000032
step 288, loss: 3.7948, current_lr: 0.000032
step 289, loss: 3.2404, current_lr: 0.000032
step 290, loss: 3.7980, current_lr: 0.000032
step 291, loss: 3.4092, current_lr: 0.000032
step 292, loss: 3.0056, current_lr: 0.000032
step 293, loss: 3.2416, current_lr: 0.000032
step 294, loss: 3.4371, current_lr: 0.000032
step 295, loss: 3.5366, current_lr: 0.000032
step 296, loss: 3.6186, current_lr: 0.000032
step 297, loss: 3.5469, current_lr: 0.000032
step 298, loss: 3.1781, current_lr: 0.000032
step 299, loss: 3.6737, current_lr: 0.000032
step 300, loss: 3.1883, current_lr: 0.000032
step 301, loss: 3.3454, current_lr: 0.000032
step 302, loss: 3.4131, current_lr: 0.000032
step 303, loss: 3.2771, current_lr: 0.000032
step 304, loss: 3.3135, current_lr: 0.000032
step 305, loss: 3.2441, current_lr: 0.000032
step 306, loss: 3.3956, current_lr: 0.000032
step 307, loss: 3.8725, current_lr: 0.000032
step 308, loss: 3.6784, current_lr: 0.000032
step 309, loss: 3.5748, current_lr: 0.000032
step 310, loss: 3.5173, current_lr: 0.000032
step 311, loss: 3.2782, current_lr: 0.000032
step 312, loss: 3.2450, current_lr: 0.000033
step 313, loss: 2.9619, current_lr: 0.000033
step 314, loss: 3.4614, current_lr: 0.000033
step 315, loss: 3.4804, current_lr: 0.000033
step 316, loss: 3.4060, current_lr: 0.000033
step 317, loss: 3.2152, current_lr: 0.000033
step 318, loss: 3.3105, current_lr: 0.000033
step 319, loss: 3.0687, current_lr: 0.000033
step 320, loss: 3.1086, current_lr: 0.000033
step 321, loss: 3.1525, current_lr: 0.000033
step 322, loss: 2.9943, current_lr: 0.000033
step 323, loss: 2.8076, current_lr: 0.000033
Saved best model with loss: 2.8076 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.8076.pt
step 324, loss: 3.4143, current_lr: 0.000033
step 325, loss: 3.6907, current_lr: 0.000033
step 326, loss: 3.7424, current_lr: 0.000033
step 327, loss: 3.8345, current_lr: 0.000033
step 328, loss: 3.2478, current_lr: 0.000033
step 329, loss: 3.3054, current_lr: 0.000033
step 330, loss: 3.5908, current_lr: 0.000033
step 331, loss: 3.5019, current_lr: 0.000033
step 332, loss: 3.1928, current_lr: 0.000033
step 333, loss: 3.5549, current_lr: 0.000033
step 334, loss: 3.5897, current_lr: 0.000033
step 335, loss: 3.4436, current_lr: 0.000033
step 336, loss: 3.4215, current_lr: 0.000033
step 337, loss: 3.5433, current_lr: 0.000033
step 338, loss: 3.2682, current_lr: 0.000033
step 339, loss: 3.4025, current_lr: 0.000033
step 340, loss: 3.3932, current_lr: 0.000033
step 341, loss: 2.9359, current_lr: 0.000033
step 342, loss: 3.3682, current_lr: 0.000033
step 343, loss: 3.2672, current_lr: 0.000033
step 344, loss: 3.3555, current_lr: 0.000033
step 345, loss: 3.0276, current_lr: 0.000034
step 346, loss: 3.2402, current_lr: 0.000034
step 347, loss: 3.1757, current_lr: 0.000034
step 348, loss: 3.3950, current_lr: 0.000034
step 349, loss: 3.3382, current_lr: 0.000034
step 350, loss: 3.6026, current_lr: 0.000034
step 351, loss: 3.3960, current_lr: 0.000034
step 352, loss: 3.3445, current_lr: 0.000034
step 353, loss: 3.4271, current_lr: 0.000034
step 354, loss: 3.1902, current_lr: 0.000034
step 355, loss: 3.0855, current_lr: 0.000034
step 356, loss: 3.2732, current_lr: 0.000034
step 357, loss: 3.3022, current_lr: 0.000034
step 358, loss: 3.3574, current_lr: 0.000034
step 359, loss: 2.9910, current_lr: 0.000034
step 360, loss: 3.3261, current_lr: 0.000034
step 361, loss: 3.3066, current_lr: 0.000034
step 362, loss: 3.0272, current_lr: 0.000034
step 363, loss: 2.9533, current_lr: 0.000034
step 364, loss: 3.1665, current_lr: 0.000034
step 365, loss: 3.0877, current_lr: 0.000034
step 366, loss: 3.4937, current_lr: 0.000034
step 367, loss: 3.4789, current_lr: 0.000034
step 368, loss: 3.3282, current_lr: 0.000034
step 369, loss: 2.9891, current_lr: 0.000034
step 370, loss: 3.5392, current_lr: 0.000034
step 371, loss: 3.0863, current_lr: 0.000034
step 372, loss: 2.9934, current_lr: 0.000034
step 373, loss: 3.0317, current_lr: 0.000034
step 374, loss: 3.4234, current_lr: 0.000034
step 375, loss: 3.3318, current_lr: 0.000034
step 376, loss: 3.3810, current_lr: 0.000035
step 377, loss: 3.7085, current_lr: 0.000035
step 378, loss: 3.5172, current_lr: 0.000035
step 379, loss: 3.5794, current_lr: 0.000035
step 380, loss: 3.5367, current_lr: 0.000035
step 381, loss: 3.6414, current_lr: 0.000035
step 382, loss: 3.4965, current_lr: 0.000035
step 383, loss: 3.2391, current_lr: 0.000035
step 384, loss: 3.3087, current_lr: 0.000035
step 385, loss: 3.6221, current_lr: 0.000035
step 386, loss: 3.5030, current_lr: 0.000035
step 387, loss: 3.4418, current_lr: 0.000035
step 388, loss: 3.4272, current_lr: 0.000035
step 389, loss: 3.3604, current_lr: 0.000035
step 390, loss: 3.2949, current_lr: 0.000035
step 391, loss: 3.3157, current_lr: 0.000035
step 392, loss: 2.8948, current_lr: 0.000035
step 393, loss: 3.4354, current_lr: 0.000035
step 394, loss: 3.0683, current_lr: 0.000035
step 395, loss: 3.3618, current_lr: 0.000035
step 396, loss: 3.4822, current_lr: 0.000035
step 397, loss: 3.4610, current_lr: 0.000035
step 398, loss: 3.6040, current_lr: 0.000035
step 399, loss: 3.3425, current_lr: 0.000035
step 400, loss: 3.1820, current_lr: 0.000035
step 401, loss: 3.5990, current_lr: 0.000035
step 402, loss: 3.1341, current_lr: 0.000035
step 403, loss: 3.3502, current_lr: 0.000035
step 404, loss: 3.1855, current_lr: 0.000036
step 405, loss: 3.5249, current_lr: 0.000036
step 406, loss: 3.2404, current_lr: 0.000036
step 407, loss: 3.2712, current_lr: 0.000036
step 408, loss: 3.1033, current_lr: 0.000036
step 409, loss: 3.1606, current_lr: 0.000036
step 410, loss: 3.2469, current_lr: 0.000036
step 411, loss: 3.1835, current_lr: 0.000036
step 412, loss: 3.5621, current_lr: 0.000036
step 413, loss: 3.3497, current_lr: 0.000036
step 414, loss: 3.3995, current_lr: 0.000036
step 415, loss: 2.9396, current_lr: 0.000036
step 416, loss: 2.9951, current_lr: 0.000036
step 417, loss: 3.3622, current_lr: 0.000036
step 418, loss: 3.5240, current_lr: 0.000036
step 419, loss: 3.5988, current_lr: 0.000036
step 420, loss: 3.2427, current_lr: 0.000036
step 421, loss: 3.5258, current_lr: 0.000036
step 422, loss: 3.4650, current_lr: 0.000036
step 423, loss: 3.1480, current_lr: 0.000036
step 424, loss: 2.7471, current_lr: 0.000036
Saved best model with loss: 2.7471 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.7471.pt
step 425, loss: 3.2962, current_lr: 0.000036
step 426, loss: 3.2186, current_lr: 0.000036
step 427, loss: 3.3237, current_lr: 0.000036
step 428, loss: 3.1760, current_lr: 0.000036
step 429, loss: 3.1437, current_lr: 0.000036
step 430, loss: 3.1270, current_lr: 0.000037
step 431, loss: 3.3889, current_lr: 0.000037
step 432, loss: 3.0476, current_lr: 0.000037
step 433, loss: 3.3305, current_lr: 0.000037
step 434, loss: 3.6373, current_lr: 0.000037
step 435, loss: 3.7825, current_lr: 0.000037
step 436, loss: 3.6549, current_lr: 0.000037
step 437, loss: 3.5919, current_lr: 0.000037
step 438, loss: 3.7049, current_lr: 0.000037
step 439, loss: 3.4893, current_lr: 0.000037
step 440, loss: 3.7159, current_lr: 0.000037
step 441, loss: 3.7495, current_lr: 0.000037
step 442, loss: 3.7787, current_lr: 0.000037
step 443, loss: 3.8119, current_lr: 0.000037
step 444, loss: 3.5214, current_lr: 0.000037
step 445, loss: 3.7579, current_lr: 0.000037
step 446, loss: 3.3332, current_lr: 0.000037
step 447, loss: 3.5742, current_lr: 0.000037
step 448, loss: 3.4468, current_lr: 0.000037
step 449, loss: 3.3699, current_lr: 0.000037
step 450, loss: 3.6204, current_lr: 0.000037
step 451, loss: 3.6593, current_lr: 0.000037
step 452, loss: 3.5189, current_lr: 0.000037
step 453, loss: 3.6431, current_lr: 0.000037
step 454, loss: 3.0471, current_lr: 0.000037
step 455, loss: 3.6185, current_lr: 0.000038
step 456, loss: 3.2491, current_lr: 0.000038
step 457, loss: 2.8364, current_lr: 0.000038
step 458, loss: 3.1082, current_lr: 0.000038
step 459, loss: 3.3173, current_lr: 0.000038
step 460, loss: 3.4188, current_lr: 0.000038
step 461, loss: 3.4937, current_lr: 0.000038
step 462, loss: 3.4066, current_lr: 0.000038
step 463, loss: 3.0631, current_lr: 0.000038
step 464, loss: 3.5629, current_lr: 0.000038
step 465, loss: 3.0503, current_lr: 0.000038
step 466, loss: 3.1904, current_lr: 0.000038
step 467, loss: 3.3007, current_lr: 0.000038
step 468, loss: 3.1588, current_lr: 0.000038
step 469, loss: 3.1945, current_lr: 0.000038
step 470, loss: 3.1325, current_lr: 0.000038
step 471, loss: 3.2805, current_lr: 0.000038
step 472, loss: 3.7367, current_lr: 0.000038
step 473, loss: 3.5398, current_lr: 0.000038
step 474, loss: 3.4106, current_lr: 0.000038
step 475, loss: 3.3473, current_lr: 0.000038
step 476, loss: 3.1239, current_lr: 0.000038
step 477, loss: 3.1172, current_lr: 0.000038
step 478, loss: 2.8545, current_lr: 0.000039
step 479, loss: 3.3197, current_lr: 0.000039
step 480, loss: 3.3159, current_lr: 0.000039
step 481, loss: 3.2669, current_lr: 0.000039
step 482, loss: 3.0827, current_lr: 0.000039
step 483, loss: 3.1718, current_lr: 0.000039
step 484, loss: 2.9190, current_lr: 0.000039
step 485, loss: 2.9692, current_lr: 0.000039
step 486, loss: 2.9926, current_lr: 0.000039
step 487, loss: 2.8489, current_lr: 0.000039
step 488, loss: 2.6732, current_lr: 0.000039
Saved best model with loss: 2.6732 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.6732.pt
step 489, loss: 3.2645, current_lr: 0.000039
step 490, loss: 3.5572, current_lr: 0.000039
step 491, loss: 3.6001, current_lr: 0.000039
step 492, loss: 3.6651, current_lr: 0.000039
step 493, loss: 3.0892, current_lr: 0.000039
step 494, loss: 3.1543, current_lr: 0.000039
step 495, loss: 3.4615, current_lr: 0.000039
step 496, loss: 3.3968, current_lr: 0.000039
step 497, loss: 3.1007, current_lr: 0.000039
step 498, loss: 3.4347, current_lr: 0.000039
step 499, loss: 3.4814, current_lr: 0.000039
step 500, loss: 3.3316, current_lr: 0.000039
step 501, loss: 3.3089, current_lr: 0.000040
step 502, loss: 3.4262, current_lr: 0.000040
step 503, loss: 3.1601, current_lr: 0.000040
step 504, loss: 3.2979, current_lr: 0.000040
step 505, loss: 3.2950, current_lr: 0.000040
step 506, loss: 2.8374, current_lr: 0.000040
step 507, loss: 3.2727, current_lr: 0.000040
step 508, loss: 3.1567, current_lr: 0.000040
step 509, loss: 3.2595, current_lr: 0.000040
step 510, loss: 2.9272, current_lr: 0.000040
step 511, loss: 3.1257, current_lr: 0.000040
step 512, loss: 3.0567, current_lr: 0.000040
step 513, loss: 3.2944, current_lr: 0.000040
step 514, loss: 3.2316, current_lr: 0.000040
step 515, loss: 3.4974, current_lr: 0.000040
step 516, loss: 3.2776, current_lr: 0.000040
step 517, loss: 3.2194, current_lr: 0.000040
step 518, loss: 3.3126, current_lr: 0.000040
step 519, loss: 3.0647, current_lr: 0.000040
step 520, loss: 2.9756, current_lr: 0.000040
step 521, loss: 3.1627, current_lr: 0.000040
step 522, loss: 3.1779, current_lr: 0.000041
step 523, loss: 3.2394, current_lr: 0.000041
step 524, loss: 2.8643, current_lr: 0.000041
step 525, loss: 3.2061, current_lr: 0.000041
step 526, loss: 3.1752, current_lr: 0.000041
step 527, loss: 2.9187, current_lr: 0.000041
step 528, loss: 2.8294, current_lr: 0.000041
step 529, loss: 3.0545, current_lr: 0.000041
step 530, loss: 2.9557, current_lr: 0.000041
step 531, loss: 3.3618, current_lr: 0.000041
step 532, loss: 3.3540, current_lr: 0.000041
step 533, loss: 3.1948, current_lr: 0.000041
step 534, loss: 2.8745, current_lr: 0.000041
step 535, loss: 3.4099, current_lr: 0.000041
step 536, loss: 2.9678, current_lr: 0.000041
step 537, loss: 2.8969, current_lr: 0.000041
step 538, loss: 2.9088, current_lr: 0.000041
step 539, loss: 3.2583, current_lr: 0.000041
step 540, loss: 3.1691, current_lr: 0.000041
step 541, loss: 3.2304, current_lr: 0.000041
step 542, loss: 3.5658, current_lr: 0.000041
step 543, loss: 3.3665, current_lr: 0.000042
step 544, loss: 3.4251, current_lr: 0.000042
step 545, loss: 3.3780, current_lr: 0.000042
step 546, loss: 3.4938, current_lr: 0.000042
step 547, loss: 3.3554, current_lr: 0.000042
step 548, loss: 3.1035, current_lr: 0.000042
step 549, loss: 3.1721, current_lr: 0.000042
step 550, loss: 3.4741, current_lr: 0.000042
step 551, loss: 3.3512, current_lr: 0.000042
step 552, loss: 3.3142, current_lr: 0.000042
step 553, loss: 3.2716, current_lr: 0.000042
step 554, loss: 3.2116, current_lr: 0.000042
step 555, loss: 3.1690, current_lr: 0.000042
step 556, loss: 3.1974, current_lr: 0.000042
step 557, loss: 2.7798, current_lr: 0.000042
step 558, loss: 3.3087, current_lr: 0.000042
step 559, loss: 2.9342, current_lr: 0.000042
step 560, loss: 3.2374, current_lr: 0.000042
step 561, loss: 3.3537, current_lr: 0.000042
step 562, loss: 3.3323, current_lr: 0.000042
step 563, loss: 3.4644, current_lr: 0.000043
step 564, loss: 3.2121, current_lr: 0.000043
step 565, loss: 3.0619, current_lr: 0.000043
step 566, loss: 3.4648, current_lr: 0.000043
step 567, loss: 3.0281, current_lr: 0.000043
step 568, loss: 3.2163, current_lr: 0.000043
step 569, loss: 3.0465, current_lr: 0.000043
step 570, loss: 3.3918, current_lr: 0.000043
step 571, loss: 3.0985, current_lr: 0.000043
step 572, loss: 3.1351, current_lr: 0.000043
step 573, loss: 2.9795, current_lr: 0.000043
step 574, loss: 3.0383, current_lr: 0.000043
step 575, loss: 3.1235, current_lr: 0.000043
step 576, loss: 3.0706, current_lr: 0.000043
step 577, loss: 3.4157, current_lr: 0.000043
step 578, loss: 3.2264, current_lr: 0.000043
step 579, loss: 3.2717, current_lr: 0.000043
step 580, loss: 2.8133, current_lr: 0.000043
step 581, loss: 2.8769, current_lr: 0.000043
step 582, loss: 3.2164, current_lr: 0.000044
step 583, loss: 3.3898, current_lr: 0.000044
step 584, loss: 3.4623, current_lr: 0.000044
step 585, loss: 3.1092, current_lr: 0.000044
step 586, loss: 3.3822, current_lr: 0.000044
step 587, loss: 3.3298, current_lr: 0.000044
step 588, loss: 3.0134, current_lr: 0.000044
step 589, loss: 2.6118, current_lr: 0.000044
Saved best model with loss: 2.6118 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.6118.pt
step 590, loss: 3.1569, current_lr: 0.000044
step 591, loss: 3.0722, current_lr: 0.000044
step 592, loss: 3.1806, current_lr: 0.000044
step 593, loss: 3.0481, current_lr: 0.000044
step 594, loss: 3.0069, current_lr: 0.000044
step 595, loss: 2.9932, current_lr: 0.000044
step 596, loss: 3.2387, current_lr: 0.000044
step 597, loss: 2.9037, current_lr: 0.000044
step 598, loss: 3.1713, current_lr: 0.000044
step 599, loss: 3.4812, current_lr: 0.000044
step 600, loss: 3.6214, current_lr: 0.000045
step 601, loss: 3.4923, current_lr: 0.000045
step 602, loss: 3.4265, current_lr: 0.000045
step 603, loss: 3.5522, current_lr: 0.000045
step 604, loss: 3.3420, current_lr: 0.000045
step 605, loss: 3.5562, current_lr: 0.000045
step 606, loss: 3.5978, current_lr: 0.000045
step 607, loss: 3.6152, current_lr: 0.000045
step 608, loss: 3.6567, current_lr: 0.000045
step 609, loss: 3.3686, current_lr: 0.000045
step 610, loss: 3.5982, current_lr: 0.000045
step 611, loss: 3.2037, current_lr: 0.000045
step 612, loss: 3.4529, current_lr: 0.000045
step 613, loss: 3.3153, current_lr: 0.000045
step 614, loss: 3.2361, current_lr: 0.000045
step 615, loss: 3.4929, current_lr: 0.000045
step 616, loss: 3.5141, current_lr: 0.000045
step 617, loss: 3.3848, current_lr: 0.000045
step 618, loss: 3.5113, current_lr: 0.000045
step 619, loss: 2.9157, current_lr: 0.000046
step 620, loss: 3.5016, current_lr: 0.000046
step 621, loss: 3.1247, current_lr: 0.000046
step 622, loss: 2.6946, current_lr: 0.000046
step 623, loss: 2.9958, current_lr: 0.000046
step 624, loss: 3.2042, current_lr: 0.000046
step 625, loss: 3.2925, current_lr: 0.000046
step 626, loss: 3.3497, current_lr: 0.000046
step 627, loss: 3.2470, current_lr: 0.000046
step 628, loss: 2.9276, current_lr: 0.000046
step 629, loss: 3.4166, current_lr: 0.000046
step 630, loss: 2.9151, current_lr: 0.000046
step 631, loss: 3.0247, current_lr: 0.000046
step 632, loss: 3.1576, current_lr: 0.000046
step 633, loss: 3.0205, current_lr: 0.000046
step 634, loss: 3.0641, current_lr: 0.000046
step 635, loss: 2.9907, current_lr: 0.000046
step 636, loss: 3.1383, current_lr: 0.000047
step 637, loss: 3.5830, current_lr: 0.000047
step 638, loss: 3.4003, current_lr: 0.000047
step 639, loss: 3.2725, current_lr: 0.000047
step 640, loss: 3.1930, current_lr: 0.000047
step 641, loss: 2.9923, current_lr: 0.000047
step 642, loss: 3.0029, current_lr: 0.000047
step 643, loss: 2.7420, current_lr: 0.000047
step 644, loss: 3.1853, current_lr: 0.000047
step 645, loss: 3.1676, current_lr: 0.000047
step 646, loss: 3.1322, current_lr: 0.000047
step 647, loss: 2.9592, current_lr: 0.000047
step 648, loss: 3.0291, current_lr: 0.000047
step 649, loss: 2.7922, current_lr: 0.000047
step 650, loss: 2.8364, current_lr: 0.000047
step 651, loss: 2.8702, current_lr: 0.000047
step 652, loss: 2.7194, current_lr: 0.000047
step 653, loss: 2.5667, current_lr: 0.000048
Saved best model with loss: 2.5667 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.5667.pt
step 654, loss: 3.1256, current_lr: 0.000048
step 655, loss: 3.4284, current_lr: 0.000048
step 656, loss: 3.4631, current_lr: 0.000048
step 657, loss: 3.5136, current_lr: 0.000048
step 658, loss: 2.9649, current_lr: 0.000048
step 659, loss: 3.0205, current_lr: 0.000048
step 660, loss: 3.3243, current_lr: 0.000048
step 661, loss: 3.2753, current_lr: 0.000048
step 662, loss: 2.9964, current_lr: 0.000048
step 663, loss: 3.3125, current_lr: 0.000048
step 664, loss: 3.3675, current_lr: 0.000048
step 665, loss: 3.2074, current_lr: 0.000048
step 666, loss: 3.1787, current_lr: 0.000048
step 667, loss: 3.2919, current_lr: 0.000048
step 668, loss: 3.0238, current_lr: 0.000048
step 669, loss: 3.1623, current_lr: 0.000048
step 670, loss: 3.1691, current_lr: 0.000049
step 671, loss: 2.7191, current_lr: 0.000049
step 672, loss: 3.1437, current_lr: 0.000049
step 673, loss: 3.0180, current_lr: 0.000049
step 674, loss: 3.1389, current_lr: 0.000049
step 675, loss: 2.8124, current_lr: 0.000049
step 676, loss: 3.0020, current_lr: 0.000049
step 677, loss: 2.9322, current_lr: 0.000049
step 678, loss: 3.1777, current_lr: 0.000049
step 679, loss: 3.1184, current_lr: 0.000049
step 680, loss: 3.3784, current_lr: 0.000049
step 681, loss: 3.1462, current_lr: 0.000049
step 682, loss: 3.0905, current_lr: 0.000049
step 683, loss: 3.1864, current_lr: 0.000049
step 684, loss: 2.9320, current_lr: 0.000049
step 685, loss: 2.8677, current_lr: 0.000049
step 686, loss: 3.0379, current_lr: 0.000050
step 687, loss: 3.0445, current_lr: 0.000050
step 688, loss: 3.1038, current_lr: 0.000050
step 689, loss: 2.7356, current_lr: 0.000050
step 690, loss: 3.0602, current_lr: 0.000050
step 691, loss: 3.0381, current_lr: 0.000050
step 692, loss: 2.8103, current_lr: 0.000050
step 693, loss: 2.6842, current_lr: 0.000050
step 694, loss: 2.9143, current_lr: 0.000050
step 695, loss: 2.8157, current_lr: 0.000050
step 696, loss: 3.2011, current_lr: 0.000050
step 697, loss: 3.2016, current_lr: 0.000050
step 698, loss: 3.0434, current_lr: 0.000050
step 699, loss: 2.7500, current_lr: 0.000050
step 700, loss: 3.2589, current_lr: 0.000050
step 701, loss: 2.8355, current_lr: 0.000050
step 702, loss: 2.7769, current_lr: 0.000051
step 703, loss: 2.7581, current_lr: 0.000051
step 704, loss: 3.1210, current_lr: 0.000051
step 705, loss: 3.0342, current_lr: 0.000051
step 706, loss: 3.1021, current_lr: 0.000051
step 707, loss: 3.4111, current_lr: 0.000051
step 708, loss: 3.2017, current_lr: 0.000051
step 709, loss: 3.2475, current_lr: 0.000051
step 710, loss: 3.2133, current_lr: 0.000051
step 711, loss: 3.3288, current_lr: 0.000051
step 712, loss: 3.1889, current_lr: 0.000051
step 713, loss: 2.9660, current_lr: 0.000051
step 714, loss: 3.0179, current_lr: 0.000051
step 715, loss: 3.3050, current_lr: 0.000051
step 716, loss: 3.1877, current_lr: 0.000051
step 717, loss: 3.1585, current_lr: 0.000051
step 718, loss: 3.1035, current_lr: 0.000052
step 719, loss: 3.0398, current_lr: 0.000052
step 720, loss: 3.0126, current_lr: 0.000052
step 721, loss: 3.0510, current_lr: 0.000052
step 722, loss: 2.6367, current_lr: 0.000052
step 723, loss: 3.1388, current_lr: 0.000052
step 724, loss: 2.7847, current_lr: 0.000052
step 725, loss: 3.0748, current_lr: 0.000052
step 726, loss: 3.1822, current_lr: 0.000052
step 727, loss: 3.1541, current_lr: 0.000052
step 728, loss: 3.2781, current_lr: 0.000052
step 729, loss: 3.0365, current_lr: 0.000052
step 730, loss: 2.9123, current_lr: 0.000052
step 731, loss: 3.2802, current_lr: 0.000052
step 732, loss: 2.8523, current_lr: 0.000052
step 733, loss: 3.0431, current_lr: 0.000053
step 734, loss: 2.8632, current_lr: 0.000053
step 735, loss: 3.2322, current_lr: 0.000053
step 736, loss: 2.9330, current_lr: 0.000053
step 737, loss: 2.9654, current_lr: 0.000053
step 738, loss: 2.8245, current_lr: 0.000053
step 739, loss: 2.8884, current_lr: 0.000053
step 740, loss: 2.9667, current_lr: 0.000053
step 741, loss: 2.8961, current_lr: 0.000053
step 742, loss: 3.2306, current_lr: 0.000053
step 743, loss: 3.0784, current_lr: 0.000053
step 744, loss: 3.1095, current_lr: 0.000053
step 745, loss: 2.6646, current_lr: 0.000053
step 746, loss: 2.7295, current_lr: 0.000053
step 747, loss: 3.0418, current_lr: 0.000053
step 748, loss: 3.2160, current_lr: 0.000054
step 749, loss: 3.2667, current_lr: 0.000054
step 750, loss: 2.9391, current_lr: 0.000054
step 751, loss: 3.2019, current_lr: 0.000054
step 752, loss: 3.1658, current_lr: 0.000054
step 753, loss: 2.8541, current_lr: 0.000054
step 754, loss: 2.4820, current_lr: 0.000054
Saved best model with loss: 2.4820 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.4820.pt
step 755, loss: 2.9947, current_lr: 0.000054
step 756, loss: 2.9127, current_lr: 0.000054
step 757, loss: 3.0262, current_lr: 0.000054
step 758, loss: 2.9004, current_lr: 0.000054
step 759, loss: 2.8581, current_lr: 0.000054
step 760, loss: 2.8485, current_lr: 0.000054
step 761, loss: 3.0749, current_lr: 0.000054
step 762, loss: 2.7641, current_lr: 0.000054
step 763, loss: 2.9939, current_lr: 0.000055
step 764, loss: 3.3111, current_lr: 0.000055
step 765, loss: 3.4321, current_lr: 0.000055
step 766, loss: 3.2936, current_lr: 0.000055
step 767, loss: 3.2289, current_lr: 0.000055
step 768, loss: 3.3626, current_lr: 0.000055
step 769, loss: 3.1566, current_lr: 0.000055
step 770, loss: 3.3468, current_lr: 0.000055
step 771, loss: 3.4050, current_lr: 0.000055
step 772, loss: 3.3951, current_lr: 0.000055
step 773, loss: 3.4390, current_lr: 0.000055
step 774, loss: 3.1719, current_lr: 0.000055
step 775, loss: 3.3677, current_lr: 0.000055
step 776, loss: 3.0275, current_lr: 0.000055
step 777, loss: 3.2729, current_lr: 0.000056
step 778, loss: 3.1416, current_lr: 0.000056
step 779, loss: 3.0470, current_lr: 0.000056
step 780, loss: 3.3148, current_lr: 0.000056
step 781, loss: 3.3356, current_lr: 0.000056
step 782, loss: 3.2059, current_lr: 0.000056
step 783, loss: 3.3575, current_lr: 0.000056
step 784, loss: 2.7516, current_lr: 0.000056
step 785, loss: 3.3333, current_lr: 0.000056
step 786, loss: 2.9632, current_lr: 0.000056
step 787, loss: 2.5252, current_lr: 0.000056
step 788, loss: 2.8497, current_lr: 0.000056
step 789, loss: 3.0632, current_lr: 0.000056
step 790, loss: 3.1380, current_lr: 0.000056
step 791, loss: 3.1871, current_lr: 0.000056
step 792, loss: 3.0789, current_lr: 0.000057
step 793, loss: 2.7899, current_lr: 0.000057
step 794, loss: 3.2550, current_lr: 0.000057
step 795, loss: 2.7749, current_lr: 0.000057
step 796, loss: 2.8531, current_lr: 0.000057
step 797, loss: 3.0191, current_lr: 0.000057
step 798, loss: 2.8693, current_lr: 0.000057
step 799, loss: 2.9282, current_lr: 0.000057
step 800, loss: 2.8469, current_lr: 0.000057
step 801, loss: 2.9958, current_lr: 0.000057
step 802, loss: 3.3814, current_lr: 0.000057
step 803, loss: 3.2139, current_lr: 0.000057
step 804, loss: 3.0920, current_lr: 0.000057
step 805, loss: 3.0048, current_lr: 0.000057
step 806, loss: 2.8274, current_lr: 0.000058
step 807, loss: 2.8496, current_lr: 0.000058
step 808, loss: 2.6115, current_lr: 0.000058
step 809, loss: 3.0124, current_lr: 0.000058
step 810, loss: 2.9711, current_lr: 0.000058
step 811, loss: 2.9623, current_lr: 0.000058
step 812, loss: 2.7954, current_lr: 0.000058
step 813, loss: 2.8610, current_lr: 0.000058
step 814, loss: 2.6324, current_lr: 0.000058
step 815, loss: 2.6725, current_lr: 0.000058
step 816, loss: 2.7083, current_lr: 0.000058
step 817, loss: 2.5614, current_lr: 0.000058
step 818, loss: 2.4224, current_lr: 0.000058
Saved best model with loss: 2.4224 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.4224.pt
step 819, loss: 2.9528, current_lr: 0.000059
step 820, loss: 3.2686, current_lr: 0.000059
step 821, loss: 3.2885, current_lr: 0.000059
step 822, loss: 3.3328, current_lr: 0.000059
step 823, loss: 2.8322, current_lr: 0.000059
step 824, loss: 2.8989, current_lr: 0.000059
step 825, loss: 3.1513, current_lr: 0.000059
step 826, loss: 3.1152, current_lr: 0.000059
step 827, loss: 2.8455, current_lr: 0.000059
step 828, loss: 3.1573, current_lr: 0.000059
step 829, loss: 3.2155, current_lr: 0.000059
step 830, loss: 3.0406, current_lr: 0.000059
step 831, loss: 3.0119, current_lr: 0.000059
step 832, loss: 3.1316, current_lr: 0.000059
step 833, loss: 2.8648, current_lr: 0.000060
step 834, loss: 3.0023, current_lr: 0.000060
step 835, loss: 3.0113, current_lr: 0.000060
step 836, loss: 2.5692, current_lr: 0.000060
step 837, loss: 2.9861, current_lr: 0.000060
step 838, loss: 2.8533, current_lr: 0.000060
step 839, loss: 2.9819, current_lr: 0.000060
step 840, loss: 2.6522, current_lr: 0.000060
step 841, loss: 2.8237, current_lr: 0.000060
step 842, loss: 2.7420, current_lr: 0.000060
step 843, loss: 2.9980, current_lr: 0.000060
step 844, loss: 2.9400, current_lr: 0.000060
step 845, loss: 3.2192, current_lr: 0.000060
step 846, loss: 2.9586, current_lr: 0.000061
step 847, loss: 2.9182, current_lr: 0.000061
step 848, loss: 3.0112, current_lr: 0.000061
step 849, loss: 2.7582, current_lr: 0.000061
step 850, loss: 2.7068, current_lr: 0.000061
step 851, loss: 2.8596, current_lr: 0.000061
step 852, loss: 2.8588, current_lr: 0.000061
step 853, loss: 2.9347, current_lr: 0.000061
step 854, loss: 2.5959, current_lr: 0.000061
step 855, loss: 2.8865, current_lr: 0.000061
step 856, loss: 2.8632, current_lr: 0.000061
step 857, loss: 2.6717, current_lr: 0.000061
step 858, loss: 2.5266, current_lr: 0.000061
step 859, loss: 2.7499, current_lr: 0.000062
step 860, loss: 2.6501, current_lr: 0.000062
step 861, loss: 3.0115, current_lr: 0.000062
step 862, loss: 3.0130, current_lr: 0.000062
step 863, loss: 2.8532, current_lr: 0.000062
step 864, loss: 2.5699, current_lr: 0.000062
step 865, loss: 3.0582, current_lr: 0.000062
step 866, loss: 2.6582, current_lr: 0.000062
step 867, loss: 2.6090, current_lr: 0.000062
step 868, loss: 2.5599, current_lr: 0.000062
step 869, loss: 2.8892, current_lr: 0.000062
step 870, loss: 2.8075, current_lr: 0.000062
step 871, loss: 2.8926, current_lr: 0.000062
step 872, loss: 3.1975, current_lr: 0.000063
step 873, loss: 2.9828, current_lr: 0.000063
step 874, loss: 3.0073, current_lr: 0.000063
step 875, loss: 2.9615, current_lr: 0.000063
step 876, loss: 3.1013, current_lr: 0.000063
step 877, loss: 2.9570, current_lr: 0.000063
step 878, loss: 2.7707, current_lr: 0.000063
step 879, loss: 2.8002, current_lr: 0.000063
step 880, loss: 3.0607, current_lr: 0.000063
step 881, loss: 2.9639, current_lr: 0.000063
step 882, loss: 2.9324, current_lr: 0.000063
step 883, loss: 2.8787, current_lr: 0.000063
step 884, loss: 2.7837, current_lr: 0.000063
step 885, loss: 2.7920, current_lr: 0.000064
step 886, loss: 2.8420, current_lr: 0.000064
step 887, loss: 2.4476, current_lr: 0.000064
step 888, loss: 2.9057, current_lr: 0.000064
step 889, loss: 2.5853, current_lr: 0.000064
step 890, loss: 2.8626, current_lr: 0.000064
step 891, loss: 2.9446, current_lr: 0.000064
step 892, loss: 2.9189, current_lr: 0.000064
step 893, loss: 3.0376, current_lr: 0.000064
step 894, loss: 2.8090, current_lr: 0.000064
step 895, loss: 2.7111, current_lr: 0.000064
step 896, loss: 3.0297, current_lr: 0.000064
step 897, loss: 2.6338, current_lr: 0.000064
step 898, loss: 2.8177, current_lr: 0.000065
step 899, loss: 2.6229, current_lr: 0.000065
step 900, loss: 3.0055, current_lr: 0.000065
step 901, loss: 2.7167, current_lr: 0.000065
step 902, loss: 2.7296, current_lr: 0.000065
step 903, loss: 2.6108, current_lr: 0.000065
step 904, loss: 2.6819, current_lr: 0.000065
step 905, loss: 2.7456, current_lr: 0.000065
step 906, loss: 2.6746, current_lr: 0.000065
step 907, loss: 2.9859, current_lr: 0.000065
step 908, loss: 2.8925, current_lr: 0.000065
step 909, loss: 2.8928, current_lr: 0.000065
step 910, loss: 2.4621, current_lr: 0.000066
step 911, loss: 2.5356, current_lr: 0.000066
step 912, loss: 2.7951, current_lr: 0.000066
step 913, loss: 2.9542, current_lr: 0.000066
step 914, loss: 3.0147, current_lr: 0.000066
step 915, loss: 2.7157, current_lr: 0.000066
step 916, loss: 2.9350, current_lr: 0.000066
step 917, loss: 2.9365, current_lr: 0.000066
step 918, loss: 2.6521, current_lr: 0.000066
step 919, loss: 2.2869, current_lr: 0.000066
Saved best model with loss: 2.2869 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.2869.pt
step 920, loss: 2.7579, current_lr: 0.000066
step 921, loss: 2.7095, current_lr: 0.000066
step 922, loss: 2.8054, current_lr: 0.000066
step 923, loss: 2.6849, current_lr: 0.000067
step 924, loss: 2.6400, current_lr: 0.000067
step 925, loss: 2.6398, current_lr: 0.000067
step 926, loss: 2.8681, current_lr: 0.000067
step 927, loss: 2.5680, current_lr: 0.000067
step 928, loss: 2.7670, current_lr: 0.000067
step 929, loss: 3.0348, current_lr: 0.000067
step 930, loss: 3.1528, current_lr: 0.000067
step 931, loss: 3.0249, current_lr: 0.000067
step 932, loss: 2.9639, current_lr: 0.000067
step 933, loss: 3.1265, current_lr: 0.000067
step 934, loss: 2.9211, current_lr: 0.000067
step 935, loss: 3.0893, current_lr: 0.000068
step 936, loss: 3.1261, current_lr: 0.000068
step 937, loss: 3.1042, current_lr: 0.000068
step 938, loss: 3.1330, current_lr: 0.000068
step 939, loss: 2.9151, current_lr: 0.000068
step 940, loss: 3.0687, current_lr: 0.000068
step 941, loss: 2.7931, current_lr: 0.000068
step 942, loss: 3.0139, current_lr: 0.000068
step 943, loss: 2.9082, current_lr: 0.000068
step 944, loss: 2.8042, current_lr: 0.000068
step 945, loss: 3.0877, current_lr: 0.000068
step 946, loss: 3.0906, current_lr: 0.000068
step 947, loss: 2.9480, current_lr: 0.000069
step 948, loss: 3.1213, current_lr: 0.000069
step 949, loss: 2.5497, current_lr: 0.000069
step 950, loss: 3.1443, current_lr: 0.000069
step 951, loss: 2.7516, current_lr: 0.000069
step 952, loss: 2.3009, current_lr: 0.000069
step 953, loss: 2.6239, current_lr: 0.000069
step 954, loss: 2.8433, current_lr: 0.000069
step 955, loss: 2.9402, current_lr: 0.000069
step 956, loss: 2.9962, current_lr: 0.000069
step 957, loss: 2.8924, current_lr: 0.000069
step 958, loss: 2.5817, current_lr: 0.000069
step 959, loss: 2.9951, current_lr: 0.000070
step 960, loss: 2.5720, current_lr: 0.000070
step 961, loss: 2.6958, current_lr: 0.000070
step 962, loss: 2.8885, current_lr: 0.000070
step 963, loss: 2.7344, current_lr: 0.000070
step 964, loss: 2.7819, current_lr: 0.000070
step 965, loss: 2.6768, current_lr: 0.000070
step 966, loss: 2.8089, current_lr: 0.000070
step 967, loss: 3.1534, current_lr: 0.000070
step 968, loss: 3.0203, current_lr: 0.000070
step 969, loss: 2.9265, current_lr: 0.000070
step 970, loss: 2.8222, current_lr: 0.000071
step 971, loss: 2.6802, current_lr: 0.000071
step 972, loss: 2.7132, current_lr: 0.000071
step 973, loss: 2.4544, current_lr: 0.000071
step 974, loss: 2.8220, current_lr: 0.000071
step 975, loss: 2.7474, current_lr: 0.000071
step 976, loss: 2.7647, current_lr: 0.000071
step 977, loss: 2.6022, current_lr: 0.000071
step 978, loss: 2.6718, current_lr: 0.000071
step 979, loss: 2.4514, current_lr: 0.000071
step 980, loss: 2.4797, current_lr: 0.000071
step 981, loss: 2.5299, current_lr: 0.000071
step 982, loss: 2.3603, current_lr: 0.000072
step 983, loss: 2.2541, current_lr: 0.000072
Saved best model with loss: 2.2541 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.2541.pt
step 984, loss: 2.7299, current_lr: 0.000072
step 985, loss: 3.0267, current_lr: 0.000072
step 986, loss: 3.0384, current_lr: 0.000072
step 987, loss: 3.0915, current_lr: 0.000072
step 988, loss: 2.6341, current_lr: 0.000072
step 989, loss: 2.7108, current_lr: 0.000072
step 990, loss: 2.9353, current_lr: 0.000072
step 991, loss: 2.9067, current_lr: 0.000072
step 992, loss: 2.6647, current_lr: 0.000072
step 993, loss: 2.9385, current_lr: 0.000073
step 994, loss: 3.0192, current_lr: 0.000073
step 995, loss: 2.8391, current_lr: 0.000073
step 996, loss: 2.8085, current_lr: 0.000073
step 997, loss: 2.9251, current_lr: 0.000073
step 998, loss: 2.6678, current_lr: 0.000073
step 999, loss: 2.8038, current_lr: 0.000073
step 1000, loss: 2.8169, current_lr: 0.000073
step 1001, loss: 2.3797, current_lr: 0.000073
step 1002, loss: 2.7961, current_lr: 0.000073
step 1003, loss: 2.6471, current_lr: 0.000073
step 1004, loss: 2.7618, current_lr: 0.000073
step 1005, loss: 2.4671, current_lr: 0.000074
step 1006, loss: 2.6364, current_lr: 0.000074
step 1007, loss: 2.5425, current_lr: 0.000074
step 1008, loss: 2.7757, current_lr: 0.000074
step 1009, loss: 2.7402, current_lr: 0.000074
step 1010, loss: 3.0193, current_lr: 0.000074
step 1011, loss: 2.7268, current_lr: 0.000074
step 1012, loss: 2.7204, current_lr: 0.000074
step 1013, loss: 2.7841, current_lr: 0.000074
step 1014, loss: 2.5268, current_lr: 0.000074
step 1015, loss: 2.4996, current_lr: 0.000074
step 1016, loss: 2.6638, current_lr: 0.000075
step 1017, loss: 2.6116, current_lr: 0.000075
step 1018, loss: 2.6914, current_lr: 0.000075
step 1019, loss: 2.3595, current_lr: 0.000075
step 1020, loss: 2.6345, current_lr: 0.000075
step 1021, loss: 2.6550, current_lr: 0.000075
step 1022, loss: 2.5043, current_lr: 0.000075
step 1023, loss: 2.3236, current_lr: 0.000075
step 1024, loss: 2.5257, current_lr: 0.000075
step 1025, loss: 2.4511, current_lr: 0.000075
step 1026, loss: 2.8042, current_lr: 0.000075
step 1027, loss: 2.7953, current_lr: 0.000076
step 1028, loss: 2.6387, current_lr: 0.000076
step 1029, loss: 2.3856, current_lr: 0.000076
step 1030, loss: 2.8075, current_lr: 0.000076
step 1031, loss: 2.4559, current_lr: 0.000076
step 1032, loss: 2.4388, current_lr: 0.000076
step 1033, loss: 2.4072, current_lr: 0.000076
step 1034, loss: 2.6946, current_lr: 0.000076
step 1035, loss: 2.6069, current_lr: 0.000076
step 1036, loss: 2.6764, current_lr: 0.000076
step 1037, loss: 2.9574, current_lr: 0.000076
step 1038, loss: 2.7420, current_lr: 0.000077
step 1039, loss: 2.7543, current_lr: 0.000077
step 1040, loss: 2.7294, current_lr: 0.000077
step 1041, loss: 2.8148, current_lr: 0.000077
step 1042, loss: 2.6673, current_lr: 0.000077
step 1043, loss: 2.5270, current_lr: 0.000077
step 1044, loss: 2.5597, current_lr: 0.000077
step 1045, loss: 2.7926, current_lr: 0.000077
step 1046, loss: 2.6776, current_lr: 0.000077
step 1047, loss: 2.6389, current_lr: 0.000077
step 1048, loss: 2.5851, current_lr: 0.000077
step 1049, loss: 2.4943, current_lr: 0.000078
step 1050, loss: 2.4909, current_lr: 0.000078
step 1051, loss: 2.5706, current_lr: 0.000078
step 1052, loss: 2.2116, current_lr: 0.000078
Saved best model with loss: 2.2116 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.2116.pt
step 1053, loss: 2.6222, current_lr: 0.000078
step 1054, loss: 2.3549, current_lr: 0.000078
step 1055, loss: 2.5913, current_lr: 0.000078
step 1056, loss: 2.6627, current_lr: 0.000078
step 1057, loss: 2.6383, current_lr: 0.000078
step 1058, loss: 2.7633, current_lr: 0.000078
step 1059, loss: 2.5227, current_lr: 0.000078
step 1060, loss: 2.4705, current_lr: 0.000079
step 1061, loss: 2.7164, current_lr: 0.000079
step 1062, loss: 2.3683, current_lr: 0.000079
step 1063, loss: 2.5584, current_lr: 0.000079
step 1064, loss: 2.3765, current_lr: 0.000079
step 1065, loss: 2.7252, current_lr: 0.000079
step 1066, loss: 2.4456, current_lr: 0.000079
step 1067, loss: 2.4595, current_lr: 0.000079
step 1068, loss: 2.3778, current_lr: 0.000079
step 1069, loss: 2.4458, current_lr: 0.000079
step 1070, loss: 2.5119, current_lr: 0.000079
step 1071, loss: 2.3882, current_lr: 0.000080
step 1072, loss: 2.6712, current_lr: 0.000080
step 1073, loss: 2.6395, current_lr: 0.000080
step 1074, loss: 2.6148, current_lr: 0.000080
step 1075, loss: 2.2405, current_lr: 0.000080
step 1076, loss: 2.3167, current_lr: 0.000080
step 1077, loss: 2.5004, current_lr: 0.000080
step 1078, loss: 2.6411, current_lr: 0.000080
step 1079, loss: 2.6734, current_lr: 0.000080
step 1080, loss: 2.3987, current_lr: 0.000080
step 1081, loss: 2.5839, current_lr: 0.000081
step 1082, loss: 2.6130, current_lr: 0.000081
step 1083, loss: 2.3866, current_lr: 0.000081
step 1084, loss: 2.0446, current_lr: 0.000081
Saved best model with loss: 2.0446 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.0446.pt
step 1085, loss: 2.4985, current_lr: 0.000081
step 1086, loss: 2.4399, current_lr: 0.000081
step 1087, loss: 2.5103, current_lr: 0.000081
step 1088, loss: 2.4152, current_lr: 0.000081
step 1089, loss: 2.4346, current_lr: 0.000081
step 1090, loss: 2.4167, current_lr: 0.000081
step 1091, loss: 2.5925, current_lr: 0.000081
step 1092, loss: 2.3371, current_lr: 0.000082
step 1093, loss: 2.5000, current_lr: 0.000082
step 1094, loss: 2.7991, current_lr: 0.000082
step 1095, loss: 2.8957, current_lr: 0.000082
step 1096, loss: 2.7378, current_lr: 0.000082
step 1097, loss: 2.6442, current_lr: 0.000082
step 1098, loss: 2.8359, current_lr: 0.000082
step 1099, loss: 2.6679, current_lr: 0.000082
step 1100, loss: 2.7801, current_lr: 0.000082
step 1101, loss: 2.7769, current_lr: 0.000082
step 1102, loss: 2.7290, current_lr: 0.000083
step 1103, loss: 2.7577, current_lr: 0.000083
step 1104, loss: 2.5762, current_lr: 0.000083
step 1105, loss: 2.7397, current_lr: 0.000083
step 1106, loss: 2.5149, current_lr: 0.000083
step 1107, loss: 2.7220, current_lr: 0.000083
step 1108, loss: 2.6047, current_lr: 0.000083
step 1109, loss: 2.4810, current_lr: 0.000083
step 1110, loss: 2.7846, current_lr: 0.000083
step 1111, loss: 2.7600, current_lr: 0.000083
step 1112, loss: 2.6358, current_lr: 0.000083
step 1113, loss: 2.8306, current_lr: 0.000084
step 1114, loss: 2.2873, current_lr: 0.000084
step 1115, loss: 2.8165, current_lr: 0.000084
step 1116, loss: 2.4335, current_lr: 0.000084
step 1117, loss: 2.0268, current_lr: 0.000084
Saved best model with loss: 2.0268 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.0268.pt
step 1118, loss: 2.3862, current_lr: 0.000084
step 1119, loss: 2.6453, current_lr: 0.000084
step 1120, loss: 2.6619, current_lr: 0.000084
step 1121, loss: 2.6795, current_lr: 0.000084
step 1122, loss: 2.5714, current_lr: 0.000084
step 1123, loss: 2.3766, current_lr: 0.000085
step 1124, loss: 2.8395, current_lr: 0.000085
step 1125, loss: 2.3949, current_lr: 0.000085
step 1126, loss: 2.4394, current_lr: 0.000085
step 1127, loss: 2.6341, current_lr: 0.000085
step 1128, loss: 2.5199, current_lr: 0.000085
step 1129, loss: 2.5602, current_lr: 0.000085
step 1130, loss: 2.4689, current_lr: 0.000085
step 1131, loss: 2.6261, current_lr: 0.000085
step 1132, loss: 2.9223, current_lr: 0.000085
step 1133, loss: 2.8092, current_lr: 0.000086
step 1134, loss: 2.7769, current_lr: 0.000086
step 1135, loss: 2.6601, current_lr: 0.000086
step 1136, loss: 2.5159, current_lr: 0.000086
step 1137, loss: 2.5483, current_lr: 0.000086
step 1138, loss: 2.3338, current_lr: 0.000086
step 1139, loss: 2.6449, current_lr: 0.000086
step 1140, loss: 2.5976, current_lr: 0.000086
step 1141, loss: 2.6234, current_lr: 0.000086
step 1142, loss: 2.4608, current_lr: 0.000086
step 1143, loss: 2.5401, current_lr: 0.000087
step 1144, loss: 2.3259, current_lr: 0.000087
step 1145, loss: 2.3362, current_lr: 0.000087
step 1146, loss: 2.3606, current_lr: 0.000087
step 1147, loss: 2.2254, current_lr: 0.000087
step 1148, loss: 2.1003, current_lr: 0.000087
step 1149, loss: 2.5387, current_lr: 0.000087
step 1150, loss: 2.8498, current_lr: 0.000087
step 1151, loss: 2.8290, current_lr: 0.000087
step 1152, loss: 2.8830, current_lr: 0.000087
step 1153, loss: 2.4339, current_lr: 0.000088
step 1154, loss: 2.4868, current_lr: 0.000088
step 1155, loss: 2.6956, current_lr: 0.000088
step 1156, loss: 2.6793, current_lr: 0.000088
step 1157, loss: 2.4632, current_lr: 0.000088
step 1158, loss: 2.7533, current_lr: 0.000088
step 1159, loss: 2.8231, current_lr: 0.000088
step 1160, loss: 2.6053, current_lr: 0.000088
step 1161, loss: 2.5880, current_lr: 0.000088
step 1162, loss: 2.7053, current_lr: 0.000088
step 1163, loss: 2.4942, current_lr: 0.000089
step 1164, loss: 2.5946, current_lr: 0.000089
step 1165, loss: 2.6109, current_lr: 0.000089
step 1166, loss: 2.1835, current_lr: 0.000089
step 1167, loss: 2.5770, current_lr: 0.000089
step 1168, loss: 2.4341, current_lr: 0.000089
step 1169, loss: 2.5478, current_lr: 0.000089
step 1170, loss: 2.2873, current_lr: 0.000089
step 1171, loss: 2.4475, current_lr: 0.000089
step 1172, loss: 2.3303, current_lr: 0.000089
step 1173, loss: 2.5921, current_lr: 0.000090
step 1174, loss: 2.5789, current_lr: 0.000090
step 1175, loss: 2.8261, current_lr: 0.000090
step 1176, loss: 2.5337, current_lr: 0.000090
step 1177, loss: 2.5554, current_lr: 0.000090
step 1178, loss: 2.5722, current_lr: 0.000090
step 1179, loss: 2.3183, current_lr: 0.000090
step 1180, loss: 2.2838, current_lr: 0.000090
step 1181, loss: 2.4053, current_lr: 0.000090
step 1182, loss: 2.3994, current_lr: 0.000090
step 1183, loss: 2.4965, current_lr: 0.000091
step 1184, loss: 2.1774, current_lr: 0.000091
step 1185, loss: 2.4296, current_lr: 0.000091
step 1186, loss: 2.3890, current_lr: 0.000091
step 1187, loss: 2.2989, current_lr: 0.000091
step 1188, loss: 2.1337, current_lr: 0.000091
step 1189, loss: 2.3815, current_lr: 0.000091
step 1190, loss: 2.3103, current_lr: 0.000091
step 1191, loss: 2.5295, current_lr: 0.000091
step 1192, loss: 2.5002, current_lr: 0.000091
step 1193, loss: 2.4382, current_lr: 0.000092
step 1194, loss: 2.2243, current_lr: 0.000092
step 1195, loss: 2.6961, current_lr: 0.000092
step 1196, loss: 2.3414, current_lr: 0.000092
step 1197, loss: 2.2745, current_lr: 0.000092
step 1198, loss: 2.1931, current_lr: 0.000092
step 1199, loss: 2.4382, current_lr: 0.000092
step 1200, loss: 2.3859, current_lr: 0.000092
step 1201, loss: 2.4961, current_lr: 0.000092
step 1202, loss: 2.8116, current_lr: 0.000093
step 1203, loss: 2.6349, current_lr: 0.000093
step 1204, loss: 2.6376, current_lr: 0.000093
step 1205, loss: 2.5813, current_lr: 0.000093
step 1206, loss: 2.6889, current_lr: 0.000093
step 1207, loss: 2.5900, current_lr: 0.000093
step 1208, loss: 2.4340, current_lr: 0.000093
step 1209, loss: 2.4422, current_lr: 0.000093
step 1210, loss: 2.6584, current_lr: 0.000093
step 1211, loss: 2.5143, current_lr: 0.000093
step 1212, loss: 2.4916, current_lr: 0.000094
step 1213, loss: 2.4648, current_lr: 0.000094
step 1214, loss: 2.3422, current_lr: 0.000094
step 1215, loss: 2.3483, current_lr: 0.000094
step 1216, loss: 2.4376, current_lr: 0.000094
step 1217, loss: 2.0728, current_lr: 0.000094
step 1218, loss: 2.4377, current_lr: 0.000094
step 1219, loss: 2.1568, current_lr: 0.000094
step 1220, loss: 2.3978, current_lr: 0.000094
step 1221, loss: 2.4586, current_lr: 0.000094
step 1222, loss: 2.3929, current_lr: 0.000095
step 1223, loss: 2.5365, current_lr: 0.000095
step 1224, loss: 2.3219, current_lr: 0.000095
step 1225, loss: 2.3206, current_lr: 0.000095
step 1226, loss: 2.5541, current_lr: 0.000095
step 1227, loss: 2.2307, current_lr: 0.000095
step 1228, loss: 2.3969, current_lr: 0.000095
step 1229, loss: 2.2202, current_lr: 0.000095
step 1230, loss: 2.5544, current_lr: 0.000095
step 1231, loss: 2.3063, current_lr: 0.000096
step 1232, loss: 2.3536, current_lr: 0.000096
step 1233, loss: 2.2443, current_lr: 0.000096
step 1234, loss: 2.2904, current_lr: 0.000096
step 1235, loss: 2.3632, current_lr: 0.000096
step 1236, loss: 2.3089, current_lr: 0.000096
step 1237, loss: 2.5321, current_lr: 0.000096
step 1238, loss: 2.4788, current_lr: 0.000096
step 1239, loss: 2.4354, current_lr: 0.000096
step 1240, loss: 2.1020, current_lr: 0.000096
step 1241, loss: 2.1635, current_lr: 0.000097
step 1242, loss: 2.3986, current_lr: 0.000097
step 1243, loss: 2.5339, current_lr: 0.000097
step 1244, loss: 2.6167, current_lr: 0.000097
step 1245, loss: 2.2814, current_lr: 0.000097
step 1246, loss: 2.4480, current_lr: 0.000097
step 1247, loss: 2.4359, current_lr: 0.000097
step 1248, loss: 2.1997, current_lr: 0.000097
step 1249, loss: 1.9240, current_lr: 0.000097
Saved best model with loss: 1.9240 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.9240.pt
step 1250, loss: 2.3289, current_lr: 0.000098
step 1251, loss: 2.2524, current_lr: 0.000098
step 1252, loss: 2.3476, current_lr: 0.000098
step 1253, loss: 2.2454, current_lr: 0.000098
step 1254, loss: 2.2157, current_lr: 0.000098
step 1255, loss: 2.2043, current_lr: 0.000098
step 1256, loss: 2.3924, current_lr: 0.000098
step 1257, loss: 2.2076, current_lr: 0.000098
step 1258, loss: 2.3150, current_lr: 0.000098
step 1259, loss: 2.6194, current_lr: 0.000099
step 1260, loss: 2.6217, current_lr: 0.000099
step 1261, loss: 2.5465, current_lr: 0.000099
step 1262, loss: 2.4913, current_lr: 0.000099
step 1263, loss: 2.6778, current_lr: 0.000099
step 1264, loss: 2.4812, current_lr: 0.000099
step 1265, loss: 2.5772, current_lr: 0.000099
step 1266, loss: 2.5689, current_lr: 0.000099
step 1267, loss: 2.5719, current_lr: 0.000099
step 1268, loss: 2.6350, current_lr: 0.000100
step 1269, loss: 2.4280, current_lr: 0.000100
step 1270, loss: 2.5853, current_lr: 0.000100
step 1271, loss: 2.3661, current_lr: 0.000100
step 1272, loss: 2.5521, current_lr: 0.000100
step 1273, loss: 2.4117, current_lr: 0.000100
step 1274, loss: 2.3616, current_lr: 0.000100
step 1275, loss: 2.6689, current_lr: 0.000100
step 1276, loss: 2.6728, current_lr: 0.000100
step 1277, loss: 2.5082, current_lr: 0.000100
step 1278, loss: 2.6573, current_lr: 0.000101
step 1279, loss: 2.1331, current_lr: 0.000101
step 1280, loss: 2.6337, current_lr: 0.000101
step 1281, loss: 2.2875, current_lr: 0.000101
step 1282, loss: 1.8862, current_lr: 0.000101
Saved best model with loss: 1.8862 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.8862.pt
step 1283, loss: 2.1963, current_lr: 0.000101
step 1284, loss: 2.3928, current_lr: 0.000101
step 1285, loss: 2.4596, current_lr: 0.000101
step 1286, loss: 2.4891, current_lr: 0.000101
step 1287, loss: 2.5149, current_lr: 0.000102
step 1288, loss: 2.2651, current_lr: 0.000102
step 1289, loss: 2.6022, current_lr: 0.000102
step 1290, loss: 2.2414, current_lr: 0.000102
step 1291, loss: 2.2655, current_lr: 0.000102
step 1292, loss: 2.4732, current_lr: 0.000102
step 1293, loss: 2.3469, current_lr: 0.000102
step 1294, loss: 2.4389, current_lr: 0.000102
step 1295, loss: 2.3156, current_lr: 0.000102
step 1296, loss: 2.4678, current_lr: 0.000103
step 1297, loss: 2.7771, current_lr: 0.000103
step 1298, loss: 2.6608, current_lr: 0.000103
step 1299, loss: 2.5554, current_lr: 0.000103
step 1300, loss: 2.4873, current_lr: 0.000103
step 1301, loss: 2.3414, current_lr: 0.000103
step 1302, loss: 2.4018, current_lr: 0.000103
step 1303, loss: 2.1784, current_lr: 0.000103
step 1304, loss: 2.4554, current_lr: 0.000103
step 1305, loss: 2.3944, current_lr: 0.000104
step 1306, loss: 2.4454, current_lr: 0.000104
step 1307, loss: 2.2640, current_lr: 0.000104
step 1308, loss: 2.3914, current_lr: 0.000104
step 1309, loss: 2.1897, current_lr: 0.000104
step 1310, loss: 2.2098, current_lr: 0.000104
step 1311, loss: 2.2503, current_lr: 0.000104
step 1312, loss: 2.1313, current_lr: 0.000104
step 1313, loss: 2.0165, current_lr: 0.000104
step 1314, loss: 2.4129, current_lr: 0.000105
step 1315, loss: 2.7318, current_lr: 0.000105
step 1316, loss: 2.7048, current_lr: 0.000105
step 1317, loss: 2.7357, current_lr: 0.000105
step 1318, loss: 2.3173, current_lr: 0.000105
step 1319, loss: 2.3487, current_lr: 0.000105
step 1320, loss: 2.5973, current_lr: 0.000105
step 1321, loss: 2.5649, current_lr: 0.000105
step 1322, loss: 2.3429, current_lr: 0.000105
step 1323, loss: 2.5559, current_lr: 0.000106
step 1324, loss: 2.6060, current_lr: 0.000106
step 1325, loss: 2.4105, current_lr: 0.000106
step 1326, loss: 2.4095, current_lr: 0.000106
step 1327, loss: 2.5530, current_lr: 0.000106
step 1328, loss: 2.3060, current_lr: 0.000106
step 1329, loss: 2.4529, current_lr: 0.000106
step 1330, loss: 2.4559, current_lr: 0.000106
step 1331, loss: 2.0538, current_lr: 0.000106
step 1332, loss: 2.4145, current_lr: 0.000107
step 1333, loss: 2.2676, current_lr: 0.000107
step 1334, loss: 2.4202, current_lr: 0.000107
step 1335, loss: 2.1564, current_lr: 0.000107
step 1336, loss: 2.2835, current_lr: 0.000107
step 1337, loss: 2.1710, current_lr: 0.000107
step 1338, loss: 2.4112, current_lr: 0.000107
step 1339, loss: 2.3747, current_lr: 0.000107
step 1340, loss: 2.5943, current_lr: 0.000108
step 1341, loss: 2.2890, current_lr: 0.000108
step 1342, loss: 2.3260, current_lr: 0.000108
step 1343, loss: 2.3918, current_lr: 0.000108
step 1344, loss: 2.1379, current_lr: 0.000108
step 1345, loss: 2.1347, current_lr: 0.000108
step 1346, loss: 2.2359, current_lr: 0.000108
step 1347, loss: 2.2018, current_lr: 0.000108
step 1348, loss: 2.2711, current_lr: 0.000108
step 1349, loss: 1.9937, current_lr: 0.000109
step 1350, loss: 2.2353, current_lr: 0.000109
step 1351, loss: 2.2166, current_lr: 0.000109
step 1352, loss: 2.1651, current_lr: 0.000109
step 1353, loss: 2.0698, current_lr: 0.000109
step 1354, loss: 2.1519, current_lr: 0.000109
step 1355, loss: 2.0419, current_lr: 0.000109
step 1356, loss: 2.3467, current_lr: 0.000109
step 1357, loss: 2.2999, current_lr: 0.000109
step 1358, loss: 2.2046, current_lr: 0.000110
step 1359, loss: 1.9994, current_lr: 0.000110
step 1360, loss: 2.3460, current_lr: 0.000110
step 1361, loss: 2.0249, current_lr: 0.000110
step 1362, loss: 1.9958, current_lr: 0.000110
step 1363, loss: 1.9580, current_lr: 0.000110
step 1364, loss: 2.1947, current_lr: 0.000110
step 1365, loss: 2.1975, current_lr: 0.000110
step 1366, loss: 2.2545, current_lr: 0.000110
step 1367, loss: 2.4765, current_lr: 0.000111
step 1368, loss: 2.3123, current_lr: 0.000111
step 1369, loss: 2.2631, current_lr: 0.000111
step 1370, loss: 2.2791, current_lr: 0.000111
step 1371, loss: 2.3867, current_lr: 0.000111
step 1372, loss: 2.3103, current_lr: 0.000111
step 1373, loss: 2.1918, current_lr: 0.000111
step 1374, loss: 2.1764, current_lr: 0.000111
step 1375, loss: 2.4014, current_lr: 0.000112
step 1376, loss: 2.3140, current_lr: 0.000112
step 1377, loss: 2.3187, current_lr: 0.000112
step 1378, loss: 2.2950, current_lr: 0.000112
step 1379, loss: 2.1537, current_lr: 0.000112
step 1380, loss: 2.1832, current_lr: 0.000112
step 1381, loss: 2.2167, current_lr: 0.000112
step 1382, loss: 1.9180, current_lr: 0.000112
step 1383, loss: 2.1968, current_lr: 0.000112
step 1384, loss: 2.0423, current_lr: 0.000113
step 1385, loss: 2.2397, current_lr: 0.000113
step 1386, loss: 2.3123, current_lr: 0.000113
step 1387, loss: 2.2638, current_lr: 0.000113
step 1388, loss: 2.4266, current_lr: 0.000113
step 1389, loss: 2.2056, current_lr: 0.000113
step 1390, loss: 2.0834, current_lr: 0.000113
step 1391, loss: 2.3274, current_lr: 0.000113
step 1392, loss: 2.0302, current_lr: 0.000114
step 1393, loss: 2.2036, current_lr: 0.000114
step 1394, loss: 2.0422, current_lr: 0.000114
step 1395, loss: 2.3570, current_lr: 0.000114
step 1396, loss: 2.1388, current_lr: 0.000114
step 1397, loss: 2.1772, current_lr: 0.000114
step 1398, loss: 2.0611, current_lr: 0.000114
step 1399, loss: 2.1092, current_lr: 0.000114
step 1400, loss: 2.1540, current_lr: 0.000114
step 1401, loss: 2.0511, current_lr: 0.000115
step 1402, loss: 2.2409, current_lr: 0.000115
step 1403, loss: 2.2817, current_lr: 0.000115
step 1404, loss: 2.2302, current_lr: 0.000115
step 1405, loss: 1.9926, current_lr: 0.000115
step 1406, loss: 2.0079, current_lr: 0.000115
step 1407, loss: 2.1568, current_lr: 0.000115
step 1408, loss: 2.2732, current_lr: 0.000115
step 1409, loss: 2.2867, current_lr: 0.000116
step 1410, loss: 2.0578, current_lr: 0.000116
step 1411, loss: 2.2542, current_lr: 0.000116
step 1412, loss: 2.2597, current_lr: 0.000116
step 1413, loss: 2.0559, current_lr: 0.000116
step 1414, loss: 1.8226, current_lr: 0.000116
Saved best model with loss: 1.8226 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.8226.pt
step 1415, loss: 2.1641, current_lr: 0.000116
step 1416, loss: 2.0834, current_lr: 0.000116
step 1417, loss: 2.1720, current_lr: 0.000116
step 1418, loss: 2.0439, current_lr: 0.000117
step 1419, loss: 1.9799, current_lr: 0.000117
step 1420, loss: 1.9947, current_lr: 0.000117
step 1421, loss: 2.1542, current_lr: 0.000117
step 1422, loss: 1.9950, current_lr: 0.000117
step 1423, loss: 2.1201, current_lr: 0.000117
step 1424, loss: 2.3772, current_lr: 0.000117
step 1425, loss: 2.4358, current_lr: 0.000117
step 1426, loss: 2.3765, current_lr: 0.000118
step 1427, loss: 2.2349, current_lr: 0.000118
step 1428, loss: 2.4316, current_lr: 0.000118
step 1429, loss: 2.2173, current_lr: 0.000118
step 1430, loss: 2.3261, current_lr: 0.000118
step 1431, loss: 2.3480, current_lr: 0.000118
step 1432, loss: 2.3174, current_lr: 0.000118
step 1433, loss: 2.2988, current_lr: 0.000118
step 1434, loss: 2.0974, current_lr: 0.000118
step 1435, loss: 2.2019, current_lr: 0.000119
step 1436, loss: 2.1377, current_lr: 0.000119
step 1437, loss: 2.3531, current_lr: 0.000119
step 1438, loss: 2.2382, current_lr: 0.000119
step 1439, loss: 2.0774, current_lr: 0.000119
step 1440, loss: 2.5204, current_lr: 0.000119
step 1441, loss: 2.3925, current_lr: 0.000119
step 1442, loss: 2.2507, current_lr: 0.000119
step 1443, loss: 2.4430, current_lr: 0.000120
step 1444, loss: 1.9762, current_lr: 0.000120
step 1445, loss: 2.4604, current_lr: 0.000120
step 1446, loss: 2.1689, current_lr: 0.000120
step 1447, loss: 1.7636, current_lr: 0.000120
Saved best model with loss: 1.7636 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.7636.pt
step 1448, loss: 2.0796, current_lr: 0.000120
step 1449, loss: 2.2632, current_lr: 0.000120
step 1450, loss: 2.2965, current_lr: 0.000120
step 1451, loss: 2.2768, current_lr: 0.000121
step 1452, loss: 2.2648, current_lr: 0.000121
step 1453, loss: 2.0162, current_lr: 0.000121
step 1454, loss: 2.3378, current_lr: 0.000121
step 1455, loss: 2.0084, current_lr: 0.000121
step 1456, loss: 1.9846, current_lr: 0.000121
step 1457, loss: 2.1886, current_lr: 0.000121
step 1458, loss: 2.1391, current_lr: 0.000121
step 1459, loss: 2.2198, current_lr: 0.000122
step 1460, loss: 2.1339, current_lr: 0.000122
step 1461, loss: 2.3242, current_lr: 0.000122
step 1462, loss: 2.5405, current_lr: 0.000122
step 1463, loss: 2.4176, current_lr: 0.000122
step 1464, loss: 2.3144, current_lr: 0.000122
step 1465, loss: 2.2463, current_lr: 0.000122
step 1466, loss: 2.0898, current_lr: 0.000122
step 1467, loss: 2.1565, current_lr: 0.000122
step 1468, loss: 1.9673, current_lr: 0.000123
step 1469, loss: 2.2427, current_lr: 0.000123
step 1470, loss: 2.1515, current_lr: 0.000123
step 1471, loss: 2.2187, current_lr: 0.000123
step 1472, loss: 2.0310, current_lr: 0.000123
step 1473, loss: 2.1482, current_lr: 0.000123
step 1474, loss: 2.0077, current_lr: 0.000123
step 1475, loss: 2.0073, current_lr: 0.000123
step 1476, loss: 2.0641, current_lr: 0.000124
step 1477, loss: 1.9612, current_lr: 0.000124
step 1478, loss: 1.7987, current_lr: 0.000124
step 1479, loss: 2.1703, current_lr: 0.000124
step 1480, loss: 2.4317, current_lr: 0.000124
step 1481, loss: 2.3850, current_lr: 0.000124
step 1482, loss: 2.4162, current_lr: 0.000124
step 1483, loss: 2.0614, current_lr: 0.000124
step 1484, loss: 2.0736, current_lr: 0.000125
step 1485, loss: 2.2905, current_lr: 0.000125
step 1486, loss: 2.2849, current_lr: 0.000125
step 1487, loss: 2.1055, current_lr: 0.000125
step 1488, loss: 2.3853, current_lr: 0.000125
step 1489, loss: 2.3915, current_lr: 0.000125
step 1490, loss: 2.1535, current_lr: 0.000125
step 1491, loss: 2.1312, current_lr: 0.000125
step 1492, loss: 2.2599, current_lr: 0.000126
step 1493, loss: 2.0533, current_lr: 0.000126
step 1494, loss: 2.2174, current_lr: 0.000126
step 1495, loss: 2.1927, current_lr: 0.000126
step 1496, loss: 1.8247, current_lr: 0.000126
step 1497, loss: 2.1817, current_lr: 0.000126
step 1498, loss: 1.9487, current_lr: 0.000126
step 1499, loss: 2.1418, current_lr: 0.000126
step 1500, loss: 1.8681, current_lr: 0.000127
step 1501, loss: 1.9863, current_lr: 0.000127
step 1502, loss: 1.8778, current_lr: 0.000127
step 1503, loss: 2.1423, current_lr: 0.000127
step 1504, loss: 2.1401, current_lr: 0.000127
step 1505, loss: 2.3646, current_lr: 0.000127
step 1506, loss: 2.0626, current_lr: 0.000127
step 1507, loss: 2.0674, current_lr: 0.000127
step 1508, loss: 2.1148, current_lr: 0.000128
step 1509, loss: 1.8834, current_lr: 0.000128
step 1510, loss: 1.8781, current_lr: 0.000128
step 1511, loss: 2.0138, current_lr: 0.000128
step 1512, loss: 1.9391, current_lr: 0.000128
step 1513, loss: 2.0351, current_lr: 0.000128
step 1514, loss: 1.7276, current_lr: 0.000128
Saved best model with loss: 1.7276 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.7276.pt
step 1515, loss: 1.9417, current_lr: 0.000128
step 1516, loss: 1.9171, current_lr: 0.000129
step 1517, loss: 1.8332, current_lr: 0.000129
step 1518, loss: 1.7096, current_lr: 0.000129
Saved best model with loss: 1.7096 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.7096.pt
step 1519, loss: 1.8163, current_lr: 0.000129
step 1520, loss: 1.7879, current_lr: 0.000129
step 1521, loss: 2.0235, current_lr: 0.000129
step 1522, loss: 1.9885, current_lr: 0.000129
step 1523, loss: 1.9457, current_lr: 0.000129
step 1524, loss: 1.7327, current_lr: 0.000130
step 1525, loss: 2.0272, current_lr: 0.000130
step 1526, loss: 1.7428, current_lr: 0.000130
step 1527, loss: 1.7425, current_lr: 0.000130
step 1528, loss: 1.6812, current_lr: 0.000130
Saved best model with loss: 1.6812 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.6812.pt
step 1529, loss: 1.8788, current_lr: 0.000130
step 1530, loss: 1.8507, current_lr: 0.000130
step 1531, loss: 1.9302, current_lr: 0.000130
step 1532, loss: 2.0789, current_lr: 0.000131
step 1533, loss: 2.0362, current_lr: 0.000131
step 1534, loss: 1.9595, current_lr: 0.000131
step 1535, loss: 2.0060, current_lr: 0.000131
step 1536, loss: 2.0377, current_lr: 0.000131
step 1537, loss: 1.9410, current_lr: 0.000131
step 1538, loss: 1.8632, current_lr: 0.000131
step 1539, loss: 1.8978, current_lr: 0.000131
step 1540, loss: 2.0653, current_lr: 0.000132
step 1541, loss: 1.9756, current_lr: 0.000132
step 1542, loss: 1.9556, current_lr: 0.000132
step 1543, loss: 1.8667, current_lr: 0.000132
step 1544, loss: 1.8069, current_lr: 0.000132
step 1545, loss: 1.8938, current_lr: 0.000132
step 1546, loss: 1.9813, current_lr: 0.000132
step 1547, loss: 1.6624, current_lr: 0.000132
Saved best model with loss: 1.6624 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.6624.pt
step 1548, loss: 1.9514, current_lr: 0.000133
step 1549, loss: 1.7582, current_lr: 0.000133
step 1550, loss: 1.9631, current_lr: 0.000133
step 1551, loss: 1.9948, current_lr: 0.000133
step 1552, loss: 1.9416, current_lr: 0.000133
step 1553, loss: 2.0705, current_lr: 0.000133
step 1554, loss: 1.9399, current_lr: 0.000133
step 1555, loss: 1.8437, current_lr: 0.000134
step 1556, loss: 2.0324, current_lr: 0.000134
step 1557, loss: 1.8454, current_lr: 0.000134
step 1558, loss: 2.0422, current_lr: 0.000134
step 1559, loss: 1.8576, current_lr: 0.000134
step 1560, loss: 2.1186, current_lr: 0.000134
step 1561, loss: 1.9115, current_lr: 0.000134
step 1562, loss: 1.8953, current_lr: 0.000134
step 1563, loss: 1.8389, current_lr: 0.000135
step 1564, loss: 1.8727, current_lr: 0.000135
step 1565, loss: 1.9680, current_lr: 0.000135
step 1566, loss: 1.8801, current_lr: 0.000135
step 1567, loss: 2.1139, current_lr: 0.000135
step 1568, loss: 2.1014, current_lr: 0.000135
step 1569, loss: 2.0238, current_lr: 0.000135
step 1570, loss: 1.8039, current_lr: 0.000135
step 1571, loss: 1.8062, current_lr: 0.000136
step 1572, loss: 1.9793, current_lr: 0.000136
step 1573, loss: 2.0520, current_lr: 0.000136
step 1574, loss: 2.0394, current_lr: 0.000136
step 1575, loss: 1.8674, current_lr: 0.000136
step 1576, loss: 1.9917, current_lr: 0.000136
step 1577, loss: 1.9615, current_lr: 0.000136
step 1578, loss: 1.8206, current_lr: 0.000136
step 1579, loss: 1.5650, current_lr: 0.000137
Saved best model with loss: 1.5650 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.5650.pt
step 1580, loss: 1.8344, current_lr: 0.000137
step 1581, loss: 1.8174, current_lr: 0.000137
step 1582, loss: 1.8482, current_lr: 0.000137
step 1583, loss: 1.8104, current_lr: 0.000137
step 1584, loss: 1.7384, current_lr: 0.000137
step 1585, loss: 1.7276, current_lr: 0.000137
step 1586, loss: 1.8935, current_lr: 0.000138
step 1587, loss: 1.6996, current_lr: 0.000138
step 1588, loss: 1.8215, current_lr: 0.000138
step 1589, loss: 2.0642, current_lr: 0.000138
step 1590, loss: 2.1847, current_lr: 0.000138
step 1591, loss: 2.0685, current_lr: 0.000138
step 1592, loss: 2.0139, current_lr: 0.000138
step 1593, loss: 2.1749, current_lr: 0.000138
step 1594, loss: 2.0227, current_lr: 0.000139
step 1595, loss: 2.0502, current_lr: 0.000139
step 1596, loss: 2.0697, current_lr: 0.000139
step 1597, loss: 2.0363, current_lr: 0.000139
step 1598, loss: 2.0905, current_lr: 0.000139
step 1599, loss: 1.9600, current_lr: 0.000139
step 1600, loss: 1.9556, current_lr: 0.000139
step 1601, loss: 1.9062, current_lr: 0.000139
step 1602, loss: 2.0854, current_lr: 0.000140
step 1603, loss: 1.8857, current_lr: 0.000140
step 1604, loss: 1.7651, current_lr: 0.000140
step 1605, loss: 2.1824, current_lr: 0.000140
step 1606, loss: 2.0966, current_lr: 0.000140
step 1607, loss: 2.0010, current_lr: 0.000140
step 1608, loss: 2.1020, current_lr: 0.000140
step 1609, loss: 1.6793, current_lr: 0.000141
step 1610, loss: 2.1669, current_lr: 0.000141
step 1611, loss: 1.8929, current_lr: 0.000141
step 1612, loss: 1.5284, current_lr: 0.000141
Saved best model with loss: 1.5284 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.5284.pt
step 1613, loss: 1.8240, current_lr: 0.000141
step 1614, loss: 1.9512, current_lr: 0.000141
step 1615, loss: 2.0410, current_lr: 0.000141
step 1616, loss: 2.0011, current_lr: 0.000141
step 1617, loss: 1.9369, current_lr: 0.000142
step 1618, loss: 1.7892, current_lr: 0.000142
step 1619, loss: 2.0243, current_lr: 0.000142
step 1620, loss: 1.7721, current_lr: 0.000142
step 1621, loss: 1.7164, current_lr: 0.000142
step 1622, loss: 1.9174, current_lr: 0.000142
step 1623, loss: 1.8457, current_lr: 0.000142
step 1624, loss: 1.9635, current_lr: 0.000142
step 1625, loss: 1.8404, current_lr: 0.000143
step 1626, loss: 1.9519, current_lr: 0.000143
step 1627, loss: 2.1218, current_lr: 0.000143
step 1628, loss: 2.1018, current_lr: 0.000143
step 1629, loss: 2.0851, current_lr: 0.000143
step 1630, loss: 1.9879, current_lr: 0.000143
step 1631, loss: 1.9361, current_lr: 0.000143
step 1632, loss: 1.9510, current_lr: 0.000144
step 1633, loss: 1.7517, current_lr: 0.000144
step 1634, loss: 2.0108, current_lr: 0.000144
step 1635, loss: 1.8694, current_lr: 0.000144
step 1636, loss: 1.9870, current_lr: 0.000144
step 1637, loss: 1.7729, current_lr: 0.000144
step 1638, loss: 1.8864, current_lr: 0.000144
step 1639, loss: 1.7616, current_lr: 0.000144
step 1640, loss: 1.7736, current_lr: 0.000145
step 1641, loss: 1.8295, current_lr: 0.000145
step 1642, loss: 1.7207, current_lr: 0.000145
step 1643, loss: 1.5916, current_lr: 0.000145
step 1644, loss: 1.9261, current_lr: 0.000145
step 1645, loss: 2.1630, current_lr: 0.000145
step 1646, loss: 2.0806, current_lr: 0.000145
step 1647, loss: 2.1350, current_lr: 0.000146
step 1648, loss: 1.8409, current_lr: 0.000146
step 1649, loss: 1.7935, current_lr: 0.000146
step 1650, loss: 2.0149, current_lr: 0.000146
step 1651, loss: 1.9437, current_lr: 0.000146
step 1652, loss: 1.7755, current_lr: 0.000146
step 1653, loss: 1.9912, current_lr: 0.000146
step 1654, loss: 2.0196, current_lr: 0.000146
step 1655, loss: 1.8868, current_lr: 0.000147
step 1656, loss: 1.8694, current_lr: 0.000147
step 1657, loss: 1.9547, current_lr: 0.000147
step 1658, loss: 1.7747, current_lr: 0.000147
step 1659, loss: 1.9063, current_lr: 0.000147
step 1660, loss: 1.9348, current_lr: 0.000147
step 1661, loss: 1.5947, current_lr: 0.000147
step 1662, loss: 1.9302, current_lr: 0.000148
step 1663, loss: 1.7298, current_lr: 0.000148
step 1664, loss: 1.8275, current_lr: 0.000148
step 1665, loss: 1.6584, current_lr: 0.000148
step 1666, loss: 1.7896, current_lr: 0.000148
step 1667, loss: 1.6471, current_lr: 0.000148
step 1668, loss: 1.8862, current_lr: 0.000148
step 1669, loss: 1.8914, current_lr: 0.000149
step 1670, loss: 2.0438, current_lr: 0.000149
step 1671, loss: 1.7701, current_lr: 0.000149
step 1672, loss: 1.7888, current_lr: 0.000149
step 1673, loss: 1.8670, current_lr: 0.000149
step 1674, loss: 1.6432, current_lr: 0.000149
step 1675, loss: 1.6636, current_lr: 0.000149
step 1676, loss: 1.7825, current_lr: 0.000149
step 1677, loss: 1.6974, current_lr: 0.000150
step 1678, loss: 1.7679, current_lr: 0.000150
step 1679, loss: 1.5708, current_lr: 0.000150
step 1680, loss: 1.7098, current_lr: 0.000150
step 1681, loss: 1.7710, current_lr: 0.000150
step 1682, loss: 1.7097, current_lr: 0.000150
step 1683, loss: 1.5304, current_lr: 0.000150
step 1684, loss: 1.6328, current_lr: 0.000151
step 1685, loss: 1.5822, current_lr: 0.000151
step 1686, loss: 1.7723, current_lr: 0.000151
step 1687, loss: 1.6967, current_lr: 0.000151
step 1688, loss: 1.6908, current_lr: 0.000151
step 1689, loss: 1.4883, current_lr: 0.000151
Saved best model with loss: 1.4883 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4883.pt
step 1690, loss: 1.7269, current_lr: 0.000151
step 1691, loss: 1.4970, current_lr: 0.000151
step 1692, loss: 1.5400, current_lr: 0.000152
step 1693, loss: 1.5026, current_lr: 0.000152
step 1694, loss: 1.6571, current_lr: 0.000152
step 1695, loss: 1.6408, current_lr: 0.000152
step 1696, loss: 1.6668, current_lr: 0.000152
step 1697, loss: 1.9156, current_lr: 0.000152
step 1698, loss: 1.7716, current_lr: 0.000152
step 1699, loss: 1.6961, current_lr: 0.000153
step 1700, loss: 1.6866, current_lr: 0.000153
step 1701, loss: 1.7238, current_lr: 0.000153
step 1702, loss: 1.6459, current_lr: 0.000153
step 1703, loss: 1.6188, current_lr: 0.000153
step 1704, loss: 1.6159, current_lr: 0.000153
step 1705, loss: 1.7689, current_lr: 0.000153
step 1706, loss: 1.6799, current_lr: 0.000154
step 1707, loss: 1.6717, current_lr: 0.000154
step 1708, loss: 1.6503, current_lr: 0.000154
step 1709, loss: 1.5172, current_lr: 0.000154
step 1710, loss: 1.5845, current_lr: 0.000154
step 1711, loss: 1.6129, current_lr: 0.000154
step 1712, loss: 1.4620, current_lr: 0.000154
Saved best model with loss: 1.4620 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4620.pt
step 1713, loss: 1.6639, current_lr: 0.000155
step 1714, loss: 1.5352, current_lr: 0.000155
step 1715, loss: 1.7021, current_lr: 0.000155
step 1716, loss: 1.7266, current_lr: 0.000155
step 1717, loss: 1.6992, current_lr: 0.000155
step 1718, loss: 1.7548, current_lr: 0.000155
step 1719, loss: 1.7507, current_lr: 0.000155
step 1720, loss: 1.6859, current_lr: 0.000155
step 1721, loss: 1.8198, current_lr: 0.000156
step 1722, loss: 1.6003, current_lr: 0.000156
step 1723, loss: 1.7009, current_lr: 0.000156
step 1724, loss: 1.5105, current_lr: 0.000156
step 1725, loss: 1.6995, current_lr: 0.000156
step 1726, loss: 1.6407, current_lr: 0.000156
step 1727, loss: 1.6809, current_lr: 0.000156
step 1728, loss: 1.6488, current_lr: 0.000157
step 1729, loss: 1.6188, current_lr: 0.000157
step 1730, loss: 1.6843, current_lr: 0.000157
step 1731, loss: 1.5381, current_lr: 0.000157
step 1732, loss: 1.7302, current_lr: 0.000157
step 1733, loss: 1.7114, current_lr: 0.000157
step 1734, loss: 1.6755, current_lr: 0.000157
step 1735, loss: 1.5679, current_lr: 0.000158
step 1736, loss: 1.5879, current_lr: 0.000158
step 1737, loss: 1.7154, current_lr: 0.000158
step 1738, loss: 1.7359, current_lr: 0.000158
step 1739, loss: 1.7360, current_lr: 0.000158
step 1740, loss: 1.5864, current_lr: 0.000158
step 1741, loss: 1.6777, current_lr: 0.000158
step 1742, loss: 1.7035, current_lr: 0.000159
step 1743, loss: 1.5225, current_lr: 0.000159
step 1744, loss: 1.3568, current_lr: 0.000159
Saved best model with loss: 1.3568 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.3568.pt
step 1745, loss: 1.5757, current_lr: 0.000159
step 1746, loss: 1.5518, current_lr: 0.000159
step 1747, loss: 1.6210, current_lr: 0.000159
step 1748, loss: 1.5437, current_lr: 0.000159
step 1749, loss: 1.4828, current_lr: 0.000159
step 1750, loss: 1.4436, current_lr: 0.000160
step 1751, loss: 1.5566, current_lr: 0.000160
step 1752, loss: 1.4610, current_lr: 0.000160
step 1753, loss: 1.5247, current_lr: 0.000160
step 1754, loss: 1.7163, current_lr: 0.000160
step 1755, loss: 1.8770, current_lr: 0.000160
step 1756, loss: 1.7992, current_lr: 0.000160
step 1757, loss: 1.7254, current_lr: 0.000161
step 1758, loss: 1.9419, current_lr: 0.000161
step 1759, loss: 1.8419, current_lr: 0.000161
step 1760, loss: 1.8699, current_lr: 0.000161
step 1761, loss: 1.8414, current_lr: 0.000161
step 1762, loss: 1.8541, current_lr: 0.000161
step 1763, loss: 1.7970, current_lr: 0.000161
step 1764, loss: 1.6796, current_lr: 0.000162
step 1765, loss: 1.6443, current_lr: 0.000162
step 1766, loss: 1.5855, current_lr: 0.000162
step 1767, loss: 1.8003, current_lr: 0.000162
step 1768, loss: 1.6042, current_lr: 0.000162
step 1769, loss: 1.5509, current_lr: 0.000162
step 1770, loss: 1.9312, current_lr: 0.000162
step 1771, loss: 1.8198, current_lr: 0.000163
step 1772, loss: 1.7269, current_lr: 0.000163
step 1773, loss: 1.7750, current_lr: 0.000163
step 1774, loss: 1.4582, current_lr: 0.000163
step 1775, loss: 1.8636, current_lr: 0.000163
step 1776, loss: 1.5759, current_lr: 0.000163
step 1777, loss: 1.2954, current_lr: 0.000163
Saved best model with loss: 1.2954 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.2954.pt
step 1778, loss: 1.6025, current_lr: 0.000164
step 1779, loss: 1.6846, current_lr: 0.000164
step 1780, loss: 1.7653, current_lr: 0.000164
step 1781, loss: 1.7214, current_lr: 0.000164
step 1782, loss: 1.6266, current_lr: 0.000164
step 1783, loss: 1.5082, current_lr: 0.000164
step 1784, loss: 1.7116, current_lr: 0.000164
step 1785, loss: 1.4882, current_lr: 0.000165
step 1786, loss: 1.4625, current_lr: 0.000165
step 1787, loss: 1.5983, current_lr: 0.000165
step 1788, loss: 1.5815, current_lr: 0.000165
step 1789, loss: 1.6290, current_lr: 0.000165
step 1790, loss: 1.5934, current_lr: 0.000165
step 1791, loss: 1.6491, current_lr: 0.000165
step 1792, loss: 1.7647, current_lr: 0.000166
step 1793, loss: 1.7480, current_lr: 0.000166
step 1794, loss: 1.7179, current_lr: 0.000166
step 1795, loss: 1.6525, current_lr: 0.000166
step 1796, loss: 1.6007, current_lr: 0.000166
step 1797, loss: 1.6019, current_lr: 0.000166
step 1798, loss: 1.4324, current_lr: 0.000166
step 1799, loss: 1.6918, current_lr: 0.000167
step 1800, loss: 1.5996, current_lr: 0.000167
step 1801, loss: 1.6666, current_lr: 0.000167
step 1802, loss: 1.5537, current_lr: 0.000167
step 1803, loss: 1.6706, current_lr: 0.000167
step 1804, loss: 1.5759, current_lr: 0.000167
step 1805, loss: 1.5210, current_lr: 0.000167
step 1806, loss: 1.5625, current_lr: 0.000168
step 1807, loss: 1.4948, current_lr: 0.000168
step 1808, loss: 1.3113, current_lr: 0.000168
step 1809, loss: 1.6056, current_lr: 0.000168
step 1810, loss: 1.8237, current_lr: 0.000168
step 1811, loss: 1.7593, current_lr: 0.000168
step 1812, loss: 1.8041, current_lr: 0.000168
step 1813, loss: 1.6355, current_lr: 0.000169
step 1814, loss: 1.5605, current_lr: 0.000169
step 1815, loss: 1.7414, current_lr: 0.000169
step 1816, loss: 1.6810, current_lr: 0.000169
step 1817, loss: 1.5527, current_lr: 0.000169
step 1818, loss: 1.6619, current_lr: 0.000169
step 1819, loss: 1.7062, current_lr: 0.000169
step 1820, loss: 1.5220, current_lr: 0.000170
step 1821, loss: 1.5509, current_lr: 0.000170
step 1822, loss: 1.6550, current_lr: 0.000170
step 1823, loss: 1.5454, current_lr: 0.000170
step 1824, loss: 1.6281, current_lr: 0.000170
step 1825, loss: 1.6169, current_lr: 0.000170
step 1826, loss: 1.3807, current_lr: 0.000170
step 1827, loss: 1.6487, current_lr: 0.000171
step 1828, loss: 1.4658, current_lr: 0.000171
step 1829, loss: 1.5729, current_lr: 0.000171
step 1830, loss: 1.4505, current_lr: 0.000171
step 1831, loss: 1.5532, current_lr: 0.000171
step 1832, loss: 1.4098, current_lr: 0.000171
step 1833, loss: 1.5922, current_lr: 0.000171
step 1834, loss: 1.6040, current_lr: 0.000172
step 1835, loss: 1.7375, current_lr: 0.000172
step 1836, loss: 1.5594, current_lr: 0.000172
step 1837, loss: 1.5777, current_lr: 0.000172
step 1838, loss: 1.6310, current_lr: 0.000172
step 1839, loss: 1.4429, current_lr: 0.000172
step 1840, loss: 1.4207, current_lr: 0.000172
step 1841, loss: 1.5568, current_lr: 0.000173
step 1842, loss: 1.4579, current_lr: 0.000173
step 1843, loss: 1.5384, current_lr: 0.000173
step 1844, loss: 1.3785, current_lr: 0.000173
step 1845, loss: 1.5185, current_lr: 0.000173
step 1846, loss: 1.5159, current_lr: 0.000173
step 1847, loss: 1.4317, current_lr: 0.000173
step 1848, loss: 1.3059, current_lr: 0.000174
step 1849, loss: 1.4193, current_lr: 0.000174
step 1850, loss: 1.4156, current_lr: 0.000174
step 1851, loss: 1.5165, current_lr: 0.000174
step 1852, loss: 1.3920, current_lr: 0.000174
step 1853, loss: 1.4218, current_lr: 0.000174
step 1854, loss: 1.2496, current_lr: 0.000174
Saved best model with loss: 1.2496 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.2496.pt
step 1855, loss: 1.4743, current_lr: 0.000175
step 1856, loss: 1.2925, current_lr: 0.000175
step 1857, loss: 1.3525, current_lr: 0.000175
step 1858, loss: 1.3066, current_lr: 0.000175
step 1859, loss: 1.3900, current_lr: 0.000175
step 1860, loss: 1.4158, current_lr: 0.000175
step 1861, loss: 1.3751, current_lr: 0.000175
step 1862, loss: 1.5358, current_lr: 0.000176
step 1863, loss: 1.4346, current_lr: 0.000176
step 1864, loss: 1.4508, current_lr: 0.000176
step 1865, loss: 1.4107, current_lr: 0.000176
step 1866, loss: 1.4717, current_lr: 0.000176
step 1867, loss: 1.4882, current_lr: 0.000176
step 1868, loss: 1.4477, current_lr: 0.000176
step 1869, loss: 1.3692, current_lr: 0.000177
step 1870, loss: 1.4546, current_lr: 0.000177
step 1871, loss: 1.3723, current_lr: 0.000177
step 1872, loss: 1.4311, current_lr: 0.000177
step 1873, loss: 1.3637, current_lr: 0.000177
step 1874, loss: 1.2855, current_lr: 0.000177
step 1875, loss: 1.3335, current_lr: 0.000178
step 1876, loss: 1.3816, current_lr: 0.000178
step 1877, loss: 1.2303, current_lr: 0.000178
Saved best model with loss: 1.2303 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.2303.pt
step 1878, loss: 1.4117, current_lr: 0.000178
step 1879, loss: 1.3119, current_lr: 0.000178
step 1880, loss: 1.4024, current_lr: 0.000178
step 1881, loss: 1.4245, current_lr: 0.000178
step 1882, loss: 1.3597, current_lr: 0.000179
step 1883, loss: 1.4195, current_lr: 0.000179
step 1884, loss: 1.3902, current_lr: 0.000179
step 1885, loss: 1.3339, current_lr: 0.000179
step 1886, loss: 1.4914, current_lr: 0.000179
step 1887, loss: 1.3556, current_lr: 0.000179
step 1888, loss: 1.4755, current_lr: 0.000179
step 1889, loss: 1.3044, current_lr: 0.000180
step 1890, loss: 1.4639, current_lr: 0.000180
step 1891, loss: 1.3871, current_lr: 0.000180
step 1892, loss: 1.4042, current_lr: 0.000180
step 1893, loss: 1.3886, current_lr: 0.000180
step 1894, loss: 1.3714, current_lr: 0.000180
step 1895, loss: 1.4407, current_lr: 0.000180
step 1896, loss: 1.3176, current_lr: 0.000181
step 1897, loss: 1.3543, current_lr: 0.000181
step 1898, loss: 1.4203, current_lr: 0.000181
step 1899, loss: 1.3663, current_lr: 0.000181
step 1900, loss: 1.3210, current_lr: 0.000181
step 1901, loss: 1.2881, current_lr: 0.000181
step 1902, loss: 1.3771, current_lr: 0.000181
step 1903, loss: 1.4173, current_lr: 0.000182
step 1904, loss: 1.4508, current_lr: 0.000182
step 1905, loss: 1.3290, current_lr: 0.000182
step 1906, loss: 1.4022, current_lr: 0.000182
step 1907, loss: 1.3828, current_lr: 0.000182
step 1908, loss: 1.2529, current_lr: 0.000182
step 1909, loss: 1.0932, current_lr: 0.000183
Saved best model with loss: 1.0932 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.0932.pt
step 1910, loss: 1.2970, current_lr: 0.000183
step 1911, loss: 1.2857, current_lr: 0.000183
step 1912, loss: 1.2885, current_lr: 0.000183
step 1913, loss: 1.2528, current_lr: 0.000183
step 1914, loss: 1.2021, current_lr: 0.000183
step 1915, loss: 1.2191, current_lr: 0.000183
step 1916, loss: 1.3476, current_lr: 0.000184
step 1917, loss: 1.2617, current_lr: 0.000184
step 1918, loss: 1.2770, current_lr: 0.000184
step 1919, loss: 1.4869, current_lr: 0.000184
step 1920, loss: 1.4889, current_lr: 0.000184
step 1921, loss: 1.3920, current_lr: 0.000184
step 1922, loss: 1.3859, current_lr: 0.000184
step 1923, loss: 1.5697, current_lr: 0.000185
step 1924, loss: 1.4962, current_lr: 0.000185
step 1925, loss: 1.5687, current_lr: 0.000185
step 1926, loss: 1.5633, current_lr: 0.000185
step 1927, loss: 1.5185, current_lr: 0.000185
step 1928, loss: 1.4485, current_lr: 0.000185
step 1929, loss: 1.4009, current_lr: 0.000185
step 1930, loss: 1.3327, current_lr: 0.000186
step 1931, loss: 1.3312, current_lr: 0.000186
step 1932, loss: 1.6023, current_lr: 0.000186
step 1933, loss: 1.3492, current_lr: 0.000186
step 1934, loss: 1.3208, current_lr: 0.000186
step 1935, loss: 1.6254, current_lr: 0.000186
step 1936, loss: 1.5169, current_lr: 0.000187
step 1937, loss: 1.4175, current_lr: 0.000187
step 1938, loss: 1.5430, current_lr: 0.000187
step 1939, loss: 1.1878, current_lr: 0.000187
step 1940, loss: 1.5025, current_lr: 0.000187
step 1941, loss: 1.3816, current_lr: 0.000187
step 1942, loss: 1.1230, current_lr: 0.000187
step 1943, loss: 1.3533, current_lr: 0.000188
step 1944, loss: 1.4794, current_lr: 0.000188
step 1945, loss: 1.4032, current_lr: 0.000188
step 1946, loss: 1.3511, current_lr: 0.000188
step 1947, loss: 1.3136, current_lr: 0.000188
step 1948, loss: 1.2161, current_lr: 0.000188
step 1949, loss: 1.3865, current_lr: 0.000188
step 1950, loss: 1.2337, current_lr: 0.000189
step 1951, loss: 1.2225, current_lr: 0.000189
step 1952, loss: 1.3103, current_lr: 0.000189
step 1953, loss: 1.3433, current_lr: 0.000189
step 1954, loss: 1.3506, current_lr: 0.000189
step 1955, loss: 1.2683, current_lr: 0.000189
step 1956, loss: 1.3398, current_lr: 0.000190
step 1957, loss: 1.3725, current_lr: 0.000190
step 1958, loss: 1.3931, current_lr: 0.000190
step 1959, loss: 1.3981, current_lr: 0.000190
step 1960, loss: 1.3562, current_lr: 0.000190
step 1961, loss: 1.3103, current_lr: 0.000190
step 1962, loss: 1.3940, current_lr: 0.000190
step 1963, loss: 1.2380, current_lr: 0.000191
step 1964, loss: 1.4028, current_lr: 0.000191
step 1965, loss: 1.2634, current_lr: 0.000191
step 1966, loss: 1.3093, current_lr: 0.000191
step 1967, loss: 1.1994, current_lr: 0.000191
step 1968, loss: 1.3551, current_lr: 0.000191
step 1969, loss: 1.2619, current_lr: 0.000191
step 1970, loss: 1.2790, current_lr: 0.000192
step 1971, loss: 1.3900, current_lr: 0.000192
step 1972, loss: 1.2594, current_lr: 0.000192
step 1973, loss: 1.1559, current_lr: 0.000192
step 1974, loss: 1.3182, current_lr: 0.000192
step 1975, loss: 1.4926, current_lr: 0.000192
step 1976, loss: 1.3737, current_lr: 0.000193
step 1977, loss: 1.4630, current_lr: 0.000193
step 1978, loss: 1.3767, current_lr: 0.000193
step 1979, loss: 1.2941, current_lr: 0.000193
step 1980, loss: 1.4152, current_lr: 0.000193
step 1981, loss: 1.3587, current_lr: 0.000193
step 1982, loss: 1.3373, current_lr: 0.000193
step 1983, loss: 1.3712, current_lr: 0.000194
step 1984, loss: 1.3969, current_lr: 0.000194
step 1985, loss: 1.2322, current_lr: 0.000194
step 1986, loss: 1.2606, current_lr: 0.000194
step 1987, loss: 1.3088, current_lr: 0.000194
step 1988, loss: 1.2536, current_lr: 0.000194
step 1989, loss: 1.2804, current_lr: 0.000195
step 1990, loss: 1.3131, current_lr: 0.000195
step 1991, loss: 1.1336, current_lr: 0.000195
step 1992, loss: 1.3018, current_lr: 0.000195
step 1993, loss: 1.1846, current_lr: 0.000195
step 1994, loss: 1.2712, current_lr: 0.000195
step 1995, loss: 1.1675, current_lr: 0.000195
step 1996, loss: 1.2284, current_lr: 0.000196
step 1997, loss: 1.1056, current_lr: 0.000196
step 1998, loss: 1.3126, current_lr: 0.000196
step 1999, loss: 1.2795, current_lr: 0.000196
step 2000, loss: 1.4515, current_lr: 0.000196
step 2001, loss: 1.2735, current_lr: 0.000196
step 2002, loss: 1.2717, current_lr: 0.000197
step 2003, loss: 1.3013, current_lr: 0.000197
step 2004, loss: 1.1271, current_lr: 0.000197
step 2005, loss: 1.1661, current_lr: 0.000197
step 2006, loss: 1.3247, current_lr: 0.000197
step 2007, loss: 1.2403, current_lr: 0.000197
step 2008, loss: 1.2949, current_lr: 0.000197
step 2009, loss: 1.1739, current_lr: 0.000198
step 2010, loss: 1.3079, current_lr: 0.000198
step 2011, loss: 1.3180, current_lr: 0.000198
step 2012, loss: 1.2234, current_lr: 0.000198
step 2013, loss: 1.1244, current_lr: 0.000198
step 2014, loss: 1.2368, current_lr: 0.000198
step 2015, loss: 1.1888, current_lr: 0.000198
step 2016, loss: 1.2825, current_lr: 0.000199
step 2017, loss: 1.2200, current_lr: 0.000199
step 2018, loss: 1.2148, current_lr: 0.000199
step 2019, loss: 1.0484, current_lr: 0.000199
Saved best model with loss: 1.0484 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.0484.pt
step 2020, loss: 1.1224, current_lr: 0.000199
step 2021, loss: 1.0218, current_lr: 0.000199
Saved best model with loss: 1.0218 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.0218.pt
step 2022, loss: 0.9541, current_lr: 0.000200
Saved best model with loss: 0.9541 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.9541.pt
step 2023, loss: 1.0054, current_lr: 0.000200
step 2024, loss: 1.1110, current_lr: 0.000200
step 2025, loss: 1.1755, current_lr: 0.000200
step 2026, loss: 1.1420, current_lr: 0.000200
step 2027, loss: 1.2607, current_lr: 0.000200
step 2028, loss: 1.2297, current_lr: 0.000200
step 2029, loss: 1.1905, current_lr: 0.000201
step 2030, loss: 1.0863, current_lr: 0.000201
step 2031, loss: 1.1516, current_lr: 0.000201
step 2032, loss: 1.1187, current_lr: 0.000201
step 2033, loss: 1.1408, current_lr: 0.000201
step 2034, loss: 1.1691, current_lr: 0.000201
step 2035, loss: 1.2399, current_lr: 0.000202
step 2036, loss: 1.1675, current_lr: 0.000202
step 2037, loss: 1.1573, current_lr: 0.000202
step 2038, loss: 1.1051, current_lr: 0.000202
step 2039, loss: 0.9815, current_lr: 0.000202
step 2040, loss: 1.0641, current_lr: 0.000202
step 2041, loss: 1.0705, current_lr: 0.000202
step 2042, loss: 0.9561, current_lr: 0.000203
step 2043, loss: 1.0824, current_lr: 0.000203
step 2044, loss: 1.0286, current_lr: 0.000203
step 2045, loss: 1.0995, current_lr: 0.000203
step 2046, loss: 1.1107, current_lr: 0.000203
step 2047, loss: 1.1421, current_lr: 0.000203
step 2048, loss: 1.1736, current_lr: 0.000204
step 2049, loss: 1.1456, current_lr: 0.000204
step 2050, loss: 1.0390, current_lr: 0.000204
step 2051, loss: 1.0904, current_lr: 0.000204
step 2052, loss: 0.9976, current_lr: 0.000204
step 2053, loss: 1.0932, current_lr: 0.000204
step 2054, loss: 1.0674, current_lr: 0.000205
step 2055, loss: 1.1270, current_lr: 0.000205
step 2056, loss: 1.1103, current_lr: 0.000205
step 2057, loss: 1.1192, current_lr: 0.000205
step 2058, loss: 1.1487, current_lr: 0.000205
step 2059, loss: 1.0647, current_lr: 0.000205
step 2060, loss: 1.1608, current_lr: 0.000205
step 2061, loss: 1.0912, current_lr: 0.000206
step 2062, loss: 1.1715, current_lr: 0.000206
step 2063, loss: 1.2259, current_lr: 0.000206
step 2064, loss: 1.2175, current_lr: 0.000206
step 2065, loss: 1.0973, current_lr: 0.000206
step 2066, loss: 1.1129, current_lr: 0.000206
step 2067, loss: 1.1232, current_lr: 0.000207
step 2068, loss: 1.1901, current_lr: 0.000207
step 2069, loss: 1.1535, current_lr: 0.000207
step 2070, loss: 1.1184, current_lr: 0.000207
step 2071, loss: 1.1105, current_lr: 0.000207
step 2072, loss: 1.1725, current_lr: 0.000207
step 2073, loss: 1.1242, current_lr: 0.000207
step 2074, loss: 1.0267, current_lr: 0.000208
step 2075, loss: 1.1057, current_lr: 0.000208
step 2076, loss: 1.1438, current_lr: 0.000208
step 2077, loss: 1.1082, current_lr: 0.000208
step 2078, loss: 1.1110, current_lr: 0.000208
step 2079, loss: 1.0884, current_lr: 0.000208
step 2080, loss: 1.0035, current_lr: 0.000209
step 2081, loss: 1.0749, current_lr: 0.000209
step 2082, loss: 1.0551, current_lr: 0.000209
step 2083, loss: 0.9872, current_lr: 0.000209
step 2084, loss: 1.1890, current_lr: 0.000209
step 2085, loss: 1.2083, current_lr: 0.000209
step 2086, loss: 1.1987, current_lr: 0.000209
step 2087, loss: 1.1894, current_lr: 0.000210
step 2088, loss: 1.3244, current_lr: 0.000210
step 2089, loss: 1.2826, current_lr: 0.000210
step 2090, loss: 1.2841, current_lr: 0.000210
step 2091, loss: 1.2057, current_lr: 0.000210
step 2092, loss: 1.2479, current_lr: 0.000210
step 2093, loss: 1.2330, current_lr: 0.000211
step 2094, loss: 1.1274, current_lr: 0.000211
step 2095, loss: 1.1751, current_lr: 0.000211
step 2096, loss: 1.0816, current_lr: 0.000211
step 2097, loss: 1.3003, current_lr: 0.000211
step 2098, loss: 1.0974, current_lr: 0.000211
step 2099, loss: 1.0870, current_lr: 0.000212
step 2100, loss: 1.2946, current_lr: 0.000212
step 2101, loss: 1.1677, current_lr: 0.000212
step 2102, loss: 1.1098, current_lr: 0.000212
step 2103, loss: 1.2184, current_lr: 0.000212
step 2104, loss: 0.9727, current_lr: 0.000212
step 2105, loss: 1.1785, current_lr: 0.000212
step 2106, loss: 1.0561, current_lr: 0.000213
step 2107, loss: 0.9360, current_lr: 0.000213
Saved best model with loss: 0.9360 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.9360.pt
step 2108, loss: 1.0884, current_lr: 0.000213
step 2109, loss: 1.1259, current_lr: 0.000213
step 2110, loss: 1.1399, current_lr: 0.000213
step 2111, loss: 1.1845, current_lr: 0.000213
step 2112, loss: 1.1645, current_lr: 0.000214
step 2113, loss: 1.0900, current_lr: 0.000214
step 2114, loss: 1.1852, current_lr: 0.000214
step 2115, loss: 0.9937, current_lr: 0.000214
step 2116, loss: 0.9834, current_lr: 0.000214
step 2117, loss: 1.0817, current_lr: 0.000214
step 2118, loss: 1.0757, current_lr: 0.000215
step 2119, loss: 1.1130, current_lr: 0.000215
step 2120, loss: 1.0626, current_lr: 0.000215
step 2121, loss: 1.1551, current_lr: 0.000215
step 2122, loss: 1.0781, current_lr: 0.000215
step 2123, loss: 1.1083, current_lr: 0.000215
step 2124, loss: 1.1463, current_lr: 0.000215
step 2125, loss: 1.0703, current_lr: 0.000216
step 2126, loss: 1.0943, current_lr: 0.000216
step 2127, loss: 1.1392, current_lr: 0.000216
step 2128, loss: 1.0015, current_lr: 0.000216
step 2129, loss: 1.1674, current_lr: 0.000216
step 2130, loss: 1.0738, current_lr: 0.000216
step 2131, loss: 1.1075, current_lr: 0.000217
step 2132, loss: 1.0005, current_lr: 0.000217
step 2133, loss: 1.0630, current_lr: 0.000217
step 2134, loss: 1.0264, current_lr: 0.000217
step 2135, loss: 0.9834, current_lr: 0.000217
step 2136, loss: 1.0702, current_lr: 0.000217
step 2137, loss: 1.0368, current_lr: 0.000218
step 2138, loss: 0.9649, current_lr: 0.000218
step 2139, loss: 1.0861, current_lr: 0.000218
step 2140, loss: 1.2036, current_lr: 0.000218
step 2141, loss: 1.1044, current_lr: 0.000218
step 2142, loss: 1.1029, current_lr: 0.000218
step 2143, loss: 1.0553, current_lr: 0.000218
step 2144, loss: 0.9674, current_lr: 0.000219
step 2145, loss: 1.1179, current_lr: 0.000219
step 2146, loss: 1.0700, current_lr: 0.000219
step 2147, loss: 1.1148, current_lr: 0.000219
step 2148, loss: 1.1362, current_lr: 0.000219
step 2149, loss: 1.1540, current_lr: 0.000219
step 2150, loss: 1.0239, current_lr: 0.000220
step 2151, loss: 1.0455, current_lr: 0.000220
step 2152, loss: 1.0829, current_lr: 0.000220
step 2153, loss: 1.0429, current_lr: 0.000220
step 2154, loss: 1.0997, current_lr: 0.000220
step 2155, loss: 1.0469, current_lr: 0.000220
step 2156, loss: 0.9378, current_lr: 0.000221
step 2157, loss: 1.0162, current_lr: 0.000221
step 2158, loss: 0.9046, current_lr: 0.000221
Saved best model with loss: 0.9046 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.9046.pt
step 2159, loss: 0.9948, current_lr: 0.000221
step 2160, loss: 0.9254, current_lr: 0.000221
step 2161, loss: 1.0194, current_lr: 0.000221
step 2162, loss: 0.9282, current_lr: 0.000221
step 2163, loss: 1.1218, current_lr: 0.000222
step 2164, loss: 1.0594, current_lr: 0.000222
step 2165, loss: 1.0300, current_lr: 0.000222
step 2166, loss: 0.9624, current_lr: 0.000222
step 2167, loss: 0.9667, current_lr: 0.000222
step 2168, loss: 1.0881, current_lr: 0.000222
step 2169, loss: 0.9753, current_lr: 0.000223
step 2170, loss: 0.9631, current_lr: 0.000223
step 2171, loss: 0.9795, current_lr: 0.000223
step 2172, loss: 0.9265, current_lr: 0.000223
step 2173, loss: 1.0311, current_lr: 0.000223
step 2174, loss: 0.9479, current_lr: 0.000223
step 2175, loss: 0.9798, current_lr: 0.000224
step 2176, loss: 1.0121, current_lr: 0.000224
step 2177, loss: 1.0065, current_lr: 0.000224
step 2178, loss: 0.9213, current_lr: 0.000224
step 2179, loss: 0.9977, current_lr: 0.000224
step 2180, loss: 0.9433, current_lr: 0.000224
step 2181, loss: 1.0049, current_lr: 0.000225
step 2182, loss: 0.9232, current_lr: 0.000225
step 2183, loss: 0.9839, current_lr: 0.000225
step 2184, loss: 0.8809, current_lr: 0.000225
Saved best model with loss: 0.8809 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.8809.pt
step 2185, loss: 0.9311, current_lr: 0.000225
step 2186, loss: 0.8059, current_lr: 0.000225
Saved best model with loss: 0.8059 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.8059.pt
step 2187, loss: 0.8181, current_lr: 0.000225
step 2188, loss: 0.8211, current_lr: 0.000226
step 2189, loss: 0.8316, current_lr: 0.000226
step 2190, loss: 0.8762, current_lr: 0.000226
step 2191, loss: 0.8091, current_lr: 0.000226
step 2192, loss: 0.8983, current_lr: 0.000226
step 2193, loss: 0.8984, current_lr: 0.000226
step 2194, loss: 0.9047, current_lr: 0.000227
step 2195, loss: 0.8504, current_lr: 0.000227
step 2196, loss: 0.8991, current_lr: 0.000227
step 2197, loss: 0.8681, current_lr: 0.000227
step 2198, loss: 0.8819, current_lr: 0.000227
step 2199, loss: 0.8671, current_lr: 0.000227
step 2200, loss: 0.8576, current_lr: 0.000228
step 2201, loss: 0.8542, current_lr: 0.000228
step 2202, loss: 0.8877, current_lr: 0.000228
step 2203, loss: 0.8910, current_lr: 0.000228
step 2204, loss: 0.8194, current_lr: 0.000228
step 2205, loss: 0.8675, current_lr: 0.000228
step 2206, loss: 0.8747, current_lr: 0.000229
step 2207, loss: 0.7762, current_lr: 0.000229
Saved best model with loss: 0.7762 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7762.pt
step 2208, loss: 0.8167, current_lr: 0.000229
step 2209, loss: 0.8223, current_lr: 0.000229
step 2210, loss: 0.8425, current_lr: 0.000229
step 2211, loss: 0.8346, current_lr: 0.000229
step 2212, loss: 0.8958, current_lr: 0.000229
step 2213, loss: 0.8323, current_lr: 0.000230
step 2214, loss: 0.8100, current_lr: 0.000230
step 2215, loss: 0.7887, current_lr: 0.000230
step 2216, loss: 0.8173, current_lr: 0.000230
step 2217, loss: 0.7730, current_lr: 0.000230
Saved best model with loss: 0.7730 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7730.pt
step 2218, loss: 0.8325, current_lr: 0.000230
step 2219, loss: 0.8674, current_lr: 0.000231
step 2220, loss: 0.8499, current_lr: 0.000231
step 2221, loss: 0.8514, current_lr: 0.000231
step 2222, loss: 0.8303, current_lr: 0.000231
step 2223, loss: 0.8923, current_lr: 0.000231
step 2224, loss: 0.8154, current_lr: 0.000231
step 2225, loss: 0.8395, current_lr: 0.000232
step 2226, loss: 0.8188, current_lr: 0.000232
step 2227, loss: 0.8003, current_lr: 0.000232
step 2228, loss: 0.9300, current_lr: 0.000232
step 2229, loss: 0.8970, current_lr: 0.000232
step 2230, loss: 0.8395, current_lr: 0.000232
step 2231, loss: 0.8461, current_lr: 0.000233
step 2232, loss: 0.8408, current_lr: 0.000233
step 2233, loss: 0.8408, current_lr: 0.000233
step 2234, loss: 0.8368, current_lr: 0.000233
step 2235, loss: 0.8171, current_lr: 0.000233
step 2236, loss: 0.8152, current_lr: 0.000233
step 2237, loss: 0.8463, current_lr: 0.000234
step 2238, loss: 0.7903, current_lr: 0.000234
step 2239, loss: 0.7507, current_lr: 0.000234
Saved best model with loss: 0.7507 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7507.pt
step 2240, loss: 0.7901, current_lr: 0.000234
step 2241, loss: 0.8384, current_lr: 0.000234
step 2242, loss: 0.8564, current_lr: 0.000234
step 2243, loss: 0.8143, current_lr: 0.000235
step 2244, loss: 0.8312, current_lr: 0.000235
step 2245, loss: 0.8073, current_lr: 0.000235
step 2246, loss: 0.7759, current_lr: 0.000235
step 2247, loss: 0.7793, current_lr: 0.000235
step 2248, loss: 0.7654, current_lr: 0.000235
step 2249, loss: 0.8707, current_lr: 0.000235
step 2250, loss: 0.9223, current_lr: 0.000236
step 2251, loss: 0.9472, current_lr: 0.000236
step 2252, loss: 0.8635, current_lr: 0.000236
step 2253, loss: 1.0129, current_lr: 0.000236
step 2254, loss: 0.9085, current_lr: 0.000236
step 2255, loss: 0.9347, current_lr: 0.000236
step 2256, loss: 0.9805, current_lr: 0.000237
step 2257, loss: 0.9132, current_lr: 0.000237
step 2258, loss: 0.9096, current_lr: 0.000237
step 2259, loss: 0.9039, current_lr: 0.000237
step 2260, loss: 0.8763, current_lr: 0.000237
step 2261, loss: 0.8699, current_lr: 0.000237
step 2262, loss: 0.9240, current_lr: 0.000238
step 2263, loss: 0.8123, current_lr: 0.000238
step 2264, loss: 0.8405, current_lr: 0.000238
step 2265, loss: 1.0184, current_lr: 0.000238
step 2266, loss: 0.9064, current_lr: 0.000238
step 2267, loss: 0.8462, current_lr: 0.000238
step 2268, loss: 0.9191, current_lr: 0.000239
step 2269, loss: 0.7303, current_lr: 0.000239
Saved best model with loss: 0.7303 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7303.pt
step 2270, loss: 0.9241, current_lr: 0.000239
step 2271, loss: 0.7793, current_lr: 0.000239
step 2272, loss: 0.7409, current_lr: 0.000239
step 2273, loss: 0.8030, current_lr: 0.000239
step 2274, loss: 0.8397, current_lr: 0.000240
step 2275, loss: 0.8134, current_lr: 0.000240
step 2276, loss: 0.8086, current_lr: 0.000240
step 2277, loss: 0.8291, current_lr: 0.000240
step 2278, loss: 0.8177, current_lr: 0.000240
step 2279, loss: 0.8916, current_lr: 0.000240
step 2280, loss: 0.7870, current_lr: 0.000241
step 2281, loss: 0.7722, current_lr: 0.000241
step 2282, loss: 0.8667, current_lr: 0.000241
step 2283, loss: 0.8741, current_lr: 0.000241
step 2284, loss: 0.8938, current_lr: 0.000241
step 2285, loss: 0.8426, current_lr: 0.000241
step 2286, loss: 0.9253, current_lr: 0.000242
step 2287, loss: 0.8257, current_lr: 0.000242
step 2288, loss: 0.8531, current_lr: 0.000242
step 2289, loss: 0.9087, current_lr: 0.000242
step 2290, loss: 0.8718, current_lr: 0.000242
step 2291, loss: 0.8770, current_lr: 0.000242
step 2292, loss: 0.8909, current_lr: 0.000243
step 2293, loss: 0.7690, current_lr: 0.000243
step 2294, loss: 0.8434, current_lr: 0.000243
step 2295, loss: 0.7698, current_lr: 0.000243
step 2296, loss: 0.8092, current_lr: 0.000243
step 2297, loss: 0.7874, current_lr: 0.000243
step 2298, loss: 0.8511, current_lr: 0.000243
step 2299, loss: 0.7813, current_lr: 0.000244
step 2300, loss: 0.7923, current_lr: 0.000244
step 2301, loss: 0.8565, current_lr: 0.000244
step 2302, loss: 0.8120, current_lr: 0.000244
step 2303, loss: 0.7254, current_lr: 0.000244
Saved best model with loss: 0.7254 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7254.pt
step 2304, loss: 0.7689, current_lr: 0.000244
step 2305, loss: 0.9050, current_lr: 0.000245
step 2306, loss: 0.8529, current_lr: 0.000245
step 2307, loss: 0.8341, current_lr: 0.000245
step 2308, loss: 0.8955, current_lr: 0.000245
step 2309, loss: 0.7831, current_lr: 0.000245
step 2310, loss: 0.8506, current_lr: 0.000245
step 2311, loss: 0.8155, current_lr: 0.000246
step 2312, loss: 0.7542, current_lr: 0.000246
step 2313, loss: 0.8301, current_lr: 0.000246
step 2314, loss: 0.8768, current_lr: 0.000246
step 2315, loss: 0.7627, current_lr: 0.000246
step 2316, loss: 0.8077, current_lr: 0.000246
step 2317, loss: 0.7756, current_lr: 0.000247
step 2318, loss: 0.7774, current_lr: 0.000247
step 2319, loss: 0.8046, current_lr: 0.000247
step 2320, loss: 0.7588, current_lr: 0.000247
step 2321, loss: 0.7440, current_lr: 0.000247
step 2322, loss: 0.8070, current_lr: 0.000247
step 2323, loss: 0.7369, current_lr: 0.000248
step 2324, loss: 0.7894, current_lr: 0.000248
step 2325, loss: 0.7687, current_lr: 0.000248
step 2326, loss: 0.7738, current_lr: 0.000248
step 2327, loss: 0.6370, current_lr: 0.000248
Saved best model with loss: 0.6370 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6370.pt
step 2328, loss: 0.7947, current_lr: 0.000248
step 2329, loss: 0.7245, current_lr: 0.000249
step 2330, loss: 0.8191, current_lr: 0.000249
step 2331, loss: 0.7491, current_lr: 0.000249
step 2332, loss: 0.7562, current_lr: 0.000249
step 2333, loss: 0.7786, current_lr: 0.000249
step 2334, loss: 0.6895, current_lr: 0.000249
step 2335, loss: 0.7425, current_lr: 0.000250
step 2336, loss: 0.8131, current_lr: 0.000250
step 2337, loss: 0.7984, current_lr: 0.000250
step 2338, loss: 0.7591, current_lr: 0.000250
step 2339, loss: 0.6962, current_lr: 0.000250
step 2340, loss: 0.6965, current_lr: 0.000250
step 2341, loss: 0.7773, current_lr: 0.000251
step 2342, loss: 0.7751, current_lr: 0.000251
step 2343, loss: 0.6828, current_lr: 0.000251
step 2344, loss: 0.7235, current_lr: 0.000251
step 2345, loss: 0.8085, current_lr: 0.000251
step 2346, loss: 0.6998, current_lr: 0.000251
step 2347, loss: 0.7285, current_lr: 0.000252
step 2348, loss: 0.7581, current_lr: 0.000252
step 2349, loss: 0.6322, current_lr: 0.000252
Saved best model with loss: 0.6322 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6322.pt
step 2350, loss: 0.6593, current_lr: 0.000252
step 2351, loss: 0.6189, current_lr: 0.000252
Saved best model with loss: 0.6189 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6189.pt
step 2352, loss: 0.6021, current_lr: 0.000252
Saved best model with loss: 0.6021 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6021.pt
step 2353, loss: 0.7065, current_lr: 0.000253
step 2354, loss: 0.7097, current_lr: 0.000253
step 2355, loss: 0.7149, current_lr: 0.000253
step 2356, loss: 0.6555, current_lr: 0.000253
step 2357, loss: 0.7476, current_lr: 0.000253
step 2358, loss: 0.6723, current_lr: 0.000253
step 2359, loss: 0.6477, current_lr: 0.000254
step 2360, loss: 0.6812, current_lr: 0.000254
step 2361, loss: 0.6816, current_lr: 0.000254
step 2362, loss: 0.6697, current_lr: 0.000254
step 2363, loss: 0.7059, current_lr: 0.000254
step 2364, loss: 0.6531, current_lr: 0.000254
step 2365, loss: 0.6707, current_lr: 0.000255
step 2366, loss: 0.6900, current_lr: 0.000255
step 2367, loss: 0.6378, current_lr: 0.000255
step 2368, loss: 0.6391, current_lr: 0.000255
step 2369, loss: 0.5864, current_lr: 0.000255
Saved best model with loss: 0.5864 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5864.pt
step 2370, loss: 0.6397, current_lr: 0.000255
step 2371, loss: 0.6537, current_lr: 0.000256
step 2372, loss: 0.6194, current_lr: 0.000256
step 2373, loss: 0.6574, current_lr: 0.000256
step 2374, loss: 0.6467, current_lr: 0.000256
step 2375, loss: 0.6446, current_lr: 0.000256
step 2376, loss: 0.6650, current_lr: 0.000256
step 2377, loss: 0.6378, current_lr: 0.000257
step 2378, loss: 0.6334, current_lr: 0.000257
step 2379, loss: 0.6559, current_lr: 0.000257
step 2380, loss: 0.6345, current_lr: 0.000257
step 2381, loss: 0.6302, current_lr: 0.000257
step 2382, loss: 0.6240, current_lr: 0.000257
step 2383, loss: 0.6316, current_lr: 0.000258
step 2384, loss: 0.6438, current_lr: 0.000258
step 2385, loss: 0.6198, current_lr: 0.000258
step 2386, loss: 0.5982, current_lr: 0.000258
step 2387, loss: 0.6522, current_lr: 0.000258
step 2388, loss: 0.6339, current_lr: 0.000258
step 2389, loss: 0.5772, current_lr: 0.000259
Saved best model with loss: 0.5772 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5772.pt
step 2390, loss: 0.6550, current_lr: 0.000259
step 2391, loss: 0.6205, current_lr: 0.000259
step 2392, loss: 0.5959, current_lr: 0.000259
step 2393, loss: 0.6014, current_lr: 0.000259
step 2394, loss: 0.6253, current_lr: 0.000259
step 2395, loss: 0.6499, current_lr: 0.000260
step 2396, loss: 0.6416, current_lr: 0.000260
step 2397, loss: 0.6110, current_lr: 0.000260
step 2398, loss: 0.6533, current_lr: 0.000260
step 2399, loss: 0.6278, current_lr: 0.000260
step 2400, loss: 0.6412, current_lr: 0.000260
step 2401, loss: 0.6157, current_lr: 0.000261
step 2402, loss: 0.6359, current_lr: 0.000261
step 2403, loss: 0.5882, current_lr: 0.000261
step 2404, loss: 0.5355, current_lr: 0.000261
Saved best model with loss: 0.5355 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5355.pt
step 2405, loss: 0.5545, current_lr: 0.000261
step 2406, loss: 0.6066, current_lr: 0.000261
step 2407, loss: 0.5920, current_lr: 0.000262
step 2408, loss: 0.6167, current_lr: 0.000262
step 2409, loss: 0.6304, current_lr: 0.000262
step 2410, loss: 0.5909, current_lr: 0.000262
step 2411, loss: 0.6075, current_lr: 0.000262
step 2412, loss: 0.6698, current_lr: 0.000262
step 2413, loss: 0.6020, current_lr: 0.000263
step 2414, loss: 0.7241, current_lr: 0.000263
step 2415, loss: 0.6587, current_lr: 0.000263
step 2416, loss: 0.6234, current_lr: 0.000263
step 2417, loss: 0.6231, current_lr: 0.000263
step 2418, loss: 0.6790, current_lr: 0.000263
step 2419, loss: 0.6975, current_lr: 0.000264
step 2420, loss: 0.6865, current_lr: 0.000264
step 2421, loss: 0.6949, current_lr: 0.000264
step 2422, loss: 0.6983, current_lr: 0.000264
step 2423, loss: 0.6560, current_lr: 0.000264
step 2424, loss: 0.6069, current_lr: 0.000264
step 2425, loss: 0.6310, current_lr: 0.000265
step 2426, loss: 0.6478, current_lr: 0.000265
step 2427, loss: 0.7329, current_lr: 0.000265
step 2428, loss: 0.6284, current_lr: 0.000265
step 2429, loss: 0.6037, current_lr: 0.000265
step 2430, loss: 0.7478, current_lr: 0.000265
step 2431, loss: 0.6876, current_lr: 0.000266
step 2432, loss: 0.7194, current_lr: 0.000266
step 2433, loss: 0.7085, current_lr: 0.000266
step 2434, loss: 0.5950, current_lr: 0.000266
step 2435, loss: 0.6434, current_lr: 0.000266
step 2436, loss: 0.6247, current_lr: 0.000266
step 2437, loss: 0.5914, current_lr: 0.000267
step 2438, loss: 0.6834, current_lr: 0.000267
step 2439, loss: 0.5868, current_lr: 0.000267
step 2440, loss: 0.6176, current_lr: 0.000267
step 2441, loss: 0.6290, current_lr: 0.000267
step 2442, loss: 0.6101, current_lr: 0.000267
step 2443, loss: 0.5700, current_lr: 0.000268
step 2444, loss: 0.6071, current_lr: 0.000268
step 2445, loss: 0.6056, current_lr: 0.000268
step 2446, loss: 0.6002, current_lr: 0.000268
step 2447, loss: 0.6972, current_lr: 0.000268
step 2448, loss: 0.6433, current_lr: 0.000268
step 2449, loss: 0.6883, current_lr: 0.000269
step 2450, loss: 0.6152, current_lr: 0.000269
step 2451, loss: 0.6573, current_lr: 0.000269
step 2452, loss: 0.5676, current_lr: 0.000269
step 2453, loss: 0.6327, current_lr: 0.000269
step 2454, loss: 0.6896, current_lr: 0.000269
step 2455, loss: 0.6732, current_lr: 0.000270
step 2456, loss: 0.6901, current_lr: 0.000270
step 2457, loss: 0.7017, current_lr: 0.000270
step 2458, loss: 0.5970, current_lr: 0.000270
step 2459, loss: 0.6671, current_lr: 0.000270
step 2460, loss: 0.6544, current_lr: 0.000270
step 2461, loss: 0.6129, current_lr: 0.000271
step 2462, loss: 0.6495, current_lr: 0.000271
step 2463, loss: 0.6834, current_lr: 0.000271
step 2464, loss: 0.5986, current_lr: 0.000271
step 2465, loss: 0.6207, current_lr: 0.000271
step 2466, loss: 0.6656, current_lr: 0.000272
step 2467, loss: 0.6366, current_lr: 0.000272
step 2468, loss: 0.6022, current_lr: 0.000272
step 2469, loss: 0.6348, current_lr: 0.000272
step 2470, loss: 0.7549, current_lr: 0.000272
step 2471, loss: 0.6530, current_lr: 0.000272
step 2472, loss: 0.6576, current_lr: 0.000273
step 2473, loss: 0.7322, current_lr: 0.000273
step 2474, loss: 0.6225, current_lr: 0.000273
step 2475, loss: 0.7046, current_lr: 0.000273
step 2476, loss: 0.6703, current_lr: 0.000273
step 2477, loss: 0.6328, current_lr: 0.000273
step 2478, loss: 0.6354, current_lr: 0.000274
step 2479, loss: 0.6822, current_lr: 0.000274
step 2480, loss: 0.5679, current_lr: 0.000274
step 2481, loss: 0.6253, current_lr: 0.000274
step 2482, loss: 0.5897, current_lr: 0.000274
step 2483, loss: 0.5442, current_lr: 0.000274
step 2484, loss: 0.5351, current_lr: 0.000275
Saved best model with loss: 0.5351 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5351.pt
step 2485, loss: 0.5587, current_lr: 0.000275
step 2486, loss: 0.5595, current_lr: 0.000275
step 2487, loss: 0.6045, current_lr: 0.000275
step 2488, loss: 0.5468, current_lr: 0.000275
step 2489, loss: 0.5922, current_lr: 0.000275
step 2490, loss: 0.5887, current_lr: 0.000276
step 2491, loss: 0.5640, current_lr: 0.000276
step 2492, loss: 0.5338, current_lr: 0.000276
Saved best model with loss: 0.5338 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5338.pt
step 2493, loss: 0.6056, current_lr: 0.000276
step 2494, loss: 0.5593, current_lr: 0.000276
step 2495, loss: 0.5926, current_lr: 0.000276
step 2496, loss: 0.5740, current_lr: 0.000277
step 2497, loss: 0.5408, current_lr: 0.000277
step 2498, loss: 0.5780, current_lr: 0.000277
step 2499, loss: 0.5062, current_lr: 0.000277
Saved best model with loss: 0.5062 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5062.pt
step 2500, loss: 0.4886, current_lr: 0.000277
Saved best model with loss: 0.4886 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4886.pt
step 2501, loss: 0.5905, current_lr: 0.000277
step 2502, loss: 0.5530, current_lr: 0.000278
step 2503, loss: 0.5701, current_lr: 0.000278
step 2504, loss: 0.5683, current_lr: 0.000278
step 2505, loss: 0.6415, current_lr: 0.000278
step 2506, loss: 0.6299, current_lr: 0.000278
step 2507, loss: 0.5869, current_lr: 0.000278
step 2508, loss: 0.5495, current_lr: 0.000279
step 2509, loss: 0.5655, current_lr: 0.000279
step 2510, loss: 0.5442, current_lr: 0.000279
step 2511, loss: 0.5361, current_lr: 0.000279
step 2512, loss: 0.5135, current_lr: 0.000279
step 2513, loss: 0.5669, current_lr: 0.000279
step 2514, loss: 0.5013, current_lr: 0.000280
step 2515, loss: 0.5675, current_lr: 0.000280
step 2516, loss: 0.5175, current_lr: 0.000280
step 2517, loss: 0.4562, current_lr: 0.000280
Saved best model with loss: 0.4562 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4562.pt
step 2518, loss: 0.4798, current_lr: 0.000280
step 2519, loss: 0.4768, current_lr: 0.000281
step 2520, loss: 0.5291, current_lr: 0.000281
step 2521, loss: 0.4966, current_lr: 0.000281
step 2522, loss: 0.5644, current_lr: 0.000281
step 2523, loss: 0.5211, current_lr: 0.000281
step 2524, loss: 0.4893, current_lr: 0.000281
step 2525, loss: 0.5175, current_lr: 0.000282
step 2526, loss: 0.5345, current_lr: 0.000282
step 2527, loss: 0.5141, current_lr: 0.000282
step 2528, loss: 0.5639, current_lr: 0.000282
step 2529, loss: 0.4893, current_lr: 0.000282
step 2530, loss: 0.5183, current_lr: 0.000282
step 2531, loss: 0.5146, current_lr: 0.000283
step 2532, loss: 0.5171, current_lr: 0.000283
step 2533, loss: 0.5179, current_lr: 0.000283
step 2534, loss: 0.4861, current_lr: 0.000283
step 2535, loss: 0.5149, current_lr: 0.000283
step 2536, loss: 0.4887, current_lr: 0.000283
step 2537, loss: 0.4787, current_lr: 0.000284
step 2538, loss: 0.5284, current_lr: 0.000284
step 2539, loss: 0.5119, current_lr: 0.000284
step 2540, loss: 0.5006, current_lr: 0.000284
step 2541, loss: 0.5302, current_lr: 0.000284
step 2542, loss: 0.4910, current_lr: 0.000284
step 2543, loss: 0.4991, current_lr: 0.000285
step 2544, loss: 0.4857, current_lr: 0.000285
step 2545, loss: 0.4480, current_lr: 0.000285
Saved best model with loss: 0.4480 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4480.pt
step 2546, loss: 0.4448, current_lr: 0.000285
Saved best model with loss: 0.4448 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4448.pt
step 2547, loss: 0.4826, current_lr: 0.000285
step 2548, loss: 0.5203, current_lr: 0.000285
step 2549, loss: 0.4793, current_lr: 0.000286
step 2550, loss: 0.4916, current_lr: 0.000286
step 2551, loss: 0.4820, current_lr: 0.000286
step 2552, loss: 0.5141, current_lr: 0.000286
step 2553, loss: 0.5098, current_lr: 0.000286
step 2554, loss: 0.4443, current_lr: 0.000286
Saved best model with loss: 0.4443 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4443.pt
step 2555, loss: 0.4944, current_lr: 0.000287
step 2556, loss: 0.5138, current_lr: 0.000287
step 2557, loss: 0.4707, current_lr: 0.000287
step 2558, loss: 0.4715, current_lr: 0.000287
step 2559, loss: 0.4539, current_lr: 0.000287
step 2560, loss: 0.4582, current_lr: 0.000288
step 2561, loss: 0.4428, current_lr: 0.000288
Saved best model with loss: 0.4428 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4428.pt
step 2562, loss: 0.4367, current_lr: 0.000288
Saved best model with loss: 0.4367 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4367.pt
step 2563, loss: 0.4795, current_lr: 0.000288
step 2564, loss: 0.4519, current_lr: 0.000288
step 2565, loss: 0.4560, current_lr: 0.000288
step 2566, loss: 0.4671, current_lr: 0.000289
step 2567, loss: 0.4601, current_lr: 0.000289
step 2568, loss: 0.4297, current_lr: 0.000289
Saved best model with loss: 0.4297 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4297.pt
step 2569, loss: 0.3805, current_lr: 0.000289
Saved best model with loss: 0.3805 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3805.pt
step 2570, loss: 0.4239, current_lr: 0.000289
step 2571, loss: 0.4379, current_lr: 0.000289
step 2572, loss: 0.4717, current_lr: 0.000290
step 2573, loss: 0.4387, current_lr: 0.000290
step 2574, loss: 0.4250, current_lr: 0.000290
step 2575, loss: 0.3803, current_lr: 0.000290
Saved best model with loss: 0.3803 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3803.pt
step 2576, loss: 0.4255, current_lr: 0.000290
step 2577, loss: 0.4687, current_lr: 0.000290
step 2578, loss: 0.4162, current_lr: 0.000291
step 2579, loss: 0.5818, current_lr: 0.000291
step 2580, loss: 0.5148, current_lr: 0.000291
step 2581, loss: 0.5019, current_lr: 0.000291
step 2582, loss: 0.5058, current_lr: 0.000291
step 2583, loss: 0.5294, current_lr: 0.000291
step 2584, loss: 0.5123, current_lr: 0.000292
step 2585, loss: 0.4888, current_lr: 0.000292
step 2586, loss: 0.4886, current_lr: 0.000292
step 2587, loss: 0.4658, current_lr: 0.000292
step 2588, loss: 0.4775, current_lr: 0.000292
step 2589, loss: 0.5030, current_lr: 0.000292
step 2590, loss: 0.4604, current_lr: 0.000293
step 2591, loss: 0.4759, current_lr: 0.000293
step 2592, loss: 0.5285, current_lr: 0.000293
step 2593, loss: 0.5156, current_lr: 0.000293
step 2594, loss: 0.4739, current_lr: 0.000293
step 2595, loss: 0.5437, current_lr: 0.000294
step 2596, loss: 0.4982, current_lr: 0.000294
step 2597, loss: 0.4797, current_lr: 0.000294
step 2598, loss: 0.4980, current_lr: 0.000294
step 2599, loss: 0.4734, current_lr: 0.000294
step 2600, loss: 0.5418, current_lr: 0.000294
step 2601, loss: 0.5406, current_lr: 0.000295
step 2602, loss: 0.4867, current_lr: 0.000295
step 2603, loss: 0.4831, current_lr: 0.000295
step 2604, loss: 0.4249, current_lr: 0.000295
step 2605, loss: 0.4297, current_lr: 0.000295
step 2606, loss: 0.4713, current_lr: 0.000295
step 2607, loss: 0.4777, current_lr: 0.000296
step 2608, loss: 0.4667, current_lr: 0.000296
step 2609, loss: 0.4674, current_lr: 0.000296
step 2610, loss: 0.4871, current_lr: 0.000296
step 2611, loss: 0.4236, current_lr: 0.000296
step 2612, loss: 0.4622, current_lr: 0.000296
step 2613, loss: 0.4591, current_lr: 0.000297
step 2614, loss: 0.5008, current_lr: 0.000297
step 2615, loss: 0.4606, current_lr: 0.000297
step 2616, loss: 0.5373, current_lr: 0.000297
step 2617, loss: 0.5060, current_lr: 0.000297
step 2618, loss: 0.4802, current_lr: 0.000297
step 2619, loss: 0.5150, current_lr: 0.000298
step 2620, loss: 0.4360, current_lr: 0.000298
step 2621, loss: 0.5116, current_lr: 0.000298
step 2622, loss: 0.4534, current_lr: 0.000298
step 2623, loss: 0.4339, current_lr: 0.000298
step 2624, loss: 0.5121, current_lr: 0.000299
step 2625, loss: 0.4890, current_lr: 0.000299
step 2626, loss: 0.4914, current_lr: 0.000299
step 2627, loss: 0.5193, current_lr: 0.000299
step 2628, loss: 0.5605, current_lr: 0.000299
step 2629, loss: 0.4391, current_lr: 0.000299
step 2630, loss: 0.4887, current_lr: 0.000300
step 2631, loss: 0.5368, current_lr: 0.000300
step 2632, loss: 0.4939, current_lr: 0.000300
step 2633, loss: 0.4586, current_lr: 0.000300
step 2634, loss: 0.4609, current_lr: 0.000300
step 2635, loss: 0.5047, current_lr: 0.000300
step 2636, loss: 0.5536, current_lr: 0.000301
step 2637, loss: 0.5337, current_lr: 0.000301
step 2638, loss: 0.6195, current_lr: 0.000301
step 2639, loss: 0.5451, current_lr: 0.000301
step 2640, loss: 0.5502, current_lr: 0.000301
step 2641, loss: 0.5033, current_lr: 0.000301
step 2642, loss: 0.4897, current_lr: 0.000302
step 2643, loss: 0.4993, current_lr: 0.000302
step 2644, loss: 0.5302, current_lr: 0.000302
step 2645, loss: 0.4591, current_lr: 0.000302
step 2646, loss: 0.5041, current_lr: 0.000302
step 2647, loss: 0.5219, current_lr: 0.000302
step 2648, loss: 0.4934, current_lr: 0.000303
step 2649, loss: 0.5127, current_lr: 0.000303
step 2650, loss: 0.4625, current_lr: 0.000303
step 2651, loss: 0.4609, current_lr: 0.000303
step 2652, loss: 0.5073, current_lr: 0.000303
step 2653, loss: 0.4848, current_lr: 0.000304
step 2654, loss: 0.4889, current_lr: 0.000304
step 2655, loss: 0.4740, current_lr: 0.000304
step 2656, loss: 0.4161, current_lr: 0.000304
step 2657, loss: 0.4242, current_lr: 0.000304
step 2658, loss: 0.4252, current_lr: 0.000304
step 2659, loss: 0.4066, current_lr: 0.000305
step 2660, loss: 0.4309, current_lr: 0.000305
step 2661, loss: 0.4288, current_lr: 0.000305
step 2662, loss: 0.4588, current_lr: 0.000305
step 2663, loss: 0.4631, current_lr: 0.000305
step 2664, loss: 0.3921, current_lr: 0.000305
step 2665, loss: 0.4055, current_lr: 0.000306
step 2666, loss: 0.4884, current_lr: 0.000306
step 2667, loss: 0.4376, current_lr: 0.000306
step 2668, loss: 0.3838, current_lr: 0.000306
step 2669, loss: 0.3964, current_lr: 0.000306
step 2670, loss: 0.4336, current_lr: 0.000306
step 2671, loss: 0.4341, current_lr: 0.000307
step 2672, loss: 0.4403, current_lr: 0.000307
step 2673, loss: 0.3917, current_lr: 0.000307
step 2674, loss: 0.4189, current_lr: 0.000307
step 2675, loss: 0.3981, current_lr: 0.000307
step 2676, loss: 0.4039, current_lr: 0.000308
step 2677, loss: 0.3776, current_lr: 0.000308
Saved best model with loss: 0.3776 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3776.pt
step 2678, loss: 0.4210, current_lr: 0.000308
step 2679, loss: 0.4222, current_lr: 0.000308
step 2680, loss: 0.4366, current_lr: 0.000308
step 2681, loss: 0.3956, current_lr: 0.000308
step 2682, loss: 0.3612, current_lr: 0.000309
Saved best model with loss: 0.3612 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3612.pt
step 2683, loss: 0.3734, current_lr: 0.000309
step 2684, loss: 0.4096, current_lr: 0.000309
step 2685, loss: 0.4134, current_lr: 0.000309
step 2686, loss: 0.3803, current_lr: 0.000309
step 2687, loss: 0.4109, current_lr: 0.000309
step 2688, loss: 0.3995, current_lr: 0.000310
step 2689, loss: 0.3480, current_lr: 0.000310
Saved best model with loss: 0.3480 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3480.pt
step 2690, loss: 0.3701, current_lr: 0.000310
step 2691, loss: 0.3851, current_lr: 0.000310
step 2692, loss: 0.3877, current_lr: 0.000310
step 2693, loss: 0.4664, current_lr: 0.000310
step 2694, loss: 0.3888, current_lr: 0.000311
step 2695, loss: 0.3726, current_lr: 0.000311
step 2696, loss: 0.3545, current_lr: 0.000311
step 2697, loss: 0.3904, current_lr: 0.000311
step 2698, loss: 0.4073, current_lr: 0.000311
step 2699, loss: 0.3835, current_lr: 0.000312
step 2700, loss: 0.3790, current_lr: 0.000312
step 2701, loss: 0.3559, current_lr: 0.000312
step 2702, loss: 0.3759, current_lr: 0.000312
step 2703, loss: 0.3541, current_lr: 0.000312
step 2704, loss: 0.3645, current_lr: 0.000312
step 2705, loss: 0.3662, current_lr: 0.000313
step 2706, loss: 0.3728, current_lr: 0.000313
step 2707, loss: 0.4141, current_lr: 0.000313
step 2708, loss: 0.4151, current_lr: 0.000313
step 2709, loss: 0.3697, current_lr: 0.000313
step 2710, loss: 0.3866, current_lr: 0.000313
step 2711, loss: 0.3223, current_lr: 0.000314
Saved best model with loss: 0.3223 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3223.pt
step 2712, loss: 0.3451, current_lr: 0.000314
step 2713, loss: 0.3801, current_lr: 0.000314
step 2714, loss: 0.3681, current_lr: 0.000314
step 2715, loss: 0.3574, current_lr: 0.000314
step 2716, loss: 0.3675, current_lr: 0.000314
step 2717, loss: 0.3819, current_lr: 0.000315
step 2718, loss: 0.3999, current_lr: 0.000315
step 2719, loss: 0.3614, current_lr: 0.000315
step 2720, loss: 0.3746, current_lr: 0.000315
step 2721, loss: 0.3858, current_lr: 0.000315
step 2722, loss: 0.3644, current_lr: 0.000316
step 2723, loss: 0.3941, current_lr: 0.000316
step 2724, loss: 0.3924, current_lr: 0.000316
step 2725, loss: 0.3951, current_lr: 0.000316
step 2726, loss: 0.3510, current_lr: 0.000316
step 2727, loss: 0.3490, current_lr: 0.000316
step 2728, loss: 0.3145, current_lr: 0.000317
Saved best model with loss: 0.3145 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3145.pt
step 2729, loss: 0.3409, current_lr: 0.000317
step 2730, loss: 0.3528, current_lr: 0.000317
step 2731, loss: 0.3226, current_lr: 0.000317
step 2732, loss: 0.3713, current_lr: 0.000317
step 2733, loss: 0.3550, current_lr: 0.000317
step 2734, loss: 0.3243, current_lr: 0.000318
step 2735, loss: 0.3034, current_lr: 0.000318
Saved best model with loss: 0.3034 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3034.pt
step 2736, loss: 0.3447, current_lr: 0.000318
step 2737, loss: 0.3347, current_lr: 0.000318
step 2738, loss: 0.3489, current_lr: 0.000318
step 2739, loss: 0.3464, current_lr: 0.000318
step 2740, loss: 0.3298, current_lr: 0.000319
step 2741, loss: 0.3448, current_lr: 0.000319
step 2742, loss: 0.3377, current_lr: 0.000319
step 2743, loss: 0.3141, current_lr: 0.000319
step 2744, loss: 0.3550, current_lr: 0.000319
step 2745, loss: 0.3765, current_lr: 0.000320
step 2746, loss: 0.3754, current_lr: 0.000320
step 2747, loss: 0.3580, current_lr: 0.000320
step 2748, loss: 0.4029, current_lr: 0.000320
step 2749, loss: 0.3911, current_lr: 0.000320
step 2750, loss: 0.3535, current_lr: 0.000320
step 2751, loss: 0.3502, current_lr: 0.000321
step 2752, loss: 0.3557, current_lr: 0.000321
step 2753, loss: 0.3236, current_lr: 0.000321
step 2754, loss: 0.3506, current_lr: 0.000321
step 2755, loss: 0.3544, current_lr: 0.000321
step 2756, loss: 0.4009, current_lr: 0.000321
step 2757, loss: 0.4041, current_lr: 0.000322
step 2758, loss: 0.4094, current_lr: 0.000322
step 2759, loss: 0.3685, current_lr: 0.000322
step 2760, loss: 0.4368, current_lr: 0.000322
step 2761, loss: 0.4071, current_lr: 0.000322
step 2762, loss: 0.4270, current_lr: 0.000322
step 2763, loss: 0.4304, current_lr: 0.000323
step 2764, loss: 0.3956, current_lr: 0.000323
step 2765, loss: 0.3683, current_lr: 0.000323
step 2766, loss: 0.3595, current_lr: 0.000323
step 2767, loss: 0.3555, current_lr: 0.000323
step 2768, loss: 0.3877, current_lr: 0.000324
step 2769, loss: 0.3934, current_lr: 0.000324
step 2770, loss: 0.3861, current_lr: 0.000324
step 2771, loss: 0.3627, current_lr: 0.000324
step 2772, loss: 0.3927, current_lr: 0.000324
step 2773, loss: 0.3388, current_lr: 0.000324
step 2774, loss: 0.3508, current_lr: 0.000325
step 2775, loss: 0.3621, current_lr: 0.000325
step 2776, loss: 0.3221, current_lr: 0.000325
step 2777, loss: 0.3559, current_lr: 0.000325
step 2778, loss: 0.3752, current_lr: 0.000325
step 2779, loss: 0.3675, current_lr: 0.000325
step 2780, loss: 0.3618, current_lr: 0.000326
step 2781, loss: 0.3748, current_lr: 0.000326
step 2782, loss: 0.3703, current_lr: 0.000326
step 2783, loss: 0.3915, current_lr: 0.000326
step 2784, loss: 0.4152, current_lr: 0.000326
step 2785, loss: 0.3731, current_lr: 0.000327
step 2786, loss: 0.3587, current_lr: 0.000327
step 2787, loss: 0.4136, current_lr: 0.000327
step 2788, loss: 0.4099, current_lr: 0.000327
step 2789, loss: 0.3274, current_lr: 0.000327
step 2790, loss: 0.3385, current_lr: 0.000327
step 2791, loss: 0.3685, current_lr: 0.000328
step 2792, loss: 0.3707, current_lr: 0.000328
step 2793, loss: 0.4169, current_lr: 0.000328
step 2794, loss: 0.3813, current_lr: 0.000328
step 2795, loss: 0.4250, current_lr: 0.000328
step 2796, loss: 0.4232, current_lr: 0.000328
step 2797, loss: 0.3966, current_lr: 0.000329
step 2798, loss: 0.3676, current_lr: 0.000329
step 2799, loss: 0.3661, current_lr: 0.000329
step 2800, loss: 0.4154, current_lr: 0.000329
step 2801, loss: 0.4054, current_lr: 0.000329
step 2802, loss: 0.3874, current_lr: 0.000329
step 2803, loss: 0.3691, current_lr: 0.000330
step 2804, loss: 0.4002, current_lr: 0.000330
step 2805, loss: 0.4072, current_lr: 0.000330
step 2806, loss: 0.4102, current_lr: 0.000330
step 2807, loss: 0.4632, current_lr: 0.000330
step 2808, loss: 0.4387, current_lr: 0.000331
step 2809, loss: 0.4141, current_lr: 0.000331
step 2810, loss: 0.3900, current_lr: 0.000331
step 2811, loss: 0.4063, current_lr: 0.000331
step 2812, loss: 0.3841, current_lr: 0.000331
step 2813, loss: 0.4164, current_lr: 0.000331
step 2814, loss: 0.3858, current_lr: 0.000332
step 2815, loss: 0.4058, current_lr: 0.000332
step 2816, loss: 0.4440, current_lr: 0.000332
step 2817, loss: 0.4097, current_lr: 0.000332
step 2818, loss: 0.3754, current_lr: 0.000332
step 2819, loss: 0.3771, current_lr: 0.000332
step 2820, loss: 0.3791, current_lr: 0.000333
step 2821, loss: 0.3342, current_lr: 0.000333
step 2822, loss: 0.3909, current_lr: 0.000333
step 2823, loss: 0.4065, current_lr: 0.000333
step 2824, loss: 0.3736, current_lr: 0.000333
step 2825, loss: 0.3886, current_lr: 0.000334
step 2826, loss: 0.3622, current_lr: 0.000334
step 2827, loss: 0.3715, current_lr: 0.000334
step 2828, loss: 0.4023, current_lr: 0.000334
step 2829, loss: 0.3372, current_lr: 0.000334
step 2830, loss: 0.3660, current_lr: 0.000334
step 2831, loss: 0.3646, current_lr: 0.000335
step 2832, loss: 0.3144, current_lr: 0.000335
step 2833, loss: 0.2827, current_lr: 0.000335
Saved best model with loss: 0.2827 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2827.pt
step 2834, loss: 0.2926, current_lr: 0.000335
step 2835, loss: 0.3407, current_lr: 0.000335
step 2836, loss: 0.3431, current_lr: 0.000335
step 2837, loss: 0.3442, current_lr: 0.000336
step 2838, loss: 0.3057, current_lr: 0.000336
step 2839, loss: 0.3350, current_lr: 0.000336
step 2840, loss: 0.3238, current_lr: 0.000336
step 2841, loss: 0.3329, current_lr: 0.000336
step 2842, loss: 0.3111, current_lr: 0.000336
step 2843, loss: 0.3055, current_lr: 0.000337
step 2844, loss: 0.3161, current_lr: 0.000337
step 2845, loss: 0.2932, current_lr: 0.000337
step 2846, loss: 0.2629, current_lr: 0.000337
Saved best model with loss: 0.2629 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2629.pt
step 2847, loss: 0.2818, current_lr: 0.000337
step 2848, loss: 0.3179, current_lr: 0.000338
step 2849, loss: 0.3259, current_lr: 0.000338
step 2850, loss: 0.3026, current_lr: 0.000338
step 2851, loss: 0.2995, current_lr: 0.000338
step 2852, loss: 0.2915, current_lr: 0.000338
step 2853, loss: 0.2952, current_lr: 0.000338
step 2854, loss: 0.2795, current_lr: 0.000339
step 2855, loss: 0.2881, current_lr: 0.000339
step 2856, loss: 0.3008, current_lr: 0.000339
step 2857, loss: 0.2931, current_lr: 0.000339
step 2858, loss: 0.3084, current_lr: 0.000339
step 2859, loss: 0.2923, current_lr: 0.000339
step 2860, loss: 0.3253, current_lr: 0.000340
step 2861, loss: 0.2809, current_lr: 0.000340
step 2862, loss: 0.3151, current_lr: 0.000340
step 2863, loss: 0.3332, current_lr: 0.000340
step 2864, loss: 0.2862, current_lr: 0.000340
step 2865, loss: 0.2815, current_lr: 0.000341
step 2866, loss: 0.2738, current_lr: 0.000341
step 2867, loss: 0.2963, current_lr: 0.000341
step 2868, loss: 0.2977, current_lr: 0.000341
step 2869, loss: 0.3139, current_lr: 0.000341
step 2870, loss: 0.3038, current_lr: 0.000341
step 2871, loss: 0.3282, current_lr: 0.000342
step 2872, loss: 0.2669, current_lr: 0.000342
step 2873, loss: 0.3002, current_lr: 0.000342
step 2874, loss: 0.3001, current_lr: 0.000342
step 2875, loss: 0.3028, current_lr: 0.000342
step 2876, loss: 0.2624, current_lr: 0.000342
Saved best model with loss: 0.2624 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2624.pt
step 2877, loss: 0.2517, current_lr: 0.000343
Saved best model with loss: 0.2517 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2517.pt
step 2878, loss: 0.3150, current_lr: 0.000343
step 2879, loss: 0.2768, current_lr: 0.000343
step 2880, loss: 0.2826, current_lr: 0.000343
step 2881, loss: 0.2565, current_lr: 0.000343
step 2882, loss: 0.2821, current_lr: 0.000344
step 2883, loss: 0.2841, current_lr: 0.000344
step 2884, loss: 0.2730, current_lr: 0.000344
step 2885, loss: 0.2475, current_lr: 0.000344
Saved best model with loss: 0.2475 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2475.pt
step 2886, loss: 0.3024, current_lr: 0.000344
step 2887, loss: 0.2784, current_lr: 0.000344
step 2888, loss: 0.2739, current_lr: 0.000345
step 2889, loss: 0.2831, current_lr: 0.000345
step 2890, loss: 0.2721, current_lr: 0.000345
step 2891, loss: 0.3053, current_lr: 0.000345
step 2892, loss: 0.3348, current_lr: 0.000345
step 2893, loss: 0.3156, current_lr: 0.000345
step 2894, loss: 0.2894, current_lr: 0.000346
step 2895, loss: 0.2701, current_lr: 0.000346
step 2896, loss: 0.2588, current_lr: 0.000346
step 2897, loss: 0.2996, current_lr: 0.000346
step 2898, loss: 0.3149, current_lr: 0.000346
step 2899, loss: 0.2524, current_lr: 0.000347
step 2900, loss: 0.2770, current_lr: 0.000347
step 2901, loss: 0.2804, current_lr: 0.000347
step 2902, loss: 0.2607, current_lr: 0.000347
step 2903, loss: 0.2835, current_lr: 0.000347
step 2904, loss: 0.2730, current_lr: 0.000347
step 2905, loss: 0.2698, current_lr: 0.000348
step 2906, loss: 0.2731, current_lr: 0.000348
step 2907, loss: 0.2803, current_lr: 0.000348
step 2908, loss: 0.2847, current_lr: 0.000348
step 2909, loss: 0.3070, current_lr: 0.000348
step 2910, loss: 0.2927, current_lr: 0.000348
step 2911, loss: 0.2946, current_lr: 0.000349
step 2912, loss: 0.2608, current_lr: 0.000349
step 2913, loss: 0.3059, current_lr: 0.000349
step 2914, loss: 0.2952, current_lr: 0.000349
step 2915, loss: 0.3293, current_lr: 0.000349
step 2916, loss: 0.2616, current_lr: 0.000349
step 2917, loss: 0.2922, current_lr: 0.000350
step 2918, loss: 0.2606, current_lr: 0.000350
step 2919, loss: 0.3103, current_lr: 0.000350
step 2920, loss: 0.2528, current_lr: 0.000350
step 2921, loss: 0.2687, current_lr: 0.000350
step 2922, loss: 0.3157, current_lr: 0.000351
step 2923, loss: 0.3096, current_lr: 0.000351
step 2924, loss: 0.3242, current_lr: 0.000351
step 2925, loss: 0.3685, current_lr: 0.000351
step 2926, loss: 0.2792, current_lr: 0.000351
step 2927, loss: 0.2471, current_lr: 0.000351
Saved best model with loss: 0.2471 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2471.pt
step 2928, loss: 0.3206, current_lr: 0.000352
step 2929, loss: 0.3282, current_lr: 0.000352
step 2930, loss: 0.3857, current_lr: 0.000352
step 2931, loss: 0.3481, current_lr: 0.000352
step 2932, loss: 0.2863, current_lr: 0.000352
step 2933, loss: 0.2832, current_lr: 0.000352
step 2934, loss: 0.2822, current_lr: 0.000353
step 2935, loss: 0.2815, current_lr: 0.000353
step 2936, loss: 0.2860, current_lr: 0.000353
step 2937, loss: 0.3327, current_lr: 0.000353
step 2938, loss: 0.3214, current_lr: 0.000353
step 2939, loss: 0.3549, current_lr: 0.000354
step 2940, loss: 0.3116, current_lr: 0.000354
step 2941, loss: 0.2612, current_lr: 0.000354
step 2942, loss: 0.2780, current_lr: 0.000354
step 2943, loss: 0.2752, current_lr: 0.000354
step 2944, loss: 0.3278, current_lr: 0.000354
step 2945, loss: 0.3299, current_lr: 0.000355
step 2946, loss: 0.3508, current_lr: 0.000355
step 2947, loss: 0.3344, current_lr: 0.000355
step 2948, loss: 0.3545, current_lr: 0.000355
step 2949, loss: 0.3508, current_lr: 0.000355
step 2950, loss: 0.3521, current_lr: 0.000355
step 2951, loss: 0.3257, current_lr: 0.000356
step 2952, loss: 0.3083, current_lr: 0.000356
step 2953, loss: 0.2877, current_lr: 0.000356
step 2954, loss: 0.2883, current_lr: 0.000356
step 2955, loss: 0.3387, current_lr: 0.000356
step 2956, loss: 0.3330, current_lr: 0.000357
step 2957, loss: 0.2981, current_lr: 0.000357
step 2958, loss: 0.4289, current_lr: 0.000357
step 2959, loss: 0.5156, current_lr: 0.000357
step 2960, loss: 0.3684, current_lr: 0.000357
step 2961, loss: 0.3365, current_lr: 0.000357
step 2962, loss: 0.3866, current_lr: 0.000358
step 2963, loss: 0.4979, current_lr: 0.000358
step 2964, loss: 0.4634, current_lr: 0.000358
step 2965, loss: 0.3939, current_lr: 0.000358
step 2966, loss: 0.3682, current_lr: 0.000358
step 2967, loss: 0.3980, current_lr: 0.000358
step 2968, loss: 0.3512, current_lr: 0.000359
step 2969, loss: 0.3406, current_lr: 0.000359
step 2970, loss: 0.3681, current_lr: 0.000359
step 2971, loss: 0.3658, current_lr: 0.000359
step 2972, loss: 0.4948, current_lr: 0.000359
step 2973, loss: 0.3907, current_lr: 0.000360
step 2974, loss: 0.3905, current_lr: 0.000360
step 2975, loss: 0.4729, current_lr: 0.000360
step 2976, loss: 0.4099, current_lr: 0.000360
step 2977, loss: 0.3530, current_lr: 0.000360
step 2978, loss: 0.3771, current_lr: 0.000360
step 2979, loss: 0.3563, current_lr: 0.000361
step 2980, loss: 0.3519, current_lr: 0.000361
step 2981, loss: 0.3682, current_lr: 0.000361
step 2982, loss: 0.4044, current_lr: 0.000361
step 2983, loss: 0.3609, current_lr: 0.000361
step 2984, loss: 0.3804, current_lr: 0.000361
step 2985, loss: 0.3394, current_lr: 0.000362
step 2986, loss: 0.3393, current_lr: 0.000362
step 2987, loss: 0.3556, current_lr: 0.000362
step 2988, loss: 0.3656, current_lr: 0.000362
step 2989, loss: 0.3298, current_lr: 0.000362
step 2990, loss: 0.3557, current_lr: 0.000363
step 2991, loss: 0.3323, current_lr: 0.000363
step 2992, loss: 0.3079, current_lr: 0.000363
step 2993, loss: 0.3818, current_lr: 0.000363
step 2994, loss: 0.3368, current_lr: 0.000363
step 2995, loss: 0.3291, current_lr: 0.000363
step 2996, loss: 0.3896, current_lr: 0.000364
step 2997, loss: 0.3435, current_lr: 0.000364
step 2998, loss: 0.2878, current_lr: 0.000364
step 2999, loss: 0.2906, current_lr: 0.000364
step 3000, loss: 0.3350, current_lr: 0.000364
step 3001, loss: 0.2843, current_lr: 0.000364
step 3002, loss: 0.2770, current_lr: 0.000365
step 3003, loss: 0.2848, current_lr: 0.000365
step 3004, loss: 0.2928, current_lr: 0.000365
step 3005, loss: 0.3208, current_lr: 0.000365
step 3006, loss: 0.2816, current_lr: 0.000365
step 3007, loss: 0.2631, current_lr: 0.000365
step 3008, loss: 0.2911, current_lr: 0.000366
step 3009, loss: 0.2854, current_lr: 0.000366
step 3010, loss: 0.2977, current_lr: 0.000366
step 3011, loss: 0.2622, current_lr: 0.000366
step 3012, loss: 0.2153, current_lr: 0.000366
Saved best model with loss: 0.2153 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2153.pt
step 3013, loss: 0.2499, current_lr: 0.000367
step 3014, loss: 0.3017, current_lr: 0.000367
step 3015, loss: 0.3158, current_lr: 0.000367
step 3016, loss: 0.2472, current_lr: 0.000367
step 3017, loss: 0.3189, current_lr: 0.000367
step 3018, loss: 0.2539, current_lr: 0.000367
step 3019, loss: 0.2597, current_lr: 0.000368
step 3020, loss: 0.2552, current_lr: 0.000368
step 3021, loss: 0.2479, current_lr: 0.000368
step 3022, loss: 0.3111, current_lr: 0.000368
step 3023, loss: 0.3041, current_lr: 0.000368
step 3024, loss: 0.2489, current_lr: 0.000368
step 3025, loss: 0.2428, current_lr: 0.000369
step 3026, loss: 0.2692, current_lr: 0.000369
step 3027, loss: 0.2377, current_lr: 0.000369
step 3028, loss: 0.2602, current_lr: 0.000369
step 3029, loss: 0.2743, current_lr: 0.000369
step 3030, loss: 0.2648, current_lr: 0.000370
step 3031, loss: 0.2675, current_lr: 0.000370
step 3032, loss: 0.2368, current_lr: 0.000370
step 3033, loss: 0.2588, current_lr: 0.000370
step 3034, loss: 0.2303, current_lr: 0.000370
step 3035, loss: 0.2982, current_lr: 0.000370
step 3036, loss: 0.3053, current_lr: 0.000371
step 3037, loss: 0.2841, current_lr: 0.000371
step 3038, loss: 0.2842, current_lr: 0.000371
step 3039, loss: 0.2886, current_lr: 0.000371
step 3040, loss: 0.2529, current_lr: 0.000371
step 3041, loss: 0.2412, current_lr: 0.000371
step 3042, loss: 0.2564, current_lr: 0.000372
step 3043, loss: 0.2517, current_lr: 0.000372
step 3044, loss: 0.2240, current_lr: 0.000372
step 3045, loss: 0.2557, current_lr: 0.000372
step 3046, loss: 0.2407, current_lr: 0.000372
step 3047, loss: 0.2550, current_lr: 0.000373
step 3048, loss: 0.2694, current_lr: 0.000373
step 3049, loss: 0.2403, current_lr: 0.000373
step 3050, loss: 0.2446, current_lr: 0.000373
step 3051, loss: 0.2679, current_lr: 0.000373
step 3052, loss: 0.2392, current_lr: 0.000373
step 3053, loss: 0.2443, current_lr: 0.000374
step 3054, loss: 0.2463, current_lr: 0.000374
step 3055, loss: 0.2327, current_lr: 0.000374
step 3056, loss: 0.2512, current_lr: 0.000374
step 3057, loss: 0.2426, current_lr: 0.000374
step 3058, loss: 0.2353, current_lr: 0.000374
step 3059, loss: 0.2399, current_lr: 0.000375
step 3060, loss: 0.2508, current_lr: 0.000375
step 3061, loss: 0.2428, current_lr: 0.000375
step 3062, loss: 0.2483, current_lr: 0.000375
step 3063, loss: 0.2145, current_lr: 0.000375
Saved best model with loss: 0.2145 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2145.pt
step 3064, loss: 0.2369, current_lr: 0.000376
step 3065, loss: 0.1995, current_lr: 0.000376
Saved best model with loss: 0.1995 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1995.pt
step 3066, loss: 0.2260, current_lr: 0.000376
step 3067, loss: 0.2555, current_lr: 0.000376
step 3068, loss: 0.2712, current_lr: 0.000376
step 3069, loss: 0.2501, current_lr: 0.000376
step 3070, loss: 0.2134, current_lr: 0.000377
step 3071, loss: 0.2070, current_lr: 0.000377
step 3072, loss: 0.2558, current_lr: 0.000377
step 3073, loss: 0.2257, current_lr: 0.000377
step 3074, loss: 0.2443, current_lr: 0.000377
step 3075, loss: 0.2405, current_lr: 0.000377
step 3076, loss: 0.2849, current_lr: 0.000378
step 3077, loss: 0.2332, current_lr: 0.000378
step 3078, loss: 0.3021, current_lr: 0.000378
step 3079, loss: 0.2762, current_lr: 0.000378
step 3080, loss: 0.2533, current_lr: 0.000378
step 3081, loss: 0.2116, current_lr: 0.000379
step 3082, loss: 0.2239, current_lr: 0.000379
step 3083, loss: 0.2604, current_lr: 0.000379
step 3084, loss: 0.2788, current_lr: 0.000379
step 3085, loss: 0.2294, current_lr: 0.000379
step 3086, loss: 0.2307, current_lr: 0.000379
step 3087, loss: 0.2402, current_lr: 0.000380
step 3088, loss: 0.2299, current_lr: 0.000380
step 3089, loss: 0.2499, current_lr: 0.000380
step 3090, loss: 0.2853, current_lr: 0.000380
step 3091, loss: 0.2660, current_lr: 0.000380
step 3092, loss: 0.2575, current_lr: 0.000380
step 3093, loss: 0.2431, current_lr: 0.000381
step 3094, loss: 0.2112, current_lr: 0.000381
step 3095, loss: 0.2735, current_lr: 0.000381
step 3096, loss: 0.2556, current_lr: 0.000381
step 3097, loss: 0.2214, current_lr: 0.000381
step 3098, loss: 0.2459, current_lr: 0.000381
step 3099, loss: 0.2715, current_lr: 0.000382
step 3100, loss: 0.2508, current_lr: 0.000382
step 3101, loss: 0.2126, current_lr: 0.000382
step 3102, loss: 0.2572, current_lr: 0.000382
step 3103, loss: 0.2065, current_lr: 0.000382
step 3104, loss: 0.2234, current_lr: 0.000383
step 3105, loss: 0.2242, current_lr: 0.000383
step 3106, loss: 0.2490, current_lr: 0.000383
step 3107, loss: 0.2616, current_lr: 0.000383
step 3108, loss: 0.2519, current_lr: 0.000383
step 3109, loss: 0.2393, current_lr: 0.000383
step 3110, loss: 0.2067, current_lr: 0.000384
step 3111, loss: 0.2131, current_lr: 0.000384
step 3112, loss: 0.2313, current_lr: 0.000384
step 3113, loss: 0.2576, current_lr: 0.000384
step 3114, loss: 0.2537, current_lr: 0.000384
step 3115, loss: 0.2659, current_lr: 0.000384
step 3116, loss: 0.2669, current_lr: 0.000385
step 3117, loss: 0.2608, current_lr: 0.000385
step 3118, loss: 0.1945, current_lr: 0.000385
Saved best model with loss: 0.1945 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1945.pt
step 3119, loss: 0.2183, current_lr: 0.000385
step 3120, loss: 0.2490, current_lr: 0.000385
step 3121, loss: 0.2295, current_lr: 0.000386
step 3122, loss: 0.2352, current_lr: 0.000386
step 3123, loss: 0.2844, current_lr: 0.000386
step 3124, loss: 0.2630, current_lr: 0.000386
step 3125, loss: 0.2687, current_lr: 0.000386
step 3126, loss: 0.2382, current_lr: 0.000386
step 3127, loss: 0.2595, current_lr: 0.000387
step 3128, loss: 0.2606, current_lr: 0.000387
step 3129, loss: 0.2274, current_lr: 0.000387
step 3130, loss: 0.3107, current_lr: 0.000387
step 3131, loss: 0.2611, current_lr: 0.000387
step 3132, loss: 0.2679, current_lr: 0.000387
step 3133, loss: 0.2582, current_lr: 0.000388
step 3134, loss: 0.2401, current_lr: 0.000388
step 3135, loss: 0.2635, current_lr: 0.000388
step 3136, loss: 0.2913, current_lr: 0.000388
step 3137, loss: 0.2490, current_lr: 0.000388
step 3138, loss: 0.2598, current_lr: 0.000389
step 3139, loss: 0.2705, current_lr: 0.000389
step 3140, loss: 0.2556, current_lr: 0.000389
step 3141, loss: 0.2801, current_lr: 0.000389
step 3142, loss: 0.2512, current_lr: 0.000389
step 3143, loss: 0.3149, current_lr: 0.000389
step 3144, loss: 0.2589, current_lr: 0.000390
step 3145, loss: 0.2668, current_lr: 0.000390
step 3146, loss: 0.2537, current_lr: 0.000390
step 3147, loss: 0.2801, current_lr: 0.000390
step 3148, loss: 0.2681, current_lr: 0.000390
step 3149, loss: 0.2533, current_lr: 0.000390
step 3150, loss: 0.2456, current_lr: 0.000391
step 3151, loss: 0.2116, current_lr: 0.000391
step 3152, loss: 0.2140, current_lr: 0.000391
step 3153, loss: 0.2559, current_lr: 0.000391
step 3154, loss: 0.2512, current_lr: 0.000391
step 3155, loss: 0.2774, current_lr: 0.000392
step 3156, loss: 0.2756, current_lr: 0.000392
step 3157, loss: 0.2633, current_lr: 0.000392
step 3158, loss: 0.2457, current_lr: 0.000392
step 3159, loss: 0.2190, current_lr: 0.000392
step 3160, loss: 0.2217, current_lr: 0.000392
step 3161, loss: 0.2843, current_lr: 0.000393
step 3162, loss: 0.2688, current_lr: 0.000393
step 3163, loss: 0.2340, current_lr: 0.000393
step 3164, loss: 0.2567, current_lr: 0.000393
step 3165, loss: 0.2907, current_lr: 0.000393
step 3166, loss: 0.2586, current_lr: 0.000393
step 3167, loss: 0.2592, current_lr: 0.000394
step 3168, loss: 0.2222, current_lr: 0.000394
step 3169, loss: 0.2275, current_lr: 0.000394
step 3170, loss: 0.2485, current_lr: 0.000394
step 3171, loss: 0.2823, current_lr: 0.000394
step 3172, loss: 0.2176, current_lr: 0.000394
step 3173, loss: 0.2158, current_lr: 0.000395
step 3174, loss: 0.2146, current_lr: 0.000395
step 3175, loss: 0.2148, current_lr: 0.000395
step 3176, loss: 0.2155, current_lr: 0.000395
step 3177, loss: 0.2213, current_lr: 0.000395
step 3178, loss: 0.1923, current_lr: 0.000396
Saved best model with loss: 0.1923 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1923.pt
step 3179, loss: 0.2143, current_lr: 0.000396
step 3180, loss: 0.2406, current_lr: 0.000396
step 3181, loss: 0.2033, current_lr: 0.000396
step 3182, loss: 0.2141, current_lr: 0.000396
step 3183, loss: 0.2472, current_lr: 0.000396
step 3184, loss: 0.1877, current_lr: 0.000397
Saved best model with loss: 0.1877 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1877.pt
step 3185, loss: 0.2562, current_lr: 0.000397
step 3186, loss: 0.2200, current_lr: 0.000397
step 3187, loss: 0.2549, current_lr: 0.000397
step 3188, loss: 0.2234, current_lr: 0.000397
step 3189, loss: 0.2072, current_lr: 0.000397
step 3190, loss: 0.1977, current_lr: 0.000398
step 3191, loss: 0.2020, current_lr: 0.000398
step 3192, loss: 0.1883, current_lr: 0.000398
step 3193, loss: 0.2178, current_lr: 0.000398
step 3194, loss: 0.2133, current_lr: 0.000398
step 3195, loss: 0.1971, current_lr: 0.000399
step 3196, loss: 0.2289, current_lr: 0.000399
step 3197, loss: 0.2427, current_lr: 0.000399
step 3198, loss: 0.2229, current_lr: 0.000399
step 3199, loss: 0.1869, current_lr: 0.000399
Saved best model with loss: 0.1869 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1869.pt
step 3200, loss: 0.1839, current_lr: 0.000399
Saved best model with loss: 0.1839 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1839.pt
step 3201, loss: 0.2183, current_lr: 0.000400
step 3202, loss: 0.2298, current_lr: 0.000400
step 3203, loss: 0.2275, current_lr: 0.000400
step 3204, loss: 0.2098, current_lr: 0.000400
step 3205, loss: 0.2495, current_lr: 0.000400
step 3206, loss: 0.1810, current_lr: 0.000400
Saved best model with loss: 0.1810 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1810.pt
step 3207, loss: 0.1929, current_lr: 0.000401
step 3208, loss: 0.1895, current_lr: 0.000401
step 3209, loss: 0.1780, current_lr: 0.000401
Saved best model with loss: 0.1780 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1780.pt
step 3210, loss: 0.1960, current_lr: 0.000401
step 3211, loss: 0.1956, current_lr: 0.000401
step 3212, loss: 0.2320, current_lr: 0.000401
step 3213, loss: 0.2035, current_lr: 0.000402
step 3214, loss: 0.1874, current_lr: 0.000402
step 3215, loss: 0.1892, current_lr: 0.000402
step 3216, loss: 0.2207, current_lr: 0.000402
step 3217, loss: 0.1839, current_lr: 0.000402
step 3218, loss: 0.2273, current_lr: 0.000403
step 3219, loss: 0.2075, current_lr: 0.000403
step 3220, loss: 0.2537, current_lr: 0.000403
step 3221, loss: 0.2119, current_lr: 0.000403
step 3222, loss: 0.1802, current_lr: 0.000403
step 3223, loss: 0.2234, current_lr: 0.000403
step 3224, loss: 0.2150, current_lr: 0.000404
step 3225, loss: 0.2027, current_lr: 0.000404
step 3226, loss: 0.1887, current_lr: 0.000404
step 3227, loss: 0.1922, current_lr: 0.000404
step 3228, loss: 0.2006, current_lr: 0.000404
step 3229, loss: 0.1991, current_lr: 0.000404
step 3230, loss: 0.1621, current_lr: 0.000405
Saved best model with loss: 0.1621 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1621.pt
step 3231, loss: 0.1879, current_lr: 0.000405
step 3232, loss: 0.1803, current_lr: 0.000405
step 3233, loss: 0.1833, current_lr: 0.000405
step 3234, loss: 0.1869, current_lr: 0.000405
step 3235, loss: 0.1737, current_lr: 0.000406
step 3236, loss: 0.1679, current_lr: 0.000406
step 3237, loss: 0.1949, current_lr: 0.000406
step 3238, loss: 0.1612, current_lr: 0.000406
Saved best model with loss: 0.1612 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1612.pt
step 3239, loss: 0.1946, current_lr: 0.000406
step 3240, loss: 0.2104, current_lr: 0.000406
step 3241, loss: 0.2136, current_lr: 0.000407
step 3242, loss: 0.2029, current_lr: 0.000407
step 3243, loss: 0.2418, current_lr: 0.000407
step 3244, loss: 0.2442, current_lr: 0.000407
step 3245, loss: 0.2222, current_lr: 0.000407
step 3246, loss: 0.2089, current_lr: 0.000407
step 3247, loss: 0.2004, current_lr: 0.000408
step 3248, loss: 0.1909, current_lr: 0.000408
step 3249, loss: 0.2327, current_lr: 0.000408
step 3250, loss: 0.1906, current_lr: 0.000408
step 3251, loss: 0.2108, current_lr: 0.000408
step 3252, loss: 0.1967, current_lr: 0.000408
step 3253, loss: 0.1836, current_lr: 0.000409
step 3254, loss: 0.1861, current_lr: 0.000409
step 3255, loss: 0.1986, current_lr: 0.000409
step 3256, loss: 0.1907, current_lr: 0.000409
step 3257, loss: 0.2396, current_lr: 0.000409
step 3258, loss: 0.2019, current_lr: 0.000410
step 3259, loss: 0.1656, current_lr: 0.000410
step 3260, loss: 0.2205, current_lr: 0.000410
step 3261, loss: 0.2300, current_lr: 0.000410
step 3262, loss: 0.1939, current_lr: 0.000410
step 3263, loss: 0.1935, current_lr: 0.000410
step 3264, loss: 0.2120, current_lr: 0.000411
step 3265, loss: 0.1966, current_lr: 0.000411
step 3266, loss: 0.1877, current_lr: 0.000411
step 3267, loss: 0.2419, current_lr: 0.000411
step 3268, loss: 0.1992, current_lr: 0.000411
step 3269, loss: 0.1875, current_lr: 0.000411
step 3270, loss: 0.2259, current_lr: 0.000412
step 3271, loss: 0.1835, current_lr: 0.000412
step 3272, loss: 0.1892, current_lr: 0.000412
step 3273, loss: 0.1730, current_lr: 0.000412
step 3274, loss: 0.1762, current_lr: 0.000412
step 3275, loss: 0.1908, current_lr: 0.000412
step 3276, loss: 0.2120, current_lr: 0.000413
step 3277, loss: 0.1985, current_lr: 0.000413
step 3278, loss: 0.2058, current_lr: 0.000413
step 3279, loss: 0.1939, current_lr: 0.000413
step 3280, loss: 0.2043, current_lr: 0.000413
step 3281, loss: 0.1902, current_lr: 0.000414
step 3282, loss: 0.1614, current_lr: 0.000414
step 3283, loss: 0.1869, current_lr: 0.000414
step 3284, loss: 0.1638, current_lr: 0.000414
step 3285, loss: 0.1820, current_lr: 0.000414
step 3286, loss: 0.1931, current_lr: 0.000414
step 3287, loss: 0.1870, current_lr: 0.000415
step 3288, loss: 0.2202, current_lr: 0.000415
step 3289, loss: 0.1801, current_lr: 0.000415
step 3290, loss: 0.1959, current_lr: 0.000415
step 3291, loss: 0.1689, current_lr: 0.000415
step 3292, loss: 0.1817, current_lr: 0.000415
step 3293, loss: 0.1729, current_lr: 0.000416
step 3294, loss: 0.1694, current_lr: 0.000416
step 3295, loss: 0.1789, current_lr: 0.000416
step 3296, loss: 0.1994, current_lr: 0.000416
step 3297, loss: 0.2149, current_lr: 0.000416
step 3298, loss: 0.2476, current_lr: 0.000416
step 3299, loss: 0.2059, current_lr: 0.000417
step 3300, loss: 0.1907, current_lr: 0.000417
step 3301, loss: 0.2077, current_lr: 0.000417
step 3302, loss: 0.2403, current_lr: 0.000417
step 3303, loss: 0.2180, current_lr: 0.000417
step 3304, loss: 0.2498, current_lr: 0.000418
step 3305, loss: 0.1923, current_lr: 0.000418
step 3306, loss: 0.2041, current_lr: 0.000418
step 3307, loss: 0.2218, current_lr: 0.000418
step 3308, loss: 0.2459, current_lr: 0.000418
step 3309, loss: 0.2380, current_lr: 0.000418
step 3310, loss: 0.1905, current_lr: 0.000419
step 3311, loss: 0.1779, current_lr: 0.000419
step 3312, loss: 0.2265, current_lr: 0.000419
step 3313, loss: 0.2090, current_lr: 0.000419
step 3314, loss: 0.2189, current_lr: 0.000419
step 3315, loss: 0.1822, current_lr: 0.000419
step 3316, loss: 0.1958, current_lr: 0.000420
step 3317, loss: 0.2106, current_lr: 0.000420
step 3318, loss: 0.1848, current_lr: 0.000420
step 3319, loss: 0.2110, current_lr: 0.000420
step 3320, loss: 0.2273, current_lr: 0.000420
step 3321, loss: 0.1999, current_lr: 0.000420
step 3322, loss: 0.2112, current_lr: 0.000421
step 3323, loss: 0.2318, current_lr: 0.000421
step 3324, loss: 0.2149, current_lr: 0.000421
step 3325, loss: 0.1835, current_lr: 0.000421
step 3326, loss: 0.2140, current_lr: 0.000421
step 3327, loss: 0.1833, current_lr: 0.000422
step 3328, loss: 0.1880, current_lr: 0.000422
step 3329, loss: 0.1980, current_lr: 0.000422
step 3330, loss: 0.2332, current_lr: 0.000422
step 3331, loss: 0.2240, current_lr: 0.000422
step 3332, loss: 0.2344, current_lr: 0.000422
step 3333, loss: 0.3315, current_lr: 0.000423
step 3334, loss: 0.2050, current_lr: 0.000423
step 3335, loss: 0.1817, current_lr: 0.000423
step 3336, loss: 0.2069, current_lr: 0.000423
step 3337, loss: 0.1748, current_lr: 0.000423
step 3338, loss: 0.2182, current_lr: 0.000423
step 3339, loss: 0.1948, current_lr: 0.000424
step 3340, loss: 0.1935, current_lr: 0.000424
step 3341, loss: 0.1861, current_lr: 0.000424
step 3342, loss: 0.1891, current_lr: 0.000424
step 3343, loss: 0.1651, current_lr: 0.000424
step 3344, loss: 0.2002, current_lr: 0.000424
step 3345, loss: 0.2578, current_lr: 0.000425
step 3346, loss: 0.1578, current_lr: 0.000425
Saved best model with loss: 0.1578 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1578.pt
step 3347, loss: 0.1982, current_lr: 0.000425
step 3348, loss: 0.2063, current_lr: 0.000425
step 3349, loss: 0.1684, current_lr: 0.000425
step 3350, loss: 0.1615, current_lr: 0.000426
step 3351, loss: 0.2201, current_lr: 0.000426
step 3352, loss: 0.1768, current_lr: 0.000426
step 3353, loss: 0.1845, current_lr: 0.000426
step 3354, loss: 0.1780, current_lr: 0.000426
step 3355, loss: 0.1749, current_lr: 0.000426
step 3356, loss: 0.1711, current_lr: 0.000427
step 3357, loss: 0.1980, current_lr: 0.000427
step 3358, loss: 0.1634, current_lr: 0.000427
step 3359, loss: 0.1617, current_lr: 0.000427
step 3360, loss: 0.1699, current_lr: 0.000427
step 3361, loss: 0.1613, current_lr: 0.000427
step 3362, loss: 0.1852, current_lr: 0.000428
step 3363, loss: 0.1886, current_lr: 0.000428
step 3364, loss: 0.1830, current_lr: 0.000428
step 3365, loss: 0.1725, current_lr: 0.000428
step 3366, loss: 0.1851, current_lr: 0.000428
step 3367, loss: 0.1773, current_lr: 0.000428
step 3368, loss: 0.1765, current_lr: 0.000429
step 3369, loss: 0.1588, current_lr: 0.000429
step 3370, loss: 0.1697, current_lr: 0.000429
step 3371, loss: 0.1494, current_lr: 0.000429
Saved best model with loss: 0.1494 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1494.pt
step 3372, loss: 0.2001, current_lr: 0.000429
step 3373, loss: 0.2051, current_lr: 0.000429
step 3374, loss: 0.1937, current_lr: 0.000430
step 3375, loss: 0.1989, current_lr: 0.000430
step 3376, loss: 0.1499, current_lr: 0.000430
step 3377, loss: 0.1707, current_lr: 0.000430
step 3378, loss: 0.1900, current_lr: 0.000430
step 3379, loss: 0.1938, current_lr: 0.000431
step 3380, loss: 0.1919, current_lr: 0.000431
step 3381, loss: 0.2075, current_lr: 0.000431
step 3382, loss: 0.1963, current_lr: 0.000431
step 3383, loss: 0.1836, current_lr: 0.000431
step 3384, loss: 0.1779, current_lr: 0.000431
step 3385, loss: 0.2019, current_lr: 0.000432
step 3386, loss: 0.1616, current_lr: 0.000432
step 3387, loss: 0.1736, current_lr: 0.000432
step 3388, loss: 0.1981, current_lr: 0.000432
step 3389, loss: 0.1981, current_lr: 0.000432
step 3390, loss: 0.1564, current_lr: 0.000432
step 3391, loss: 0.1700, current_lr: 0.000433
step 3392, loss: 0.1863, current_lr: 0.000433
step 3393, loss: 0.1724, current_lr: 0.000433
step 3394, loss: 0.2231, current_lr: 0.000433
step 3395, loss: 0.1930, current_lr: 0.000433
step 3396, loss: 0.2040, current_lr: 0.000433
step 3397, loss: 0.1572, current_lr: 0.000434
step 3398, loss: 0.1748, current_lr: 0.000434
step 3399, loss: 0.1864, current_lr: 0.000434
step 3400, loss: 0.1510, current_lr: 0.000434
step 3401, loss: 0.1528, current_lr: 0.000434
step 3402, loss: 0.1662, current_lr: 0.000434
step 3403, loss: 0.1859, current_lr: 0.000435
step 3404, loss: 0.1834, current_lr: 0.000435
step 3405, loss: 0.1961, current_lr: 0.000435
step 3406, loss: 0.1787, current_lr: 0.000435
step 3407, loss: 0.1761, current_lr: 0.000435
step 3408, loss: 0.2071, current_lr: 0.000436
step 3409, loss: 0.2005, current_lr: 0.000436
step 3410, loss: 0.2147, current_lr: 0.000436
step 3411, loss: 0.2015, current_lr: 0.000436
step 3412, loss: 0.1848, current_lr: 0.000436
step 3413, loss: 0.1987, current_lr: 0.000436
step 3414, loss: 0.1835, current_lr: 0.000437
step 3415, loss: 0.2135, current_lr: 0.000437
step 3416, loss: 0.1735, current_lr: 0.000437
step 3417, loss: 0.1817, current_lr: 0.000437
step 3418, loss: 0.1663, current_lr: 0.000437
step 3419, loss: 0.1930, current_lr: 0.000437
step 3420, loss: 0.2184, current_lr: 0.000438
step 3421, loss: 0.1551, current_lr: 0.000438
step 3422, loss: 0.1973, current_lr: 0.000438
step 3423, loss: 0.1774, current_lr: 0.000438
step 3424, loss: 0.1565, current_lr: 0.000438
step 3425, loss: 0.2004, current_lr: 0.000438
step 3426, loss: 0.2066, current_lr: 0.000439
step 3427, loss: 0.1909, current_lr: 0.000439
step 3428, loss: 0.2397, current_lr: 0.000439
step 3429, loss: 0.1999, current_lr: 0.000439
step 3430, loss: 0.1757, current_lr: 0.000439
step 3431, loss: 0.1940, current_lr: 0.000439
step 3432, loss: 0.2131, current_lr: 0.000440
step 3433, loss: 0.1806, current_lr: 0.000440
step 3434, loss: 0.1952, current_lr: 0.000440
step 3435, loss: 0.2080, current_lr: 0.000440
step 3436, loss: 0.1713, current_lr: 0.000440
step 3437, loss: 0.1772, current_lr: 0.000440
step 3438, loss: 0.1770, current_lr: 0.000441
step 3439, loss: 0.1760, current_lr: 0.000441
step 3440, loss: 0.1738, current_lr: 0.000441
step 3441, loss: 0.2360, current_lr: 0.000441
step 3442, loss: 0.1907, current_lr: 0.000441
step 3443, loss: 0.1611, current_lr: 0.000442
step 3444, loss: 0.1892, current_lr: 0.000442
step 3445, loss: 0.1959, current_lr: 0.000442
step 3446, loss: 0.2341, current_lr: 0.000442
step 3447, loss: 0.1592, current_lr: 0.000442
step 3448, loss: 0.1433, current_lr: 0.000442
Saved best model with loss: 0.1433 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1433.pt
step 3449, loss: 0.1395, current_lr: 0.000443
Saved best model with loss: 0.1395 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1395.pt
step 3450, loss: 0.1669, current_lr: 0.000443
step 3451, loss: 0.1710, current_lr: 0.000443
step 3452, loss: 0.1824, current_lr: 0.000443
step 3453, loss: 0.1700, current_lr: 0.000443
step 3454, loss: 0.1791, current_lr: 0.000443
step 3455, loss: 0.1664, current_lr: 0.000444
step 3456, loss: 0.1679, current_lr: 0.000444
step 3457, loss: 0.1621, current_lr: 0.000444
step 3458, loss: 0.1453, current_lr: 0.000444
step 3459, loss: 0.1589, current_lr: 0.000444
step 3460, loss: 0.1797, current_lr: 0.000444
step 3461, loss: 0.1844, current_lr: 0.000445
step 3462, loss: 0.1894, current_lr: 0.000445
step 3463, loss: 0.1795, current_lr: 0.000445
step 3464, loss: 0.1600, current_lr: 0.000445
step 3465, loss: 0.1870, current_lr: 0.000445
step 3466, loss: 0.1696, current_lr: 0.000445
step 3467, loss: 0.1913, current_lr: 0.000446
step 3468, loss: 0.1789, current_lr: 0.000446
step 3469, loss: 0.2042, current_lr: 0.000446
step 3470, loss: 0.1741, current_lr: 0.000446
step 3471, loss: 0.1639, current_lr: 0.000446
step 3472, loss: 0.1859, current_lr: 0.000446
step 3473, loss: 0.1535, current_lr: 0.000447
step 3474, loss: 0.1854, current_lr: 0.000447
step 3475, loss: 0.1957, current_lr: 0.000447
step 3476, loss: 0.1922, current_lr: 0.000447
step 3477, loss: 0.2393, current_lr: 0.000447
step 3478, loss: 0.1892, current_lr: 0.000447
step 3479, loss: 0.1795, current_lr: 0.000448
step 3480, loss: 0.1873, current_lr: 0.000448
step 3481, loss: 0.1992, current_lr: 0.000448
step 3482, loss: 0.2130, current_lr: 0.000448
step 3483, loss: 0.1951, current_lr: 0.000448
step 3484, loss: 0.2100, current_lr: 0.000449
step 3485, loss: 0.2258, current_lr: 0.000449
step 3486, loss: 0.1624, current_lr: 0.000449
step 3487, loss: 0.1938, current_lr: 0.000449
step 3488, loss: 0.1936, current_lr: 0.000449
step 3489, loss: 0.1934, current_lr: 0.000449
step 3490, loss: 0.1952, current_lr: 0.000450
step 3491, loss: 0.1754, current_lr: 0.000450
step 3492, loss: 0.1767, current_lr: 0.000450
step 3493, loss: 0.1821, current_lr: 0.000450
step 3494, loss: 0.1783, current_lr: 0.000450
step 3495, loss: 0.2084, current_lr: 0.000450
step 3496, loss: 0.2044, current_lr: 0.000451
step 3497, loss: 0.1942, current_lr: 0.000451
step 3498, loss: 0.1899, current_lr: 0.000451
step 3499, loss: 0.1776, current_lr: 0.000451
step 3500, loss: 0.1820, current_lr: 0.000451
step 3501, loss: 0.1751, current_lr: 0.000451
step 3502, loss: 0.1781, current_lr: 0.000452
step 3503, loss: 0.1756, current_lr: 0.000452
step 3504, loss: 0.1782, current_lr: 0.000452
step 3505, loss: 0.1641, current_lr: 0.000452
step 3506, loss: 0.1813, current_lr: 0.000452
step 3507, loss: 0.1683, current_lr: 0.000452
step 3508, loss: 0.1618, current_lr: 0.000453
step 3509, loss: 0.1554, current_lr: 0.000453
step 3510, loss: 0.1926, current_lr: 0.000453
step 3511, loss: 0.1525, current_lr: 0.000453
step 3512, loss: 0.2136, current_lr: 0.000453
step 3513, loss: 0.1920, current_lr: 0.000453
step 3514, loss: 0.1939, current_lr: 0.000454
step 3515, loss: 0.1763, current_lr: 0.000454
step 3516, loss: 0.1864, current_lr: 0.000454
step 3517, loss: 0.2149, current_lr: 0.000454
step 3518, loss: 0.1798, current_lr: 0.000454
step 3519, loss: 0.1662, current_lr: 0.000454
step 3520, loss: 0.1582, current_lr: 0.000455
step 3521, loss: 0.1613, current_lr: 0.000455
step 3522, loss: 0.1720, current_lr: 0.000455
step 3523, loss: 0.1830, current_lr: 0.000455
step 3524, loss: 0.1818, current_lr: 0.000455
step 3525, loss: 0.1626, current_lr: 0.000455
step 3526, loss: 0.1733, current_lr: 0.000456
step 3527, loss: 0.1664, current_lr: 0.000456
step 3528, loss: 0.1816, current_lr: 0.000456
step 3529, loss: 0.1625, current_lr: 0.000456
step 3530, loss: 0.1443, current_lr: 0.000456
step 3531, loss: 0.1752, current_lr: 0.000456
step 3532, loss: 0.1648, current_lr: 0.000457
step 3533, loss: 0.1720, current_lr: 0.000457
step 3534, loss: 0.1716, current_lr: 0.000457
step 3535, loss: 0.1597, current_lr: 0.000457
step 3536, loss: 0.1570, current_lr: 0.000457
step 3537, loss: 0.1636, current_lr: 0.000458
step 3538, loss: 0.1619, current_lr: 0.000458
step 3539, loss: 0.1791, current_lr: 0.000458
step 3540, loss: 0.1743, current_lr: 0.000458
step 3541, loss: 0.1695, current_lr: 0.000458
step 3542, loss: 0.1642, current_lr: 0.000458
step 3543, loss: 0.1835, current_lr: 0.000459
step 3544, loss: 0.1516, current_lr: 0.000459
step 3545, loss: 0.1627, current_lr: 0.000459
step 3546, loss: 0.1849, current_lr: 0.000459
step 3547, loss: 0.1909, current_lr: 0.000459
step 3548, loss: 0.2030, current_lr: 0.000459
step 3549, loss: 0.1764, current_lr: 0.000460
step 3550, loss: 0.1755, current_lr: 0.000460
step 3551, loss: 0.1784, current_lr: 0.000460
step 3552, loss: 0.1924, current_lr: 0.000460
step 3553, loss: 0.1923, current_lr: 0.000460
step 3554, loss: 0.1600, current_lr: 0.000460
step 3555, loss: 0.1889, current_lr: 0.000461
step 3556, loss: 0.1710, current_lr: 0.000461
step 3557, loss: 0.1685, current_lr: 0.000461
step 3558, loss: 0.1574, current_lr: 0.000461
step 3559, loss: 0.1588, current_lr: 0.000461
step 3560, loss: 0.1561, current_lr: 0.000461
step 3561, loss: 0.1812, current_lr: 0.000462
step 3562, loss: 0.1967, current_lr: 0.000462
step 3563, loss: 0.1925, current_lr: 0.000462
step 3564, loss: 0.1595, current_lr: 0.000462
step 3565, loss: 0.1828, current_lr: 0.000462
step 3566, loss: 0.1670, current_lr: 0.000462
step 3567, loss: 0.1474, current_lr: 0.000463
step 3568, loss: 0.1640, current_lr: 0.000463
step 3569, loss: 0.1762, current_lr: 0.000463
step 3570, loss: 0.1926, current_lr: 0.000463
step 3571, loss: 0.1854, current_lr: 0.000463
step 3572, loss: 0.1988, current_lr: 0.000463
step 3573, loss: 0.2026, current_lr: 0.000464
step 3574, loss: 0.2068, current_lr: 0.000464
step 3575, loss: 0.1926, current_lr: 0.000464
step 3576, loss: 0.1823, current_lr: 0.000464
step 3577, loss: 0.2011, current_lr: 0.000464
step 3578, loss: 0.2082, current_lr: 0.000464
step 3579, loss: 0.1778, current_lr: 0.000465
step 3580, loss: 0.2106, current_lr: 0.000465
step 3581, loss: 0.1859, current_lr: 0.000465
step 3582, loss: 0.1969, current_lr: 0.000465
step 3583, loss: 0.1842, current_lr: 0.000465
step 3584, loss: 0.2075, current_lr: 0.000465
step 3585, loss: 0.2141, current_lr: 0.000466
step 3586, loss: 0.1880, current_lr: 0.000466
step 3587, loss: 0.2219, current_lr: 0.000466
step 3588, loss: 0.1784, current_lr: 0.000466
step 3589, loss: 0.1885, current_lr: 0.000466
step 3590, loss: 0.1759, current_lr: 0.000466
step 3591, loss: 0.1872, current_lr: 0.000467
step 3592, loss: 0.1728, current_lr: 0.000467
step 3593, loss: 0.1890, current_lr: 0.000467
step 3594, loss: 0.1813, current_lr: 0.000467
step 3595, loss: 0.1734, current_lr: 0.000467
step 3596, loss: 0.1810, current_lr: 0.000467
step 3597, loss: 0.1991, current_lr: 0.000468
step 3598, loss: 0.1826, current_lr: 0.000468
step 3599, loss: 0.2265, current_lr: 0.000468
step 3600, loss: 0.1820, current_lr: 0.000468
step 3601, loss: 0.1651, current_lr: 0.000468
step 3602, loss: 0.1745, current_lr: 0.000468
step 3603, loss: 0.1588, current_lr: 0.000469
step 3604, loss: 0.1735, current_lr: 0.000469
step 3605, loss: 0.1742, current_lr: 0.000469
step 3606, loss: 0.1765, current_lr: 0.000469
step 3607, loss: 0.1945, current_lr: 0.000469
step 3608, loss: 0.1874, current_lr: 0.000469
step 3609, loss: 0.1977, current_lr: 0.000470
step 3610, loss: 0.1851, current_lr: 0.000470
step 3611, loss: 0.1963, current_lr: 0.000470
step 3612, loss: 0.1950, current_lr: 0.000470
step 3613, loss: 0.1664, current_lr: 0.000470
step 3614, loss: 0.1607, current_lr: 0.000470
step 3615, loss: 0.1816, current_lr: 0.000471
step 3616, loss: 0.1621, current_lr: 0.000471
step 3617, loss: 0.2105, current_lr: 0.000471
step 3618, loss: 0.1922, current_lr: 0.000471
step 3619, loss: 0.1703, current_lr: 0.000471
step 3620, loss: 0.2114, current_lr: 0.000471
step 3621, loss: 0.1718, current_lr: 0.000472
step 3622, loss: 0.1503, current_lr: 0.000472
step 3623, loss: 0.1504, current_lr: 0.000472
step 3624, loss: 0.1760, current_lr: 0.000472
step 3625, loss: 0.1953, current_lr: 0.000472
step 3626, loss: 0.1687, current_lr: 0.000472
step 3627, loss: 0.1413, current_lr: 0.000473
step 3628, loss: 0.1993, current_lr: 0.000473
step 3629, loss: 0.1376, current_lr: 0.000473
Saved best model with loss: 0.1376 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1376.pt
step 3630, loss: 0.1662, current_lr: 0.000473
step 3631, loss: 0.1761, current_lr: 0.000473
step 3632, loss: 0.1998, current_lr: 0.000473
step 3633, loss: 0.1577, current_lr: 0.000474
step 3634, loss: 0.1933, current_lr: 0.000474
step 3635, loss: 0.1767, current_lr: 0.000474
step 3636, loss: 0.1547, current_lr: 0.000474
step 3637, loss: 0.1854, current_lr: 0.000474
step 3638, loss: 0.1642, current_lr: 0.000474
step 3639, loss: 0.1949, current_lr: 0.000475
step 3640, loss: 0.1541, current_lr: 0.000475
step 3641, loss: 0.1669, current_lr: 0.000475
step 3642, loss: 0.1692, current_lr: 0.000475
step 3643, loss: 0.1969, current_lr: 0.000475
step 3644, loss: 0.1952, current_lr: 0.000475
step 3645, loss: 0.1923, current_lr: 0.000476
step 3646, loss: 0.1866, current_lr: 0.000476
step 3647, loss: 0.1766, current_lr: 0.000476
step 3648, loss: 0.1674, current_lr: 0.000476
step 3649, loss: 0.1875, current_lr: 0.000476
step 3650, loss: 0.2068, current_lr: 0.000476
step 3651, loss: 0.1603, current_lr: 0.000477
step 3652, loss: 0.1834, current_lr: 0.000477
step 3653, loss: 0.2219, current_lr: 0.000477
step 3654, loss: 0.1912, current_lr: 0.000477
step 3655, loss: 0.1863, current_lr: 0.000477
step 3656, loss: 0.1989, current_lr: 0.000477
step 3657, loss: 0.1919, current_lr: 0.000478
step 3658, loss: 0.1975, current_lr: 0.000478
step 3659, loss: 0.1665, current_lr: 0.000478
step 3660, loss: 0.1970, current_lr: 0.000478
step 3661, loss: 0.1879, current_lr: 0.000478
step 3662, loss: 0.2074, current_lr: 0.000478
step 3663, loss: 0.1455, current_lr: 0.000479
step 3664, loss: 0.1783, current_lr: 0.000479
step 3665, loss: 0.1735, current_lr: 0.000479
step 3666, loss: 0.1896, current_lr: 0.000479
step 3667, loss: 0.1585, current_lr: 0.000479
step 3668, loss: 0.1681, current_lr: 0.000479
step 3669, loss: 0.1997, current_lr: 0.000480
step 3670, loss: 0.1719, current_lr: 0.000480
step 3671, loss: 0.2026, current_lr: 0.000480
step 3672, loss: 0.1653, current_lr: 0.000480
step 3673, loss: 0.1400, current_lr: 0.000480
step 3674, loss: 0.1513, current_lr: 0.000480
step 3675, loss: 0.1833, current_lr: 0.000481
step 3676, loss: 0.1663, current_lr: 0.000481
step 3677, loss: 0.2081, current_lr: 0.000481
step 3678, loss: 0.1927, current_lr: 0.000481
step 3679, loss: 0.1744, current_lr: 0.000481
step 3680, loss: 0.1608, current_lr: 0.000481
step 3681, loss: 0.2097, current_lr: 0.000482
step 3682, loss: 0.1928, current_lr: 0.000482
step 3683, loss: 0.1759, current_lr: 0.000482
step 3684, loss: 0.1827, current_lr: 0.000482
step 3685, loss: 0.1904, current_lr: 0.000482
step 3686, loss: 0.1847, current_lr: 0.000482
step 3687, loss: 0.1856, current_lr: 0.000483
step 3688, loss: 0.1757, current_lr: 0.000483
step 3689, loss: 0.1649, current_lr: 0.000483
step 3690, loss: 0.2019, current_lr: 0.000483
step 3691, loss: 0.1834, current_lr: 0.000483
step 3692, loss: 0.1717, current_lr: 0.000483
step 3693, loss: 0.1768, current_lr: 0.000484
step 3694, loss: 0.1845, current_lr: 0.000484
step 3695, loss: 0.1710, current_lr: 0.000484
step 3696, loss: 0.2147, current_lr: 0.000484
step 3697, loss: 0.1818, current_lr: 0.000484
step 3698, loss: 0.1804, current_lr: 0.000484
step 3699, loss: 0.1915, current_lr: 0.000485
step 3700, loss: 0.1537, current_lr: 0.000485
step 3701, loss: 0.1859, current_lr: 0.000485
step 3702, loss: 0.1489, current_lr: 0.000485
step 3703, loss: 0.1780, current_lr: 0.000485
step 3704, loss: 0.1588, current_lr: 0.000485
step 3705, loss: 0.2109, current_lr: 0.000485
step 3706, loss: 0.1575, current_lr: 0.000486
step 3707, loss: 0.1869, current_lr: 0.000486
step 3708, loss: 0.1855, current_lr: 0.000486
step 3709, loss: 0.1552, current_lr: 0.000486
step 3710, loss: 0.1636, current_lr: 0.000486
step 3711, loss: 0.2002, current_lr: 0.000486
step 3712, loss: 0.1554, current_lr: 0.000487
step 3713, loss: 0.1763, current_lr: 0.000487
step 3714, loss: 0.1685, current_lr: 0.000487
step 3715, loss: 0.1669, current_lr: 0.000487
step 3716, loss: 0.2104, current_lr: 0.000487
step 3717, loss: 0.1863, current_lr: 0.000487
step 3718, loss: 0.1986, current_lr: 0.000488
step 3719, loss: 0.1675, current_lr: 0.000488
step 3720, loss: 0.1715, current_lr: 0.000488
step 3721, loss: 0.2069, current_lr: 0.000488
step 3722, loss: 0.2170, current_lr: 0.000488
step 3723, loss: 0.1654, current_lr: 0.000488
step 3724, loss: 0.1721, current_lr: 0.000489
step 3725, loss: 0.1543, current_lr: 0.000489
step 3726, loss: 0.1712, current_lr: 0.000489
step 3727, loss: 0.1630, current_lr: 0.000489
step 3728, loss: 0.1909, current_lr: 0.000489
step 3729, loss: 0.1965, current_lr: 0.000489
step 3730, loss: 0.1912, current_lr: 0.000490
step 3731, loss: 0.2291, current_lr: 0.000490
step 3732, loss: 0.1869, current_lr: 0.000490
step 3733, loss: 0.2240, current_lr: 0.000490
step 3734, loss: 0.2301, current_lr: 0.000490
step 3735, loss: 0.2174, current_lr: 0.000490
step 3736, loss: 0.2198, current_lr: 0.000491
step 3737, loss: 0.2008, current_lr: 0.000491
step 3738, loss: 0.2044, current_lr: 0.000491
step 3739, loss: 0.1955, current_lr: 0.000491
step 3740, loss: 0.2185, current_lr: 0.000491
step 3741, loss: 0.2040, current_lr: 0.000491
step 3742, loss: 0.2036, current_lr: 0.000492
step 3743, loss: 0.1905, current_lr: 0.000492
step 3744, loss: 0.1817, current_lr: 0.000492
step 3745, loss: 0.2175, current_lr: 0.000492
step 3746, loss: 0.2225, current_lr: 0.000492
step 3747, loss: 0.2295, current_lr: 0.000492
step 3748, loss: 0.2432, current_lr: 0.000493
step 3749, loss: 0.2168, current_lr: 0.000493
step 3750, loss: 0.2047, current_lr: 0.000493
step 3751, loss: 0.1901, current_lr: 0.000493
step 3752, loss: 0.1995, current_lr: 0.000493
step 3753, loss: 0.1974, current_lr: 0.000493
step 3754, loss: 0.2154, current_lr: 0.000493
step 3755, loss: 0.1949, current_lr: 0.000494
step 3756, loss: 0.1974, current_lr: 0.000494
step 3757, loss: 0.2055, current_lr: 0.000494
step 3758, loss: 0.1890, current_lr: 0.000494
step 3759, loss: 0.1867, current_lr: 0.000494
step 3760, loss: 0.1840, current_lr: 0.000494
step 3761, loss: 0.2019, current_lr: 0.000495
step 3762, loss: 0.2209, current_lr: 0.000495
step 3763, loss: 0.2141, current_lr: 0.000495
step 3764, loss: 0.2045, current_lr: 0.000495
step 3765, loss: 0.1882, current_lr: 0.000495
step 3766, loss: 0.1765, current_lr: 0.000495
step 3767, loss: 0.2200, current_lr: 0.000496
step 3768, loss: 0.2104, current_lr: 0.000496
step 3769, loss: 0.1942, current_lr: 0.000496
step 3770, loss: 0.1673, current_lr: 0.000496
step 3771, loss: 0.1706, current_lr: 0.000496
step 3772, loss: 0.1969, current_lr: 0.000496
step 3773, loss: 0.2309, current_lr: 0.000497
step 3774, loss: 0.2048, current_lr: 0.000497
step 3775, loss: 0.2259, current_lr: 0.000497
step 3776, loss: 0.2109, current_lr: 0.000497
step 3777, loss: 0.1953, current_lr: 0.000497
step 3778, loss: 0.1627, current_lr: 0.000497
step 3779, loss: 0.1836, current_lr: 0.000498
step 3780, loss: 0.1976, current_lr: 0.000498
step 3781, loss: 0.1999, current_lr: 0.000498
step 3782, loss: 0.2027, current_lr: 0.000498
step 3783, loss: 0.1864, current_lr: 0.000498
step 3784, loss: 0.1474, current_lr: 0.000498
step 3785, loss: 0.2062, current_lr: 0.000499
step 3786, loss: 0.1741, current_lr: 0.000499
step 3787, loss: 0.1545, current_lr: 0.000499
step 3788, loss: 0.1666, current_lr: 0.000499
step 3789, loss: 0.1899, current_lr: 0.000499
step 3790, loss: 0.1916, current_lr: 0.000499
step 3791, loss: 0.2259, current_lr: 0.000499
step 3792, loss: 0.1835, current_lr: 0.000500
step 3793, loss: 0.1918, current_lr: 0.000500
step 3794, loss: 0.1643, current_lr: 0.000500
step 3795, loss: 0.1711, current_lr: 0.000500
step 3796, loss: 0.1676, current_lr: 0.000500
step 3797, loss: 0.1586, current_lr: 0.000500
step 3798, loss: 0.1610, current_lr: 0.000501
step 3799, loss: 0.1818, current_lr: 0.000501
step 3800, loss: 0.1656, current_lr: 0.000501
step 3801, loss: 0.1661, current_lr: 0.000501
step 3802, loss: 0.1730, current_lr: 0.000501
step 3803, loss: 0.1826, current_lr: 0.000501
step 3804, loss: 0.1872, current_lr: 0.000502
step 3805, loss: 0.1566, current_lr: 0.000502
step 3806, loss: 0.1956, current_lr: 0.000502
step 3807, loss: 0.1726, current_lr: 0.000502
step 3808, loss: 0.1804, current_lr: 0.000502
step 3809, loss: 0.2006, current_lr: 0.000502
step 3810, loss: 0.2087, current_lr: 0.000503
step 3811, loss: 0.1804, current_lr: 0.000503
step 3812, loss: 0.1883, current_lr: 0.000503
step 3813, loss: 0.2165, current_lr: 0.000503
step 3814, loss: 0.1662, current_lr: 0.000503
step 3815, loss: 0.2044, current_lr: 0.000503
step 3816, loss: 0.1802, current_lr: 0.000503
step 3817, loss: 0.1853, current_lr: 0.000504
step 3818, loss: 0.2042, current_lr: 0.000504
step 3819, loss: 0.1632, current_lr: 0.000504
step 3820, loss: 0.1381, current_lr: 0.000504
step 3821, loss: 0.1744, current_lr: 0.000504
step 3822, loss: 0.1637, current_lr: 0.000504
step 3823, loss: 0.1623, current_lr: 0.000505
step 3824, loss: 0.1634, current_lr: 0.000505
step 3825, loss: 0.2578, current_lr: 0.000505
step 3826, loss: 0.2007, current_lr: 0.000505
step 3827, loss: 0.1890, current_lr: 0.000505
step 3828, loss: 0.1529, current_lr: 0.000505
step 3829, loss: 0.1885, current_lr: 0.000506
step 3830, loss: 0.2238, current_lr: 0.000506
step 3831, loss: 0.2170, current_lr: 0.000506
step 3832, loss: 0.2110, current_lr: 0.000506
step 3833, loss: 0.1666, current_lr: 0.000506
step 3834, loss: 0.1727, current_lr: 0.000506
step 3835, loss: 0.1617, current_lr: 0.000507
step 3836, loss: 0.1699, current_lr: 0.000507
step 3837, loss: 0.1487, current_lr: 0.000507
step 3838, loss: 0.1751, current_lr: 0.000507
step 3839, loss: 0.2008, current_lr: 0.000507
step 3840, loss: 0.2658, current_lr: 0.000507
step 3841, loss: 0.2148, current_lr: 0.000507
step 3842, loss: 0.2031, current_lr: 0.000508
step 3843, loss: 0.2188, current_lr: 0.000508
step 3844, loss: 0.1816, current_lr: 0.000508
step 3845, loss: 0.1655, current_lr: 0.000508
step 3846, loss: 0.2139, current_lr: 0.000508
step 3847, loss: 0.2109, current_lr: 0.000508
step 3848, loss: 0.2136, current_lr: 0.000509
step 3849, loss: 0.1900, current_lr: 0.000509
step 3850, loss: 0.1980, current_lr: 0.000509
step 3851, loss: 0.1783, current_lr: 0.000509
step 3852, loss: 0.1720, current_lr: 0.000509
step 3853, loss: 0.1972, current_lr: 0.000509
step 3854, loss: 0.1852, current_lr: 0.000510
step 3855, loss: 0.1778, current_lr: 0.000510
step 3856, loss: 0.2160, current_lr: 0.000510
step 3857, loss: 0.2330, current_lr: 0.000510
step 3858, loss: 0.2264, current_lr: 0.000510
step 3859, loss: 0.2090, current_lr: 0.000510
step 3860, loss: 0.2196, current_lr: 0.000510
step 3861, loss: 0.2297, current_lr: 0.000511
step 3862, loss: 0.2110, current_lr: 0.000511
step 3863, loss: 0.1997, current_lr: 0.000511
step 3864, loss: 0.2857, current_lr: 0.000511
step 3865, loss: 0.2227, current_lr: 0.000511
step 3866, loss: 0.2125, current_lr: 0.000511
step 3867, loss: 0.1851, current_lr: 0.000512
step 3868, loss: 0.1929, current_lr: 0.000512
step 3869, loss: 0.2000, current_lr: 0.000512
step 3870, loss: 0.2073, current_lr: 0.000512
step 3871, loss: 0.2002, current_lr: 0.000512
step 3872, loss: 0.2033, current_lr: 0.000512
step 3873, loss: 0.2257, current_lr: 0.000513
step 3874, loss: 0.1921, current_lr: 0.000513
step 3875, loss: 0.2003, current_lr: 0.000513
step 3876, loss: 0.2505, current_lr: 0.000513
step 3877, loss: 0.2036, current_lr: 0.000513
step 3878, loss: 0.1918, current_lr: 0.000513
step 3879, loss: 0.2075, current_lr: 0.000513
step 3880, loss: 0.1969, current_lr: 0.000514
step 3881, loss: 0.2224, current_lr: 0.000514
step 3882, loss: 0.1807, current_lr: 0.000514
step 3883, loss: 0.2113, current_lr: 0.000514
step 3884, loss: 0.1787, current_lr: 0.000514
step 3885, loss: 0.1905, current_lr: 0.000514
step 3886, loss: 0.2330, current_lr: 0.000515
step 3887, loss: 0.2099, current_lr: 0.000515
step 3888, loss: 0.2175, current_lr: 0.000515
step 3889, loss: 0.2012, current_lr: 0.000515
step 3890, loss: 0.1767, current_lr: 0.000515
step 3891, loss: 0.1733, current_lr: 0.000515
step 3892, loss: 0.2263, current_lr: 0.000516
step 3893, loss: 0.1750, current_lr: 0.000516
step 3894, loss: 0.1828, current_lr: 0.000516
step 3895, loss: 0.1823, current_lr: 0.000516
step 3896, loss: 0.2179, current_lr: 0.000516
step 3897, loss: 0.1992, current_lr: 0.000516
step 3898, loss: 0.2213, current_lr: 0.000516
step 3899, loss: 0.2738, current_lr: 0.000517
step 3900, loss: 0.2472, current_lr: 0.000517
step 3901, loss: 0.2376, current_lr: 0.000517
step 3902, loss: 0.2390, current_lr: 0.000517
step 3903, loss: 0.2596, current_lr: 0.000517
step 3904, loss: 0.2236, current_lr: 0.000517
step 3905, loss: 0.2699, current_lr: 0.000518
step 3906, loss: 0.1946, current_lr: 0.000518
step 3907, loss: 0.2478, current_lr: 0.000518
step 3908, loss: 0.2260, current_lr: 0.000518
step 3909, loss: 0.2647, current_lr: 0.000518
step 3910, loss: 0.2413, current_lr: 0.000518
step 3911, loss: 0.2176, current_lr: 0.000519
step 3912, loss: 0.2258, current_lr: 0.000519
step 3913, loss: 0.2502, current_lr: 0.000519
step 3914, loss: 0.2429, current_lr: 0.000519
step 3915, loss: 0.2578, current_lr: 0.000519
step 3916, loss: 0.2189, current_lr: 0.000519
step 3917, loss: 0.2622, current_lr: 0.000519
step 3918, loss: 0.2195, current_lr: 0.000520
step 3919, loss: 0.2055, current_lr: 0.000520
step 3920, loss: 0.2330, current_lr: 0.000520
step 3921, loss: 0.2551, current_lr: 0.000520
step 3922, loss: 0.2147, current_lr: 0.000520
step 3923, loss: 0.2511, current_lr: 0.000520
step 3924, loss: 0.2400, current_lr: 0.000521
step 3925, loss: 0.2112, current_lr: 0.000521
step 3926, loss: 0.2315, current_lr: 0.000521
step 3927, loss: 0.2250, current_lr: 0.000521
step 3928, loss: 0.2610, current_lr: 0.000521
step 3929, loss: 0.2547, current_lr: 0.000521
step 3930, loss: 0.2095, current_lr: 0.000521
step 3931, loss: 0.2471, current_lr: 0.000522
step 3932, loss: 0.2253, current_lr: 0.000522
step 3933, loss: 0.2213, current_lr: 0.000522
step 3934, loss: 0.2068, current_lr: 0.000522
step 3935, loss: 0.2068, current_lr: 0.000522
step 3936, loss: 0.2262, current_lr: 0.000522
step 3937, loss: 0.2696, current_lr: 0.000523
step 3938, loss: 0.2673, current_lr: 0.000523
step 3939, loss: 0.2392, current_lr: 0.000523
step 3940, loss: 0.2285, current_lr: 0.000523
step 3941, loss: 0.2444, current_lr: 0.000523
step 3942, loss: 0.2250, current_lr: 0.000523
step 3943, loss: 0.1965, current_lr: 0.000523
step 3944, loss: 0.2112, current_lr: 0.000524
step 3945, loss: 0.2127, current_lr: 0.000524
step 3946, loss: 0.2115, current_lr: 0.000524
step 3947, loss: 0.2408, current_lr: 0.000524
step 3948, loss: 0.2285, current_lr: 0.000524
step 3949, loss: 0.2150, current_lr: 0.000524
step 3950, loss: 0.1941, current_lr: 0.000525
step 3951, loss: 0.2000, current_lr: 0.000525
step 3952, loss: 0.2131, current_lr: 0.000525
step 3953, loss: 0.1534, current_lr: 0.000525
step 3954, loss: 0.2399, current_lr: 0.000525
step 3955, loss: 0.2499, current_lr: 0.000525
step 3956, loss: 0.2598, current_lr: 0.000526
step 3957, loss: 0.2409, current_lr: 0.000526
step 3958, loss: 0.2074, current_lr: 0.000526
step 3959, loss: 0.1959, current_lr: 0.000526
step 3960, loss: 0.2338, current_lr: 0.000526
step 3961, loss: 0.2361, current_lr: 0.000526
step 3962, loss: 0.2388, current_lr: 0.000526
step 3963, loss: 0.2092, current_lr: 0.000527
step 3964, loss: 0.1910, current_lr: 0.000527
step 3965, loss: 0.2261, current_lr: 0.000527
step 3966, loss: 0.2077, current_lr: 0.000527
step 3967, loss: 0.1957, current_lr: 0.000527
step 3968, loss: 0.2374, current_lr: 0.000527
step 3969, loss: 0.2212, current_lr: 0.000528
step 3970, loss: 0.2055, current_lr: 0.000528
step 3971, loss: 0.2256, current_lr: 0.000528
step 3972, loss: 0.2179, current_lr: 0.000528
step 3973, loss: 0.1996, current_lr: 0.000528
step 3974, loss: 0.2165, current_lr: 0.000528
step 3975, loss: 0.2034, current_lr: 0.000528
step 3976, loss: 0.2171, current_lr: 0.000529
step 3977, loss: 0.2037, current_lr: 0.000529
step 3978, loss: 0.2355, current_lr: 0.000529
step 3979, loss: 0.2357, current_lr: 0.000529
step 3980, loss: 0.2349, current_lr: 0.000529
step 3981, loss: 0.2192, current_lr: 0.000529
step 3982, loss: 0.2653, current_lr: 0.000530
step 3983, loss: 0.2343, current_lr: 0.000530
step 3984, loss: 0.2276, current_lr: 0.000530
step 3985, loss: 0.2166, current_lr: 0.000530
step 3986, loss: 0.1993, current_lr: 0.000530
step 3987, loss: 0.2038, current_lr: 0.000530
step 3988, loss: 0.2413, current_lr: 0.000530
step 3989, loss: 0.2046, current_lr: 0.000531
step 3990, loss: 0.2493, current_lr: 0.000531
step 3991, loss: 0.2218, current_lr: 0.000531
step 3992, loss: 0.2017, current_lr: 0.000531
step 3993, loss: 0.2090, current_lr: 0.000531
step 3994, loss: 0.2270, current_lr: 0.000531
step 3995, loss: 0.2125, current_lr: 0.000531
step 3996, loss: 0.2358, current_lr: 0.000532
step 3997, loss: 0.2376, current_lr: 0.000532
step 3998, loss: 0.2448, current_lr: 0.000532
step 3999, loss: 0.2508, current_lr: 0.000532
step 4000, loss: 0.2978, current_lr: 0.000532
step 4001, loss: 0.1851, current_lr: 0.000532
step 4002, loss: 0.2199, current_lr: 0.000533
step 4003, loss: 0.2077, current_lr: 0.000533
step 4004, loss: 0.2185, current_lr: 0.000533
step 4005, loss: 0.2835, current_lr: 0.000533
step 4006, loss: 0.2526, current_lr: 0.000533
step 4007, loss: 0.2637, current_lr: 0.000533
step 4008, loss: 0.2289, current_lr: 0.000533
step 4009, loss: 0.2354, current_lr: 0.000534
step 4010, loss: 0.2369, current_lr: 0.000534
step 4011, loss: 0.2637, current_lr: 0.000534
step 4012, loss: 0.2731, current_lr: 0.000534
step 4013, loss: 0.2694, current_lr: 0.000534
step 4014, loss: 0.2276, current_lr: 0.000534
step 4015, loss: 0.2597, current_lr: 0.000535
step 4016, loss: 0.2324, current_lr: 0.000535
step 4017, loss: 0.2093, current_lr: 0.000535
step 4018, loss: 0.2279, current_lr: 0.000535
step 4019, loss: 0.2557, current_lr: 0.000535
step 4020, loss: 0.2560, current_lr: 0.000535
step 4021, loss: 0.2377, current_lr: 0.000535
step 4022, loss: 0.2743, current_lr: 0.000536
step 4023, loss: 0.2599, current_lr: 0.000536
step 4024, loss: 0.2613, current_lr: 0.000536
step 4025, loss: 0.2802, current_lr: 0.000536
step 4026, loss: 0.2703, current_lr: 0.000536
step 4027, loss: 0.2599, current_lr: 0.000536
step 4028, loss: 0.2778, current_lr: 0.000537
step 4029, loss: 0.3115, current_lr: 0.000537
step 4030, loss: 0.2217, current_lr: 0.000537
step 4031, loss: 0.3284, current_lr: 0.000537
step 4032, loss: 0.2552, current_lr: 0.000537
step 4033, loss: 0.2389, current_lr: 0.000537
step 4034, loss: 0.2431, current_lr: 0.000537
step 4035, loss: 0.2721, current_lr: 0.000538
step 4036, loss: 0.2412, current_lr: 0.000538
step 4037, loss: 0.2430, current_lr: 0.000538
step 4038, loss: 0.2662, current_lr: 0.000538
step 4039, loss: 0.2610, current_lr: 0.000538
step 4040, loss: 0.2528, current_lr: 0.000538
step 4041, loss: 0.2725, current_lr: 0.000538
step 4042, loss: 0.2981, current_lr: 0.000539
step 4043, loss: 0.2577, current_lr: 0.000539
step 4044, loss: 0.2792, current_lr: 0.000539
step 4045, loss: 0.2238, current_lr: 0.000539
step 4046, loss: 0.2409, current_lr: 0.000539
step 4047, loss: 0.2428, current_lr: 0.000539
step 4048, loss: 0.2646, current_lr: 0.000540
step 4049, loss: 0.2611, current_lr: 0.000540
step 4050, loss: 0.2377, current_lr: 0.000540
step 4051, loss: 0.2574, current_lr: 0.000540
step 4052, loss: 0.3082, current_lr: 0.000540
step 4053, loss: 0.2765, current_lr: 0.000540
step 4054, loss: 0.2532, current_lr: 0.000540
step 4055, loss: 0.2819, current_lr: 0.000541
step 4056, loss: 0.2409, current_lr: 0.000541
step 4057, loss: 0.2572, current_lr: 0.000541
step 4058, loss: 0.2937, current_lr: 0.000541
step 4059, loss: 0.2967, current_lr: 0.000541
step 4060, loss: 0.2357, current_lr: 0.000541
step 4061, loss: 0.2591, current_lr: 0.000541
step 4062, loss: 0.2560, current_lr: 0.000542
step 4063, loss: 0.2586, current_lr: 0.000542
step 4064, loss: 0.2971, current_lr: 0.000542
step 4065, loss: 0.3002, current_lr: 0.000542
step 4066, loss: 0.2975, current_lr: 0.000542
step 4067, loss: 0.2911, current_lr: 0.000542
step 4068, loss: 0.2757, current_lr: 0.000543
step 4069, loss: 0.2961, current_lr: 0.000543
step 4070, loss: 0.3022, current_lr: 0.000543
step 4071, loss: 0.3027, current_lr: 0.000543
step 4072, loss: 0.3108, current_lr: 0.000543
step 4073, loss: 0.2826, current_lr: 0.000543
step 4074, loss: 0.3153, current_lr: 0.000543
step 4075, loss: 0.2688, current_lr: 0.000544
step 4076, loss: 0.2801, current_lr: 0.000544
step 4077, loss: 0.2760, current_lr: 0.000544
step 4078, loss: 0.2944, current_lr: 0.000544
step 4079, loss: 0.2945, current_lr: 0.000544
step 4080, loss: 0.3694, current_lr: 0.000544
step 4081, loss: 0.3122, current_lr: 0.000544
step 4082, loss: 0.3368, current_lr: 0.000545
step 4083, loss: 0.3109, current_lr: 0.000545
step 4084, loss: 0.2636, current_lr: 0.000545
step 4085, loss: 0.2881, current_lr: 0.000545
step 4086, loss: 0.2932, current_lr: 0.000545
step 4087, loss: 0.2216, current_lr: 0.000545
step 4088, loss: 0.2857, current_lr: 0.000545
step 4089, loss: 0.2381, current_lr: 0.000546
step 4090, loss: 0.2888, current_lr: 0.000546
step 4091, loss: 0.2905, current_lr: 0.000546
step 4092, loss: 0.2695, current_lr: 0.000546
step 4093, loss: 0.2844, current_lr: 0.000546
step 4094, loss: 0.2900, current_lr: 0.000546
step 4095, loss: 0.2692, current_lr: 0.000547
step 4096, loss: 0.2954, current_lr: 0.000547
step 4097, loss: 0.2833, current_lr: 0.000547
step 4098, loss: 0.2792, current_lr: 0.000547
step 4099, loss: 0.2759, current_lr: 0.000547
step 4100, loss: 0.2609, current_lr: 0.000547
step 4101, loss: 0.2626, current_lr: 0.000547
step 4102, loss: 0.2989, current_lr: 0.000548
step 4103, loss: 0.3092, current_lr: 0.000548
step 4104, loss: 0.3004, current_lr: 0.000548
step 4105, loss: 0.3027, current_lr: 0.000548
step 4106, loss: 0.2753, current_lr: 0.000548
step 4107, loss: 0.2754, current_lr: 0.000548
step 4108, loss: 0.2920, current_lr: 0.000548
step 4109, loss: 0.2868, current_lr: 0.000549
step 4110, loss: 0.3223, current_lr: 0.000549
step 4111, loss: 0.2781, current_lr: 0.000549
step 4112, loss: 0.3119, current_lr: 0.000549
step 4113, loss: 0.2616, current_lr: 0.000549
step 4114, loss: 0.2268, current_lr: 0.000549
step 4115, loss: 0.2606, current_lr: 0.000549
step 4116, loss: 0.2570, current_lr: 0.000550
step 4117, loss: 0.2393, current_lr: 0.000550
step 4118, loss: 0.2468, current_lr: 0.000550
step 4119, loss: 0.2743, current_lr: 0.000550
step 4120, loss: 0.2835, current_lr: 0.000550
step 4121, loss: 0.2597, current_lr: 0.000550
step 4122, loss: 0.2832, current_lr: 0.000550
step 4123, loss: 0.2536, current_lr: 0.000551
step 4124, loss: 0.2499, current_lr: 0.000551
step 4125, loss: 0.2919, current_lr: 0.000551
step 4126, loss: 0.2513, current_lr: 0.000551
step 4127, loss: 0.3064, current_lr: 0.000551
step 4128, loss: 0.2700, current_lr: 0.000551
step 4129, loss: 0.2884, current_lr: 0.000552
step 4130, loss: 0.2675, current_lr: 0.000552
step 4131, loss: 0.2757, current_lr: 0.000552
step 4132, loss: 0.2701, current_lr: 0.000552
step 4133, loss: 0.2692, current_lr: 0.000552
step 4134, loss: 0.2758, current_lr: 0.000552
step 4135, loss: 0.2587, current_lr: 0.000552
step 4136, loss: 0.2515, current_lr: 0.000553
step 4137, loss: 0.2832, current_lr: 0.000553
step 4138, loss: 0.2560, current_lr: 0.000553
step 4139, loss: 0.2936, current_lr: 0.000553
step 4140, loss: 0.2731, current_lr: 0.000553
step 4141, loss: 0.2589, current_lr: 0.000553
step 4142, loss: 0.2320, current_lr: 0.000553
step 4143, loss: 0.2495, current_lr: 0.000554
step 4144, loss: 0.2405, current_lr: 0.000554
step 4145, loss: 0.2755, current_lr: 0.000554
step 4146, loss: 0.2622, current_lr: 0.000554
step 4147, loss: 0.2891, current_lr: 0.000554
step 4148, loss: 0.2848, current_lr: 0.000554
step 4149, loss: 0.2477, current_lr: 0.000554
step 4150, loss: 0.2503, current_lr: 0.000555
step 4151, loss: 0.2823, current_lr: 0.000555
step 4152, loss: 0.2690, current_lr: 0.000555
step 4153, loss: 0.2659, current_lr: 0.000555
step 4154, loss: 0.2918, current_lr: 0.000555
step 4155, loss: 0.3001, current_lr: 0.000555
step 4156, loss: 0.2298, current_lr: 0.000555
step 4157, loss: 0.2597, current_lr: 0.000556
step 4158, loss: 0.2345, current_lr: 0.000556
step 4159, loss: 0.2563, current_lr: 0.000556
step 4160, loss: 0.2762, current_lr: 0.000556
step 4161, loss: 0.2553, current_lr: 0.000556
step 4162, loss: 0.2995, current_lr: 0.000556
step 4163, loss: 0.2701, current_lr: 0.000556
step 4164, loss: 0.2340, current_lr: 0.000557
step 4165, loss: 0.3067, current_lr: 0.000557
step 4166, loss: 0.2592, current_lr: 0.000557
step 4167, loss: 0.2305, current_lr: 0.000557
step 4168, loss: 0.3058, current_lr: 0.000557
step 4169, loss: 0.2507, current_lr: 0.000557
step 4170, loss: 0.3187, current_lr: 0.000557
step 4171, loss: 0.2652, current_lr: 0.000558
step 4172, loss: 0.3332, current_lr: 0.000558
step 4173, loss: 0.3031, current_lr: 0.000558
step 4174, loss: 0.2692, current_lr: 0.000558
step 4175, loss: 0.2723, current_lr: 0.000558
step 4176, loss: 0.2727, current_lr: 0.000558
step 4177, loss: 0.3052, current_lr: 0.000558
step 4178, loss: 0.2925, current_lr: 0.000559
step 4179, loss: 0.3055, current_lr: 0.000559
step 4180, loss: 0.3088, current_lr: 0.000559
step 4181, loss: 0.2948, current_lr: 0.000559
step 4182, loss: 0.3010, current_lr: 0.000559
step 4183, loss: 0.2831, current_lr: 0.000559
step 4184, loss: 0.2618, current_lr: 0.000559
step 4185, loss: 0.2683, current_lr: 0.000560
step 4186, loss: 0.2679, current_lr: 0.000560
step 4187, loss: 0.2495, current_lr: 0.000560
step 4188, loss: 0.3405, current_lr: 0.000560
step 4189, loss: 0.2841, current_lr: 0.000560
step 4190, loss: 0.3233, current_lr: 0.000560
step 4191, loss: 0.3159, current_lr: 0.000560
step 4192, loss: 0.2787, current_lr: 0.000561
step 4193, loss: 0.3101, current_lr: 0.000561
step 4194, loss: 0.3194, current_lr: 0.000561
step 4195, loss: 0.2912, current_lr: 0.000561
step 4196, loss: 0.3201, current_lr: 0.000561
step 4197, loss: 0.2641, current_lr: 0.000561
step 4198, loss: 0.2925, current_lr: 0.000561
step 4199, loss: 0.2813, current_lr: 0.000562
step 4200, loss: 0.2924, current_lr: 0.000562
step 4201, loss: 0.3508, current_lr: 0.000562
step 4202, loss: 0.2818, current_lr: 0.000562
step 4203, loss: 0.3345, current_lr: 0.000562
step 4204, loss: 0.3232, current_lr: 0.000562
step 4205, loss: 0.3009, current_lr: 0.000562
step 4206, loss: 0.2975, current_lr: 0.000563
step 4207, loss: 0.2954, current_lr: 0.000563
step 4208, loss: 0.3084, current_lr: 0.000563
step 4209, loss: 0.3114, current_lr: 0.000563
step 4210, loss: 0.3003, current_lr: 0.000563
step 4211, loss: 0.2919, current_lr: 0.000563
step 4212, loss: 0.3377, current_lr: 0.000563
step 4213, loss: 0.3091, current_lr: 0.000564
step 4214, loss: 0.3549, current_lr: 0.000564
step 4215, loss: 0.2717, current_lr: 0.000564
step 4216, loss: 0.3035, current_lr: 0.000564
step 4217, loss: 0.3539, current_lr: 0.000564
step 4218, loss: 0.3120, current_lr: 0.000564
step 4219, loss: 0.2250, current_lr: 0.000564
step 4220, loss: 0.3410, current_lr: 0.000565
step 4221, loss: 0.2710, current_lr: 0.000565
step 4222, loss: 0.3585, current_lr: 0.000565
step 4223, loss: 0.3140, current_lr: 0.000565
step 4224, loss: 0.3465, current_lr: 0.000565
step 4225, loss: 0.3404, current_lr: 0.000565
step 4226, loss: 0.2828, current_lr: 0.000565
step 4227, loss: 0.2766, current_lr: 0.000566
step 4228, loss: 0.3256, current_lr: 0.000566
step 4229, loss: 0.3522, current_lr: 0.000566
step 4230, loss: 0.3399, current_lr: 0.000566
step 4231, loss: 0.3527, current_lr: 0.000566
step 4232, loss: 0.3250, current_lr: 0.000566
step 4233, loss: 0.3025, current_lr: 0.000566
step 4234, loss: 0.3195, current_lr: 0.000567
step 4235, loss: 0.3206, current_lr: 0.000567
step 4236, loss: 0.3564, current_lr: 0.000567
step 4237, loss: 0.3720, current_lr: 0.000567
step 4238, loss: 0.3258, current_lr: 0.000567
step 4239, loss: 0.2916, current_lr: 0.000567
step 4240, loss: 0.3442, current_lr: 0.000567
step 4241, loss: 0.3529, current_lr: 0.000568
step 4242, loss: 0.3312, current_lr: 0.000568
step 4243, loss: 0.3212, current_lr: 0.000568
step 4244, loss: 0.2937, current_lr: 0.000568
step 4245, loss: 0.3390, current_lr: 0.000568
step 4246, loss: 0.3808, current_lr: 0.000568
step 4247, loss: 0.3420, current_lr: 0.000568
step 4248, loss: 0.3762, current_lr: 0.000569
step 4249, loss: 0.2886, current_lr: 0.000569
step 4250, loss: 0.3470, current_lr: 0.000569
step 4251, loss: 0.3075, current_lr: 0.000569
step 4252, loss: 0.3008, current_lr: 0.000569
step 4253, loss: 0.3051, current_lr: 0.000569
step 4254, loss: 0.3323, current_lr: 0.000569
step 4255, loss: 0.3222, current_lr: 0.000569
step 4256, loss: 0.3330, current_lr: 0.000570
step 4257, loss: 0.3185, current_lr: 0.000570
step 4258, loss: 0.3086, current_lr: 0.000570
step 4259, loss: 0.2992, current_lr: 0.000570
step 4260, loss: 0.2583, current_lr: 0.000570
step 4261, loss: 0.2935, current_lr: 0.000570
step 4262, loss: 0.2839, current_lr: 0.000570
step 4263, loss: 0.3093, current_lr: 0.000571
step 4264, loss: 0.2997, current_lr: 0.000571
step 4265, loss: 0.3079, current_lr: 0.000571
step 4266, loss: 0.3007, current_lr: 0.000571
step 4267, loss: 0.3526, current_lr: 0.000571
step 4268, loss: 0.3318, current_lr: 0.000571
step 4269, loss: 0.2750, current_lr: 0.000571
step 4270, loss: 0.2919, current_lr: 0.000572
step 4271, loss: 0.3353, current_lr: 0.000572
step 4272, loss: 0.2842, current_lr: 0.000572
step 4273, loss: 0.3143, current_lr: 0.000572
step 4274, loss: 0.2931, current_lr: 0.000572
step 4275, loss: 0.3413, current_lr: 0.000572
step 4276, loss: 0.2769, current_lr: 0.000572
step 4277, loss: 0.3040, current_lr: 0.000573
step 4278, loss: 0.2946, current_lr: 0.000573
step 4279, loss: 0.2531, current_lr: 0.000573
step 4280, loss: 0.2589, current_lr: 0.000573
step 4281, loss: 0.2487, current_lr: 0.000573
step 4282, loss: 0.2616, current_lr: 0.000573
step 4283, loss: 0.2579, current_lr: 0.000573
step 4284, loss: 0.2879, current_lr: 0.000573
step 4285, loss: 0.2944, current_lr: 0.000574
step 4286, loss: 0.2869, current_lr: 0.000574
step 4287, loss: 0.3049, current_lr: 0.000574
step 4288, loss: 0.2915, current_lr: 0.000574
step 4289, loss: 0.2851, current_lr: 0.000574
step 4290, loss: 0.2900, current_lr: 0.000574
step 4291, loss: 0.2947, current_lr: 0.000574
step 4292, loss: 0.2780, current_lr: 0.000575
step 4293, loss: 0.3030, current_lr: 0.000575
step 4294, loss: 0.2734, current_lr: 0.000575
step 4295, loss: 0.3039, current_lr: 0.000575
step 4296, loss: 0.2618, current_lr: 0.000575
step 4297, loss: 0.3026, current_lr: 0.000575
step 4298, loss: 0.2594, current_lr: 0.000575
step 4299, loss: 0.2711, current_lr: 0.000576
step 4300, loss: 0.2649, current_lr: 0.000576
step 4301, loss: 0.2431, current_lr: 0.000576
step 4302, loss: 0.3146, current_lr: 0.000576
step 4303, loss: 0.2701, current_lr: 0.000576
step 4304, loss: 0.2911, current_lr: 0.000576
step 4305, loss: 0.2573, current_lr: 0.000576
step 4306, loss: 0.3245, current_lr: 0.000577
step 4307, loss: 0.2534, current_lr: 0.000577
step 4308, loss: 0.2787, current_lr: 0.000577
step 4309, loss: 0.2936, current_lr: 0.000577
step 4310, loss: 0.2886, current_lr: 0.000577
step 4311, loss: 0.2684, current_lr: 0.000577
step 4312, loss: 0.2881, current_lr: 0.000577
step 4313, loss: 0.2692, current_lr: 0.000577
step 4314, loss: 0.2492, current_lr: 0.000578
step 4315, loss: 0.2607, current_lr: 0.000578
step 4316, loss: 0.3083, current_lr: 0.000578
step 4317, loss: 0.2823, current_lr: 0.000578
step 4318, loss: 0.2470, current_lr: 0.000578
step 4319, loss: 0.2539, current_lr: 0.000578
step 4320, loss: 0.2978, current_lr: 0.000578
step 4321, loss: 0.3055, current_lr: 0.000579
step 4322, loss: 0.2818, current_lr: 0.000579
step 4323, loss: 0.2450, current_lr: 0.000579
step 4324, loss: 0.2792, current_lr: 0.000579
step 4325, loss: 0.2770, current_lr: 0.000579
step 4326, loss: 0.2919, current_lr: 0.000579
step 4327, loss: 0.2859, current_lr: 0.000579
step 4328, loss: 0.2886, current_lr: 0.000579
step 4329, loss: 0.2620, current_lr: 0.000580
step 4330, loss: 0.2938, current_lr: 0.000580
step 4331, loss: 0.2416, current_lr: 0.000580
step 4332, loss: 0.2207, current_lr: 0.000580
step 4333, loss: 0.2292, current_lr: 0.000580
step 4334, loss: 0.2578, current_lr: 0.000580
step 4335, loss: 0.2754, current_lr: 0.000580
step 4336, loss: 0.2501, current_lr: 0.000581
step 4337, loss: 0.3559, current_lr: 0.000581
step 4338, loss: 0.2568, current_lr: 0.000581
step 4339, loss: 0.2672, current_lr: 0.000581
step 4340, loss: 0.2743, current_lr: 0.000581
step 4341, loss: 0.2962, current_lr: 0.000581
step 4342, loss: 0.2743, current_lr: 0.000581
step 4343, loss: 0.2790, current_lr: 0.000582
step 4344, loss: 0.2678, current_lr: 0.000582
step 4345, loss: 0.2566, current_lr: 0.000582
step 4346, loss: 0.2828, current_lr: 0.000582
step 4347, loss: 0.2701, current_lr: 0.000582
step 4348, loss: 0.2715, current_lr: 0.000582
step 4349, loss: 0.2372, current_lr: 0.000582
step 4350, loss: 0.2949, current_lr: 0.000582
step 4351, loss: 0.3008, current_lr: 0.000583
step 4352, loss: 0.3073, current_lr: 0.000583
step 4353, loss: 0.2652, current_lr: 0.000583
step 4354, loss: 0.2442, current_lr: 0.000583
step 4355, loss: 0.3025, current_lr: 0.000583
step 4356, loss: 0.3110, current_lr: 0.000583
step 4357, loss: 0.2678, current_lr: 0.000583
step 4358, loss: 0.3159, current_lr: 0.000584
step 4359, loss: 0.2670, current_lr: 0.000584
step 4360, loss: 0.2811, current_lr: 0.000584
step 4361, loss: 0.2628, current_lr: 0.000584
step 4362, loss: 0.2634, current_lr: 0.000584
step 4363, loss: 0.3398, current_lr: 0.000584
step 4364, loss: 0.3107, current_lr: 0.000584
step 4365, loss: 0.2454, current_lr: 0.000584
step 4366, loss: 0.2684, current_lr: 0.000585
step 4367, loss: 0.2468, current_lr: 0.000585
step 4368, loss: 0.2703, current_lr: 0.000585
step 4369, loss: 0.2779, current_lr: 0.000585
step 4370, loss: 0.2457, current_lr: 0.000585
step 4371, loss: 0.3020, current_lr: 0.000585
step 4372, loss: 0.2820, current_lr: 0.000585
step 4373, loss: 0.3170, current_lr: 0.000586
step 4374, loss: 0.2757, current_lr: 0.000586
step 4375, loss: 0.2653, current_lr: 0.000586
step 4376, loss: 0.2539, current_lr: 0.000586
step 4377, loss: 0.2711, current_lr: 0.000586
step 4378, loss: 0.2814, current_lr: 0.000586
step 4379, loss: 0.2942, current_lr: 0.000586
step 4380, loss: 0.2289, current_lr: 0.000586
step 4381, loss: 0.2891, current_lr: 0.000587
step 4382, loss: 0.2837, current_lr: 0.000587
step 4383, loss: 0.2431, current_lr: 0.000587
step 4384, loss: 0.2615, current_lr: 0.000587
step 4385, loss: 0.2627, current_lr: 0.000587
step 4386, loss: 0.2438, current_lr: 0.000587
step 4387, loss: 0.2381, current_lr: 0.000587
step 4388, loss: 0.2895, current_lr: 0.000587
step 4389, loss: 0.2516, current_lr: 0.000588
step 4390, loss: 0.2352, current_lr: 0.000588
step 4391, loss: 0.2603, current_lr: 0.000588
step 4392, loss: 0.2664, current_lr: 0.000588
step 4393, loss: 0.3013, current_lr: 0.000588
step 4394, loss: 0.3052, current_lr: 0.000588
step 4395, loss: 0.3480, current_lr: 0.000588
step 4396, loss: 0.2725, current_lr: 0.000589
step 4397, loss: 0.2607, current_lr: 0.000589
step 4398, loss: 0.2918, current_lr: 0.000589
step 4399, loss: 0.2783, current_lr: 0.000589
step 4400, loss: 0.2532, current_lr: 0.000589
step 4401, loss: 0.2916, current_lr: 0.000589
step 4402, loss: 0.2856, current_lr: 0.000589
step 4403, loss: 0.2609, current_lr: 0.000589
step 4404, loss: 0.2696, current_lr: 0.000590
step 4405, loss: 0.2331, current_lr: 0.000590
step 4406, loss: 0.2967, current_lr: 0.000590
step 4407, loss: 0.2518, current_lr: 0.000590
step 4408, loss: 0.2732, current_lr: 0.000590
step 4409, loss: 0.2469, current_lr: 0.000590
step 4410, loss: 0.2560, current_lr: 0.000590
step 4411, loss: 0.2906, current_lr: 0.000590
step 4412, loss: 0.2800, current_lr: 0.000591
step 4413, loss: 0.2492, current_lr: 0.000591
step 4414, loss: 0.2551, current_lr: 0.000591
step 4415, loss: 0.2582, current_lr: 0.000591
step 4416, loss: 0.2181, current_lr: 0.000591
step 4417, loss: 0.2129, current_lr: 0.000591
step 4418, loss: 0.2829, current_lr: 0.000591
step 4419, loss: 0.2584, current_lr: 0.000592
step 4420, loss: 0.2456, current_lr: 0.000592
step 4421, loss: 0.2647, current_lr: 0.000592
step 4422, loss: 0.2736, current_lr: 0.000592
step 4423, loss: 0.2796, current_lr: 0.000592
step 4424, loss: 0.2714, current_lr: 0.000592
step 4425, loss: 0.2737, current_lr: 0.000592
step 4426, loss: 0.2120, current_lr: 0.000592
step 4427, loss: 0.2466, current_lr: 0.000593
step 4428, loss: 0.2352, current_lr: 0.000593
step 4429, loss: 0.2092, current_lr: 0.000593
step 4430, loss: 0.2560, current_lr: 0.000593
step 4431, loss: 0.2648, current_lr: 0.000593
step 4432, loss: 0.2512, current_lr: 0.000593
step 4433, loss: 0.2534, current_lr: 0.000593
step 4434, loss: 0.2746, current_lr: 0.000593
step 4435, loss: 0.2428, current_lr: 0.000594
step 4436, loss: 0.2403, current_lr: 0.000594
step 4437, loss: 0.2344, current_lr: 0.000594
step 4438, loss: 0.2753, current_lr: 0.000594
step 4439, loss: 0.2330, current_lr: 0.000594
step 4440, loss: 0.2825, current_lr: 0.000594
step 4441, loss: 0.2376, current_lr: 0.000594
step 4442, loss: 0.2330, current_lr: 0.000594
step 4443, loss: 0.2422, current_lr: 0.000595
step 4444, loss: 0.2316, current_lr: 0.000595
step 4445, loss: 0.2447, current_lr: 0.000595
step 4446, loss: 0.2470, current_lr: 0.000595
step 4447, loss: 0.2139, current_lr: 0.000595
step 4448, loss: 0.2174, current_lr: 0.000595
step 4449, loss: 0.2247, current_lr: 0.000595
step 4450, loss: 0.2277, current_lr: 0.000596
step 4451, loss: 0.2446, current_lr: 0.000596
step 4452, loss: 0.2672, current_lr: 0.000596
step 4453, loss: 0.1999, current_lr: 0.000596
step 4454, loss: 0.2088, current_lr: 0.000596
step 4455, loss: 0.2604, current_lr: 0.000596
step 4456, loss: 0.2225, current_lr: 0.000596
step 4457, loss: 0.2416, current_lr: 0.000596
step 4458, loss: 0.2444, current_lr: 0.000597
step 4459, loss: 0.2363, current_lr: 0.000597
step 4460, loss: 0.2380, current_lr: 0.000597
step 4461, loss: 0.2102, current_lr: 0.000597
step 4462, loss: 0.2409, current_lr: 0.000597
step 4463, loss: 0.2168, current_lr: 0.000597
step 4464, loss: 0.2379, current_lr: 0.000597
step 4465, loss: 0.2329, current_lr: 0.000597
step 4466, loss: 0.2314, current_lr: 0.000598
step 4467, loss: 0.2445, current_lr: 0.000598
step 4468, loss: 0.2112, current_lr: 0.000598
step 4469, loss: 0.2566, current_lr: 0.000598
step 4470, loss: 0.2360, current_lr: 0.000598
step 4471, loss: 0.2172, current_lr: 0.000598
step 4472, loss: 0.1980, current_lr: 0.000598
step 4473, loss: 0.2299, current_lr: 0.000598
step 4474, loss: 0.2356, current_lr: 0.000599
step 4475, loss: 0.2602, current_lr: 0.000599
step 4476, loss: 0.2424, current_lr: 0.000599
step 4477, loss: 0.2452, current_lr: 0.000599
step 4478, loss: 0.2636, current_lr: 0.000599
step 4479, loss: 0.1970, current_lr: 0.000599
step 4480, loss: 0.1956, current_lr: 0.000599
step 4481, loss: 0.2417, current_lr: 0.000599
step 4482, loss: 0.2456, current_lr: 0.000600
step 4483, loss: 0.2478, current_lr: 0.000600
step 4484, loss: 0.1982, current_lr: 0.000600
step 4485, loss: 0.2373, current_lr: 0.000600
step 4486, loss: 0.2390, current_lr: 0.000600
step 4487, loss: 0.2140, current_lr: 0.000600
step 4488, loss: 0.2210, current_lr: 0.000600
step 4489, loss: 0.2386, current_lr: 0.000600
step 4490, loss: 0.1953, current_lr: 0.000601
step 4491, loss: 0.2308, current_lr: 0.000601
step 4492, loss: 0.2417, current_lr: 0.000601
step 4493, loss: 0.2439, current_lr: 0.000601
step 4494, loss: 0.2350, current_lr: 0.000601
step 4495, loss: 0.2157, current_lr: 0.000601
step 4496, loss: 0.2373, current_lr: 0.000601
step 4497, loss: 0.1782, current_lr: 0.000601
step 4498, loss: 0.2033, current_lr: 0.000602
step 4499, loss: 0.2155, current_lr: 0.000602
step 4500, loss: 0.2428, current_lr: 0.000602
step 4501, loss: 0.2230, current_lr: 0.000602
step 4502, loss: 0.2759, current_lr: 0.000602
step 4503, loss: 0.2153, current_lr: 0.000602
step 4504, loss: 0.2127, current_lr: 0.000602
step 4505, loss: 0.2402, current_lr: 0.000602
step 4506, loss: 0.2537, current_lr: 0.000603
step 4507, loss: 0.2799, current_lr: 0.000603
step 4508, loss: 0.2428, current_lr: 0.000603
step 4509, loss: 0.2143, current_lr: 0.000603
step 4510, loss: 0.2354, current_lr: 0.000603
step 4511, loss: 0.2184, current_lr: 0.000603
step 4512, loss: 0.2380, current_lr: 0.000603
step 4513, loss: 0.1914, current_lr: 0.000603
step 4514, loss: 0.2219, current_lr: 0.000604
step 4515, loss: 0.2317, current_lr: 0.000604
step 4516, loss: 0.2290, current_lr: 0.000604
step 4517, loss: 0.2116, current_lr: 0.000604
step 4518, loss: 0.2166, current_lr: 0.000604
step 4519, loss: 0.2142, current_lr: 0.000604
step 4520, loss: 0.2023, current_lr: 0.000604
step 4521, loss: 0.2379, current_lr: 0.000604
step 4522, loss: 0.2449, current_lr: 0.000605
step 4523, loss: 0.2168, current_lr: 0.000605
step 4524, loss: 0.2716, current_lr: 0.000605
step 4525, loss: 0.2339, current_lr: 0.000605
step 4526, loss: 0.2234, current_lr: 0.000605
step 4527, loss: 0.2113, current_lr: 0.000605
step 4528, loss: 0.2424, current_lr: 0.000605
step 4529, loss: 0.2633, current_lr: 0.000605
step 4530, loss: 0.2218, current_lr: 0.000606
step 4531, loss: 0.2367, current_lr: 0.000606
step 4532, loss: 0.2424, current_lr: 0.000606
step 4533, loss: 0.2216, current_lr: 0.000606
step 4534, loss: 0.1947, current_lr: 0.000606
step 4535, loss: 0.1914, current_lr: 0.000606
step 4536, loss: 0.2144, current_lr: 0.000606
step 4537, loss: 0.2028, current_lr: 0.000606
step 4538, loss: 0.2332, current_lr: 0.000606
step 4539, loss: 0.2347, current_lr: 0.000607
step 4540, loss: 0.2395, current_lr: 0.000607
step 4541, loss: 0.2197, current_lr: 0.000607
step 4542, loss: 0.2145, current_lr: 0.000607
step 4543, loss: 0.2436, current_lr: 0.000607
step 4544, loss: 0.2210, current_lr: 0.000607
step 4545, loss: 0.2041, current_lr: 0.000607
step 4546, loss: 0.2392, current_lr: 0.000607
step 4547, loss: 0.2158, current_lr: 0.000608
step 4548, loss: 0.2136, current_lr: 0.000608
step 4549, loss: 0.1921, current_lr: 0.000608
step 4550, loss: 0.2069, current_lr: 0.000608
step 4551, loss: 0.2056, current_lr: 0.000608
step 4552, loss: 0.1938, current_lr: 0.000608
step 4553, loss: 0.1826, current_lr: 0.000608
step 4554, loss: 0.2005, current_lr: 0.000608
step 4555, loss: 0.2076, current_lr: 0.000609
step 4556, loss: 0.2675, current_lr: 0.000609
step 4557, loss: 0.2028, current_lr: 0.000609
step 4558, loss: 0.2163, current_lr: 0.000609
step 4559, loss: 0.2063, current_lr: 0.000609
step 4560, loss: 0.2385, current_lr: 0.000609
step 4561, loss: 0.2229, current_lr: 0.000609
step 4562, loss: 0.2254, current_lr: 0.000609
step 4563, loss: 0.2485, current_lr: 0.000610
step 4564, loss: 0.2016, current_lr: 0.000610
step 4565, loss: 0.2182, current_lr: 0.000610
step 4566, loss: 0.1984, current_lr: 0.000610
step 4567, loss: 0.2103, current_lr: 0.000610
step 4568, loss: 0.2029, current_lr: 0.000610
step 4569, loss: 0.2444, current_lr: 0.000610
step 4570, loss: 0.2073, current_lr: 0.000610
step 4571, loss: 0.2035, current_lr: 0.000610
step 4572, loss: 0.1945, current_lr: 0.000611
step 4573, loss: 0.1984, current_lr: 0.000611
step 4574, loss: 0.2149, current_lr: 0.000611
step 4575, loss: 0.2344, current_lr: 0.000611
step 4576, loss: 0.2020, current_lr: 0.000611
step 4577, loss: 0.2295, current_lr: 0.000611
step 4578, loss: 0.2342, current_lr: 0.000611
step 4579, loss: 0.2036, current_lr: 0.000611
step 4580, loss: 0.2494, current_lr: 0.000612
step 4581, loss: 0.2103, current_lr: 0.000612
step 4582, loss: 0.2068, current_lr: 0.000612
step 4583, loss: 0.1932, current_lr: 0.000612
step 4584, loss: 0.2095, current_lr: 0.000612
step 4585, loss: 0.2031, current_lr: 0.000612
step 4586, loss: 0.2307, current_lr: 0.000612
step 4587, loss: 0.2138, current_lr: 0.000612
step 4588, loss: 0.2357, current_lr: 0.000612
step 4589, loss: 0.2131, current_lr: 0.000613
step 4590, loss: 0.1955, current_lr: 0.000613
step 4591, loss: 0.1761, current_lr: 0.000613
step 4592, loss: 0.1973, current_lr: 0.000613
step 4593, loss: 0.2228, current_lr: 0.000613
step 4594, loss: 0.2197, current_lr: 0.000613
step 4595, loss: 0.2139, current_lr: 0.000613
step 4596, loss: 0.2054, current_lr: 0.000613
step 4597, loss: 0.1994, current_lr: 0.000614
step 4598, loss: 0.1860, current_lr: 0.000614
step 4599, loss: 0.2597, current_lr: 0.000614
step 4600, loss: 0.4205, current_lr: 0.000614
step 4601, loss: 0.2155, current_lr: 0.000614
step 4602, loss: 0.2424, current_lr: 0.000614
step 4603, loss: 0.1823, current_lr: 0.000614
step 4604, loss: 0.1843, current_lr: 0.000614
step 4605, loss: 0.1838, current_lr: 0.000614
step 4606, loss: 0.1991, current_lr: 0.000615
step 4607, loss: 0.2058, current_lr: 0.000615
step 4608, loss: 0.2444, current_lr: 0.000615
step 4609, loss: 0.2085, current_lr: 0.000615
step 4610, loss: 0.2032, current_lr: 0.000615
step 4611, loss: 0.1940, current_lr: 0.000615
step 4612, loss: 0.1845, current_lr: 0.000615
step 4613, loss: 0.1823, current_lr: 0.000615
step 4614, loss: 0.1820, current_lr: 0.000616
step 4615, loss: 0.1955, current_lr: 0.000616
step 4616, loss: 0.2022, current_lr: 0.000616
step 4617, loss: 0.2116, current_lr: 0.000616
step 4618, loss: 0.1773, current_lr: 0.000616
step 4619, loss: 0.1754, current_lr: 0.000616
step 4620, loss: 0.1845, current_lr: 0.000616
step 4621, loss: 0.2009, current_lr: 0.000616
step 4622, loss: 0.1812, current_lr: 0.000616
step 4623, loss: 0.1997, current_lr: 0.000617
step 4624, loss: 0.2316, current_lr: 0.000617
step 4625, loss: 0.2088, current_lr: 0.000617
step 4626, loss: 0.1563, current_lr: 0.000617
step 4627, loss: 0.1667, current_lr: 0.000617
step 4628, loss: 0.1936, current_lr: 0.000617
step 4629, loss: 0.1667, current_lr: 0.000617
step 4630, loss: 0.1658, current_lr: 0.000617
step 4631, loss: 0.1880, current_lr: 0.000618
step 4632, loss: 0.1833, current_lr: 0.000618
step 4633, loss: 0.1714, current_lr: 0.000618
step 4634, loss: 0.1864, current_lr: 0.000618
step 4635, loss: 0.1748, current_lr: 0.000618
step 4636, loss: 0.1919, current_lr: 0.000618
step 4637, loss: 0.1626, current_lr: 0.000618
step 4638, loss: 0.1921, current_lr: 0.000618
step 4639, loss: 0.1955, current_lr: 0.000618
step 4640, loss: 0.2162, current_lr: 0.000619
step 4641, loss: 0.2026, current_lr: 0.000619
step 4642, loss: 0.1920, current_lr: 0.000619
step 4643, loss: 0.2191, current_lr: 0.000619
step 4644, loss: 0.1704, current_lr: 0.000619
step 4645, loss: 0.1991, current_lr: 0.000619
step 4646, loss: 0.1912, current_lr: 0.000619
step 4647, loss: 0.1496, current_lr: 0.000619
step 4648, loss: 0.1563, current_lr: 0.000619
step 4649, loss: 0.1618, current_lr: 0.000620
step 4650, loss: 0.1893, current_lr: 0.000620
step 4651, loss: 0.2234, current_lr: 0.000620
step 4652, loss: 0.2227, current_lr: 0.000620
step 4653, loss: 0.1551, current_lr: 0.000620
step 4654, loss: 0.1449, current_lr: 0.000620
step 4655, loss: 0.1781, current_lr: 0.000620
step 4656, loss: 0.1979, current_lr: 0.000620
step 4657, loss: 0.1930, current_lr: 0.000620
step 4658, loss: 0.1754, current_lr: 0.000621
step 4659, loss: 0.1652, current_lr: 0.000621
step 4660, loss: 0.1776, current_lr: 0.000621
step 4661, loss: 0.1810, current_lr: 0.000621
step 4662, loss: 0.1549, current_lr: 0.000621
step 4663, loss: 0.1535, current_lr: 0.000621
step 4664, loss: 0.1986, current_lr: 0.000621
step 4665, loss: 0.1841, current_lr: 0.000621
step 4666, loss: 0.1816, current_lr: 0.000622
step 4667, loss: 0.1507, current_lr: 0.000622
step 4668, loss: 0.1849, current_lr: 0.000622
step 4669, loss: 0.1660, current_lr: 0.000622
step 4670, loss: 0.1797, current_lr: 0.000622
step 4671, loss: 0.1952, current_lr: 0.000622
step 4672, loss: 0.2125, current_lr: 0.000622
step 4673, loss: 0.1683, current_lr: 0.000622
step 4674, loss: 0.1519, current_lr: 0.000622
step 4675, loss: 0.1794, current_lr: 0.000623
step 4676, loss: 0.1946, current_lr: 0.000623
step 4677, loss: 0.1790, current_lr: 0.000623
step 4678, loss: 0.1789, current_lr: 0.000623
step 4679, loss: 0.1639, current_lr: 0.000623
step 4680, loss: 0.1734, current_lr: 0.000623
step 4681, loss: 0.1837, current_lr: 0.000623
step 4682, loss: 0.1634, current_lr: 0.000623
step 4683, loss: 0.1800, current_lr: 0.000623
step 4684, loss: 0.1794, current_lr: 0.000624
step 4685, loss: 0.1829, current_lr: 0.000624
step 4686, loss: 0.2088, current_lr: 0.000624
step 4687, loss: 0.1866, current_lr: 0.000624
step 4688, loss: 0.1684, current_lr: 0.000624
step 4689, loss: 0.2014, current_lr: 0.000624
step 4690, loss: 0.2100, current_lr: 0.000624
step 4691, loss: 0.1732, current_lr: 0.000624
step 4692, loss: 0.1729, current_lr: 0.000624
step 4693, loss: 0.2178, current_lr: 0.000625
step 4694, loss: 0.1748, current_lr: 0.000625
step 4695, loss: 0.1877, current_lr: 0.000625
step 4696, loss: 0.1973, current_lr: 0.000625
step 4697, loss: 0.1971, current_lr: 0.000625
step 4698, loss: 0.1741, current_lr: 0.000625
step 4699, loss: 0.1749, current_lr: 0.000625
step 4700, loss: 0.1506, current_lr: 0.000625
step 4701, loss: 0.1693, current_lr: 0.000625
step 4702, loss: 0.1553, current_lr: 0.000626
step 4703, loss: 0.1709, current_lr: 0.000626
step 4704, loss: 0.1456, current_lr: 0.000626
step 4705, loss: 0.2027, current_lr: 0.000626
step 4706, loss: 0.1608, current_lr: 0.000626
step 4707, loss: 0.1653, current_lr: 0.000626
step 4708, loss: 0.1866, current_lr: 0.000626
step 4709, loss: 0.1593, current_lr: 0.000626
step 4710, loss: 0.1497, current_lr: 0.000626
step 4711, loss: 0.1541, current_lr: 0.000627
step 4712, loss: 0.1943, current_lr: 0.000627
step 4713, loss: 0.1638, current_lr: 0.000627
step 4714, loss: 0.1330, current_lr: 0.000627
Saved best model with loss: 0.1330 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1330.pt
step 4715, loss: 0.1332, current_lr: 0.000627
step 4716, loss: 0.1824, current_lr: 0.000627
step 4717, loss: 0.1589, current_lr: 0.000627
step 4718, loss: 0.1694, current_lr: 0.000627
step 4719, loss: 0.1474, current_lr: 0.000627
step 4720, loss: 0.1720, current_lr: 0.000628
step 4721, loss: 0.1860, current_lr: 0.000628
step 4722, loss: 0.1642, current_lr: 0.000628
step 4723, loss: 0.1595, current_lr: 0.000628
step 4724, loss: 0.1843, current_lr: 0.000628
step 4725, loss: 0.1875, current_lr: 0.000628
step 4726, loss: 0.1689, current_lr: 0.000628
step 4727, loss: 0.1859, current_lr: 0.000628
step 4728, loss: 0.1629, current_lr: 0.000628
step 4729, loss: 0.1609, current_lr: 0.000628
step 4730, loss: 0.1827, current_lr: 0.000629
step 4731, loss: 0.1734, current_lr: 0.000629
step 4732, loss: 0.1733, current_lr: 0.000629
step 4733, loss: 0.1727, current_lr: 0.000629
step 4734, loss: 0.1603, current_lr: 0.000629
step 4735, loss: 0.1736, current_lr: 0.000629
step 4736, loss: 0.1775, current_lr: 0.000629
step 4737, loss: 0.1637, current_lr: 0.000629
step 4738, loss: 0.1335, current_lr: 0.000629
step 4739, loss: 0.2150, current_lr: 0.000630
step 4740, loss: 0.1703, current_lr: 0.000630
step 4741, loss: 0.1461, current_lr: 0.000630
step 4742, loss: 0.2038, current_lr: 0.000630
step 4743, loss: 0.1719, current_lr: 0.000630
step 4744, loss: 0.2225, current_lr: 0.000630
step 4745, loss: 0.1928, current_lr: 0.000630
step 4746, loss: 0.1780, current_lr: 0.000630
step 4747, loss: 0.1868, current_lr: 0.000630
step 4748, loss: 0.1523, current_lr: 0.000631
step 4749, loss: 0.1646, current_lr: 0.000631
step 4750, loss: 0.1510, current_lr: 0.000631
step 4751, loss: 0.1983, current_lr: 0.000631
step 4752, loss: 0.1612, current_lr: 0.000631
step 4753, loss: 0.1967, current_lr: 0.000631
step 4754, loss: 0.1567, current_lr: 0.000631
step 4755, loss: 0.1486, current_lr: 0.000631
step 4756, loss: 0.1384, current_lr: 0.000631
step 4757, loss: 0.1560, current_lr: 0.000632
step 4758, loss: 0.1972, current_lr: 0.000632
step 4759, loss: 0.1659, current_lr: 0.000632
step 4760, loss: 0.1493, current_lr: 0.000632
step 4761, loss: 0.1440, current_lr: 0.000632
step 4762, loss: 0.1633, current_lr: 0.000632
step 4763, loss: 0.1883, current_lr: 0.000632
step 4764, loss: 0.2034, current_lr: 0.000632
step 4765, loss: 0.1881, current_lr: 0.000632
step 4766, loss: 0.1774, current_lr: 0.000632
step 4767, loss: 0.1735, current_lr: 0.000633
step 4768, loss: 0.1284, current_lr: 0.000633
Saved best model with loss: 0.1284 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1284.pt
step 4769, loss: 0.1445, current_lr: 0.000633
step 4770, loss: 0.1745, current_lr: 0.000633
step 4771, loss: 0.1516, current_lr: 0.000633
step 4772, loss: 0.1548, current_lr: 0.000633
step 4773, loss: 0.1680, current_lr: 0.000633
step 4774, loss: 0.1590, current_lr: 0.000633
step 4775, loss: 0.1588, current_lr: 0.000633
step 4776, loss: 0.1465, current_lr: 0.000634
step 4777, loss: 0.1550, current_lr: 0.000634
step 4778, loss: 0.1426, current_lr: 0.000634
step 4779, loss: 0.1567, current_lr: 0.000634
step 4780, loss: 0.1425, current_lr: 0.000634
step 4781, loss: 0.1397, current_lr: 0.000634
step 4782, loss: 0.1474, current_lr: 0.000634
step 4783, loss: 0.1733, current_lr: 0.000634
step 4784, loss: 0.1284, current_lr: 0.000634
step 4785, loss: 0.1487, current_lr: 0.000634
step 4786, loss: 0.1670, current_lr: 0.000635
step 4787, loss: 0.1293, current_lr: 0.000635
step 4788, loss: 0.1647, current_lr: 0.000635
step 4789, loss: 0.1756, current_lr: 0.000635
step 4790, loss: 0.1517, current_lr: 0.000635
step 4791, loss: 0.1329, current_lr: 0.000635
step 4792, loss: 0.1350, current_lr: 0.000635
step 4793, loss: 0.1737, current_lr: 0.000635
step 4794, loss: 0.1555, current_lr: 0.000635
step 4795, loss: 0.1166, current_lr: 0.000635
Saved best model with loss: 0.1166 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1166.pt
step 4796, loss: 0.1556, current_lr: 0.000636
step 4797, loss: 0.1656, current_lr: 0.000636
step 4798, loss: 0.1383, current_lr: 0.000636
step 4799, loss: 0.1515, current_lr: 0.000636
step 4800, loss: 0.1526, current_lr: 0.000636
step 4801, loss: 0.1520, current_lr: 0.000636
step 4802, loss: 0.1701, current_lr: 0.000636
step 4803, loss: 0.1673, current_lr: 0.000636
step 4804, loss: 0.1684, current_lr: 0.000636
step 4805, loss: 0.1891, current_lr: 0.000637
step 4806, loss: 0.1926, current_lr: 0.000637
step 4807, loss: 0.1460, current_lr: 0.000637
step 4808, loss: 0.1731, current_lr: 0.000637
step 4809, loss: 0.1468, current_lr: 0.000637
step 4810, loss: 0.1416, current_lr: 0.000637
step 4811, loss: 0.1891, current_lr: 0.000637
step 4812, loss: 0.1544, current_lr: 0.000637
step 4813, loss: 0.1314, current_lr: 0.000637
step 4814, loss: 0.1599, current_lr: 0.000637
step 4815, loss: 0.1468, current_lr: 0.000638
step 4816, loss: 0.1762, current_lr: 0.000638
step 4817, loss: 0.1575, current_lr: 0.000638
step 4818, loss: 0.1304, current_lr: 0.000638
step 4819, loss: 0.1471, current_lr: 0.000638
step 4820, loss: 0.1530, current_lr: 0.000638
step 4821, loss: 0.1785, current_lr: 0.000638
step 4822, loss: 0.1376, current_lr: 0.000638
step 4823, loss: 0.1630, current_lr: 0.000638
step 4824, loss: 0.1390, current_lr: 0.000638
step 4825, loss: 0.1518, current_lr: 0.000639
step 4826, loss: 0.1574, current_lr: 0.000639
step 4827, loss: 0.2417, current_lr: 0.000639
step 4828, loss: 0.1578, current_lr: 0.000639
step 4829, loss: 0.1578, current_lr: 0.000639
step 4830, loss: 0.2151, current_lr: 0.000639
step 4831, loss: 0.1832, current_lr: 0.000639
step 4832, loss: 0.1577, current_lr: 0.000639
step 4833, loss: 0.1669, current_lr: 0.000639
step 4834, loss: 0.1262, current_lr: 0.000639
step 4835, loss: 0.1510, current_lr: 0.000640
step 4836, loss: 0.1772, current_lr: 0.000640
step 4837, loss: 0.1623, current_lr: 0.000640
step 4838, loss: 0.1591, current_lr: 0.000640
step 4839, loss: 0.1605, current_lr: 0.000640
step 4840, loss: 0.1812, current_lr: 0.000640
step 4841, loss: 0.1461, current_lr: 0.000640
step 4842, loss: 0.1761, current_lr: 0.000640
step 4843, loss: 0.2384, current_lr: 0.000640
step 4844, loss: 0.1517, current_lr: 0.000640
step 4845, loss: 0.1769, current_lr: 0.000641
step 4846, loss: 0.1693, current_lr: 0.000641
step 4847, loss: 0.1497, current_lr: 0.000641
step 4848, loss: 0.1779, current_lr: 0.000641
step 4849, loss: 0.1655, current_lr: 0.000641
step 4850, loss: 0.1507, current_lr: 0.000641
step 4851, loss: 0.1715, current_lr: 0.000641
step 4852, loss: 0.1555, current_lr: 0.000641
step 4853, loss: 0.1409, current_lr: 0.000641
step 4854, loss: 0.1524, current_lr: 0.000641
step 4855, loss: 0.1515, current_lr: 0.000642
step 4856, loss: 0.1609, current_lr: 0.000642
step 4857, loss: 0.1592, current_lr: 0.000642
step 4858, loss: 0.1459, current_lr: 0.000642
step 4859, loss: 0.1407, current_lr: 0.000642
step 4860, loss: 0.1587, current_lr: 0.000642
step 4861, loss: 0.1538, current_lr: 0.000642
step 4862, loss: 0.1468, current_lr: 0.000642
step 4863, loss: 0.1367, current_lr: 0.000642
step 4864, loss: 0.1558, current_lr: 0.000642
step 4865, loss: 0.1410, current_lr: 0.000643
step 4866, loss: 0.1601, current_lr: 0.000643
step 4867, loss: 0.1376, current_lr: 0.000643
step 4868, loss: 0.1575, current_lr: 0.000643
step 4869, loss: 0.1556, current_lr: 0.000643
step 4870, loss: 0.1937, current_lr: 0.000643
step 4871, loss: 0.1439, current_lr: 0.000643
step 4872, loss: 0.1540, current_lr: 0.000643
step 4873, loss: 0.1491, current_lr: 0.000643
step 4874, loss: 0.1359, current_lr: 0.000643
step 4875, loss: 0.1281, current_lr: 0.000644
step 4876, loss: 0.1568, current_lr: 0.000644
step 4877, loss: 0.1254, current_lr: 0.000644
step 4878, loss: 0.1661, current_lr: 0.000644
step 4879, loss: 0.1351, current_lr: 0.000644
step 4880, loss: 0.1170, current_lr: 0.000644
step 4881, loss: 0.1226, current_lr: 0.000644
step 4882, loss: 0.1324, current_lr: 0.000644
step 4883, loss: 0.1739, current_lr: 0.000644
step 4884, loss: 0.1350, current_lr: 0.000644
step 4885, loss: 0.1362, current_lr: 0.000645
step 4886, loss: 0.1450, current_lr: 0.000645
step 4887, loss: 0.1268, current_lr: 0.000645
step 4888, loss: 0.1206, current_lr: 0.000645
step 4889, loss: 0.1517, current_lr: 0.000645
step 4890, loss: 0.1548, current_lr: 0.000645
step 4891, loss: 0.1485, current_lr: 0.000645
step 4892, loss: 0.1422, current_lr: 0.000645
step 4893, loss: 0.1587, current_lr: 0.000645
step 4894, loss: 0.1732, current_lr: 0.000645
step 4895, loss: 0.1738, current_lr: 0.000645
step 4896, loss: 0.1467, current_lr: 0.000646
step 4897, loss: 0.1575, current_lr: 0.000646
step 4898, loss: 0.1338, current_lr: 0.000646
step 4899, loss: 0.1558, current_lr: 0.000646
step 4900, loss: 0.1729, current_lr: 0.000646
step 4901, loss: 0.1442, current_lr: 0.000646
step 4902, loss: 0.1771, current_lr: 0.000646
step 4903, loss: 0.1316, current_lr: 0.000646
step 4904, loss: 0.1792, current_lr: 0.000646
step 4905, loss: 0.1421, current_lr: 0.000646
step 4906, loss: 0.1535, current_lr: 0.000647
step 4907, loss: 0.1487, current_lr: 0.000647
step 4908, loss: 0.1296, current_lr: 0.000647
step 4909, loss: 0.1817, current_lr: 0.000647
step 4910, loss: 0.1567, current_lr: 0.000647
step 4911, loss: 0.1607, current_lr: 0.000647
step 4912, loss: 0.1627, current_lr: 0.000647
step 4913, loss: 0.1460, current_lr: 0.000647
step 4914, loss: 0.1176, current_lr: 0.000647
step 4915, loss: 0.1127, current_lr: 0.000647
Saved best model with loss: 0.1127 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1127.pt
step 4916, loss: 0.1525, current_lr: 0.000647
step 4917, loss: 0.2003, current_lr: 0.000648
step 4918, loss: 0.2105, current_lr: 0.000648
step 4919, loss: 0.1575, current_lr: 0.000648
step 4920, loss: 0.1483, current_lr: 0.000648
step 4921, loss: 0.1516, current_lr: 0.000648
step 4922, loss: 0.1398, current_lr: 0.000648
step 4923, loss: 0.1775, current_lr: 0.000648
step 4924, loss: 0.1512, current_lr: 0.000648
step 4925, loss: 0.1512, current_lr: 0.000648
step 4926, loss: 0.1439, current_lr: 0.000648
step 4927, loss: 0.1649, current_lr: 0.000649
step 4928, loss: 0.1710, current_lr: 0.000649
step 4929, loss: 0.1514, current_lr: 0.000649
step 4930, loss: 0.1494, current_lr: 0.000649
step 4931, loss: 0.1656, current_lr: 0.000649
step 4932, loss: 0.1533, current_lr: 0.000649
step 4933, loss: 0.1353, current_lr: 0.000649
step 4934, loss: 0.1429, current_lr: 0.000649
step 4935, loss: 0.1928, current_lr: 0.000649
step 4936, loss: 0.1593, current_lr: 0.000649
step 4937, loss: 0.1539, current_lr: 0.000649
step 4938, loss: 0.1709, current_lr: 0.000650
step 4939, loss: 0.1496, current_lr: 0.000650
step 4940, loss: 0.1472, current_lr: 0.000650
step 4941, loss: 0.1327, current_lr: 0.000650
step 4942, loss: 0.1267, current_lr: 0.000650
step 4943, loss: 0.1413, current_lr: 0.000650
step 4944, loss: 0.1379, current_lr: 0.000650
step 4945, loss: 0.1417, current_lr: 0.000650
step 4946, loss: 0.1489, current_lr: 0.000650
step 4947, loss: 0.1446, current_lr: 0.000650
step 4948, loss: 0.1524, current_lr: 0.000650
step 4949, loss: 0.1290, current_lr: 0.000651
step 4950, loss: 0.1251, current_lr: 0.000651
step 4951, loss: 0.1459, current_lr: 0.000651
step 4952, loss: 0.1047, current_lr: 0.000651
Saved best model with loss: 0.1047 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1047.pt
step 4953, loss: 0.1373, current_lr: 0.000651
step 4954, loss: 0.1233, current_lr: 0.000651
step 4955, loss: 0.1588, current_lr: 0.000651
step 4956, loss: 0.1315, current_lr: 0.000651
step 4957, loss: 0.1460, current_lr: 0.000651
step 4958, loss: 0.1448, current_lr: 0.000651
step 4959, loss: 0.1366, current_lr: 0.000651
step 4960, loss: 0.1424, current_lr: 0.000652
step 4961, loss: 0.1579, current_lr: 0.000652
step 4962, loss: 0.1580, current_lr: 0.000652
step 4963, loss: 0.1286, current_lr: 0.000652
step 4964, loss: 0.1597, current_lr: 0.000652
step 4965, loss: 0.1578, current_lr: 0.000652
step 4966, loss: 0.1250, current_lr: 0.000652
step 4967, loss: 0.1773, current_lr: 0.000652
step 4968, loss: 0.1634, current_lr: 0.000652
step 4969, loss: 0.1535, current_lr: 0.000652
step 4970, loss: 0.1685, current_lr: 0.000652
step 4971, loss: 0.1539, current_lr: 0.000653
step 4972, loss: 0.1530, current_lr: 0.000653
step 4973, loss: 0.1588, current_lr: 0.000653
step 4974, loss: 0.1564, current_lr: 0.000653
step 4975, loss: 0.1188, current_lr: 0.000653
step 4976, loss: 0.1530, current_lr: 0.000653
step 4977, loss: 0.1468, current_lr: 0.000653
step 4978, loss: 0.1106, current_lr: 0.000653
step 4979, loss: 0.1097, current_lr: 0.000653
step 4980, loss: 0.1461, current_lr: 0.000653
step 4981, loss: 0.1634, current_lr: 0.000653
step 4982, loss: 0.1532, current_lr: 0.000654
step 4983, loss: 0.1357, current_lr: 0.000654
step 4984, loss: 0.1237, current_lr: 0.000654
step 4985, loss: 0.1366, current_lr: 0.000654
step 4986, loss: 0.1696, current_lr: 0.000654
step 4987, loss: 0.1058, current_lr: 0.000654
step 4988, loss: 0.1488, current_lr: 0.000654
step 4989, loss: 0.1897, current_lr: 0.000654
step 4990, loss: 0.1408, current_lr: 0.000654
step 4991, loss: 0.1678, current_lr: 0.000654
step 4992, loss: 0.1402, current_lr: 0.000654
step 4993, loss: 0.1284, current_lr: 0.000655
step 4994, loss: 0.1359, current_lr: 0.000655
step 4995, loss: 0.1624, current_lr: 0.000655
step 4996, loss: 0.1812, current_lr: 0.000655
step 4997, loss: 0.1754, current_lr: 0.000655
step 4998, loss: 0.1479, current_lr: 0.000655
step 4999, loss: 0.1190, current_lr: 0.000655
step 5000, loss: 0.1499, current_lr: 0.000655
step 5001, loss: 0.1580, current_lr: 0.000655
step 5002, loss: 0.1482, current_lr: 0.000655
step 5003, loss: 0.1345, current_lr: 0.000655
step 5004, loss: 0.1666, current_lr: 0.000655
step 5005, loss: 0.1362, current_lr: 0.000656
step 5006, loss: 0.1201, current_lr: 0.000656
step 5007, loss: 0.1314, current_lr: 0.000656
step 5008, loss: 0.1554, current_lr: 0.000656
step 5009, loss: 0.1363, current_lr: 0.000656
step 5010, loss: 0.1673, current_lr: 0.000656
step 5011, loss: 0.1102, current_lr: 0.000656
step 5012, loss: 0.1486, current_lr: 0.000656
step 5013, loss: 0.1528, current_lr: 0.000656
step 5014, loss: 0.1299, current_lr: 0.000656
step 5015, loss: 0.1288, current_lr: 0.000656
step 5016, loss: 0.1757, current_lr: 0.000657
step 5017, loss: 0.1188, current_lr: 0.000657
step 5018, loss: 0.1361, current_lr: 0.000657
step 5019, loss: 0.1464, current_lr: 0.000657
step 5020, loss: 0.1156, current_lr: 0.000657
step 5021, loss: 0.1373, current_lr: 0.000657
step 5022, loss: 0.1224, current_lr: 0.000657
step 5023, loss: 0.1341, current_lr: 0.000657
step 5024, loss: 0.1749, current_lr: 0.000657
step 5025, loss: 0.1530, current_lr: 0.000657
step 5026, loss: 0.1356, current_lr: 0.000657
step 5027, loss: 0.1597, current_lr: 0.000657
step 5028, loss: 0.1463, current_lr: 0.000658
step 5029, loss: 0.1366, current_lr: 0.000658
step 5030, loss: 0.1228, current_lr: 0.000658
step 5031, loss: 0.1799, current_lr: 0.000658
step 5032, loss: 0.1359, current_lr: 0.000658
step 5033, loss: 0.1462, current_lr: 0.000658
step 5034, loss: 0.1473, current_lr: 0.000658
step 5035, loss: 0.1438, current_lr: 0.000658
step 5036, loss: 0.1410, current_lr: 0.000658
step 5037, loss: 0.1224, current_lr: 0.000658
step 5038, loss: 0.1381, current_lr: 0.000658
step 5039, loss: 0.1160, current_lr: 0.000659
step 5040, loss: 0.1252, current_lr: 0.000659
step 5041, loss: 0.1350, current_lr: 0.000659
step 5042, loss: 0.1646, current_lr: 0.000659
step 5043, loss: 0.1459, current_lr: 0.000659
step 5044, loss: 0.1338, current_lr: 0.000659
step 5045, loss: 0.1233, current_lr: 0.000659
step 5046, loss: 0.1412, current_lr: 0.000659
step 5047, loss: 0.1121, current_lr: 0.000659
step 5048, loss: 0.1321, current_lr: 0.000659
step 5049, loss: 0.1373, current_lr: 0.000659
step 5050, loss: 0.1259, current_lr: 0.000659
step 5051, loss: 0.1264, current_lr: 0.000660
step 5052, loss: 0.1259, current_lr: 0.000660
step 5053, loss: 0.0941, current_lr: 0.000660
Saved best model with loss: 0.0941 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.0941.pt
Achieved target loss of 0.09999! Stopping training.
Final loss: 0.09410800784826279

==================================================
Starting text generation...
==================================================
GENERATED TEXT 1:
> The quick brown foxes peering
Like lightning that make us wretched subjects!

GLOUCESTER:
A mankind witch, that
--------------------------------------------------
