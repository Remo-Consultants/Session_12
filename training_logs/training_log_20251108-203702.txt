using device: cpu
Loading pretrained GPT-2 model: gpt2
loaded 338025 tokens
1 epoch = 2640 batches
step 0, loss: 5.106705665588379
Saved best model with loss: 5.1067 to fine_tuned_gpt2\gpt2_finetuned_best_loss_5.1067.pt
step 1, loss: 4.932941913604736
Saved best model with loss: 4.9329 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.9329.pt
step 2, loss: 4.096211910247803
Saved best model with loss: 4.0962 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.0962.pt
step 3, loss: 4.488976955413818
step 4, loss: 4.502616882324219
step 5, loss: 4.433855056762695
step 6, loss: 5.461215972900391
step 7, loss: 4.764986991882324
step 8, loss: 4.041317939758301
Saved best model with loss: 4.0413 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.0413.pt
step 9, loss: 4.981418609619141
step 10, loss: 4.647095680236816
step 11, loss: 3.904893159866333
Saved best model with loss: 3.9049 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.9049.pt
step 12, loss: 4.3020548820495605
step 13, loss: 4.172767162322998
step 14, loss: 4.647274017333984
step 15, loss: 4.379924297332764
step 16, loss: 4.562546730041504
step 17, loss: 5.4979448318481445
step 18, loss: 4.164573669433594
step 19, loss: 4.85388708114624
step 20, loss: 4.702328681945801
step 21, loss: 4.743031024932861
step 22, loss: 3.7383012771606445
Saved best model with loss: 3.7383 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.7383.pt
step 23, loss: 4.258203506469727
step 24, loss: 3.8360934257507324
step 25, loss: 3.847895383834839
step 26, loss: 3.610481023788452
Saved best model with loss: 3.6105 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.6105.pt
step 27, loss: 4.520356178283691
step 28, loss: 4.319205284118652
step 29, loss: 4.184718132019043
step 30, loss: 4.543868541717529
step 31, loss: 4.498016834259033
step 32, loss: 4.4897141456604
step 33, loss: 3.884211301803589
step 34, loss: 5.28506326675415
step 35, loss: 4.84634256362915
step 36, loss: 4.2035369873046875
step 37, loss: 4.8974504470825195
step 38, loss: 3.8198113441467285
step 39, loss: 3.7975330352783203
step 40, loss: 4.609060287475586
step 41, loss: 3.0166618824005127
Saved best model with loss: 3.0167 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.0167.pt
step 42, loss: 3.2367117404937744
step 43, loss: 3.5632221698760986
step 44, loss: 3.8441226482391357
step 45, loss: 3.8113574981689453
step 46, loss: 3.3268041610717773
step 47, loss: 3.8089277744293213
step 48, loss: 4.844048976898193
step 49, loss: 4.381552696228027
step 50, loss: 5.120126724243164
step 51, loss: 5.033805847167969
step 52, loss: 3.642892599105835
step 53, loss: 4.651348114013672
step 54, loss: 4.687567710876465
step 55, loss: 4.0303120613098145
step 56, loss: 4.3412556648254395
step 57, loss: 4.526065349578857
step 58, loss: 3.740525960922241
step 59, loss: 4.162097930908203
step 60, loss: 3.9064910411834717
step 61, loss: 3.899695873260498
step 62, loss: 3.7830259799957275
step 63, loss: 3.591663360595703
step 64, loss: 4.527017116546631
step 65, loss: 5.009505271911621
step 66, loss: 4.185308933258057
step 67, loss: 3.6534435749053955
step 68, loss: 4.092602729797363
step 69, loss: 4.113769054412842
step 70, loss: 4.421308994293213
step 71, loss: 4.053579330444336
step 72, loss: 4.007169723510742
step 73, loss: 4.321084976196289
step 74, loss: 5.30488395690918
step 75, loss: 4.278426647186279
step 76, loss: 4.566287517547607
step 77, loss: 3.6446776390075684
step 78, loss: 3.881622552871704
step 79, loss: 3.226231575012207
step 80, loss: 4.403965473175049
step 81, loss: 4.950368404388428
step 82, loss: 4.215677738189697
step 83, loss: 3.1800544261932373
step 84, loss: 3.509648561477661
step 85, loss: 3.5838983058929443
step 86, loss: 3.7165920734405518
step 87, loss: 4.4587578773498535
step 88, loss: 4.6872711181640625
step 89, loss: 5.051091194152832
step 90, loss: 4.677273750305176
step 91, loss: 4.422355651855469
step 92, loss: 4.807075023651123
step 93, loss: 4.088461875915527
step 94, loss: 3.299227237701416
step 95, loss: 3.86476469039917
step 96, loss: 3.520080804824829
step 97, loss: 4.128359317779541
step 98, loss: 3.4274704456329346
step 99, loss: 3.8555800914764404
step 100, loss: 4.425589084625244
step 101, loss: 3.027602434158325
step 102, loss: 4.07732629776001
step 103, loss: 4.469768524169922
step 104, loss: 3.5470151901245117
step 105, loss: 5.016121864318848
step 106, loss: 5.875563621520996
step 107, loss: 4.062520980834961
step 108, loss: 4.0245280265808105
step 109, loss: 3.695667028427124
step 110, loss: 4.744691848754883
step 111, loss: 4.120723247528076
step 112, loss: 3.4777157306671143
step 113, loss: 4.466702461242676
step 114, loss: 4.680866241455078
step 115, loss: 4.5614447593688965
step 116, loss: 3.945183038711548
step 117, loss: 4.187610626220703
step 118, loss: 3.709219217300415
step 119, loss: 3.7413604259490967
step 120, loss: 4.088102340698242
step 121, loss: 4.266202449798584
step 122, loss: 5.123181343078613
step 123, loss: 5.119017601013184
step 124, loss: 5.362809658050537
step 125, loss: 3.488873243331909
step 126, loss: 3.141893148422241
step 127, loss: 3.525806427001953
step 128, loss: 3.5399858951568604
step 129, loss: 3.244546413421631
step 130, loss: 4.193114280700684
step 131, loss: 4.665785312652588
step 132, loss: 4.2849016189575195
step 133, loss: 4.197142124176025
step 134, loss: 3.5873374938964844
step 135, loss: 3.4731972217559814
step 136, loss: 2.8048601150512695
Saved best model with loss: 2.8049 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.8049.pt
step 137, loss: 3.385746479034424
step 138, loss: 3.344870090484619
step 139, loss: 3.1925888061523438
step 140, loss: 4.069642066955566
step 141, loss: 4.275033950805664
step 142, loss: 5.047226428985596
step 143, loss: 3.2601490020751953
step 144, loss: 3.1316235065460205
step 145, loss: 2.799943447113037
Saved best model with loss: 2.7999 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.7999.pt
step 146, loss: 3.526031732559204
step 147, loss: 3.6188724040985107
step 148, loss: 3.740169048309326
step 149, loss: 4.628198623657227
step 150, loss: 4.347969055175781
step 151, loss: 4.191741943359375
step 152, loss: 4.353056907653809
step 153, loss: 4.348725318908691
step 154, loss: 4.501947402954102
step 155, loss: 4.749711513519287
step 156, loss: 4.076618671417236
step 157, loss: 3.5979819297790527
step 158, loss: 3.280874490737915
step 159, loss: 3.514249324798584
step 160, loss: 2.579284429550171
Saved best model with loss: 2.5793 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.5793.pt
step 161, loss: 3.0277271270751953
step 162, loss: 3.8756966590881348
step 163, loss: 2.90122389793396
step 164, loss: 3.432889938354492
step 165, loss: 3.724303960800171
step 166, loss: 4.250707149505615
step 167, loss: 3.5191798210144043
step 168, loss: 3.4358410835266113
step 169, loss: 4.612989902496338
step 170, loss: 4.661141872406006
step 171, loss: 3.432551145553589
step 172, loss: 3.833758592605591
step 173, loss: 4.83622407913208
step 174, loss: 3.942547559738159
step 175, loss: 4.8788533210754395
step 176, loss: 3.9892022609710693
step 177, loss: 3.571556329727173
step 178, loss: 3.157681465148926
step 179, loss: 3.4599876403808594
step 180, loss: 3.28289794921875
step 181, loss: 2.6535747051239014
step 182, loss: 3.818803310394287
step 183, loss: 3.3357396125793457
step 184, loss: 2.9387364387512207
step 185, loss: 3.027184247970581
step 186, loss: 3.3393101692199707
step 187, loss: 4.2488837242126465
step 188, loss: 3.8062212467193604
step 189, loss: 3.629671573638916
step 190, loss: 2.7521309852600098
step 191, loss: 2.509352684020996
Saved best model with loss: 2.5094 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.5094.pt
step 192, loss: 3.9980382919311523
step 193, loss: 3.7783851623535156
step 194, loss: 3.6421236991882324
step 195, loss: 4.310742378234863
step 196, loss: 3.839794874191284
step 197, loss: 2.781621217727661
step 198, loss: 4.821766376495361
step 199, loss: 3.5788493156433105
step 200, loss: 3.2486014366149902
step 201, loss: 3.3283820152282715
step 202, loss: 3.547105550765991
step 203, loss: 3.3613598346710205
step 204, loss: 4.468174934387207
step 205, loss: 3.8686940670013428
step 206, loss: 5.017037391662598
step 207, loss: 4.058818817138672
step 208, loss: 3.004500150680542
step 209, loss: 3.8595428466796875
step 210, loss: 4.6276044845581055
step 211, loss: 4.311402797698975
step 212, loss: 4.206668853759766
step 213, loss: 3.3535590171813965
step 214, loss: 3.7701168060302734
step 215, loss: 3.397894859313965
step 216, loss: 3.749880790710449
step 217, loss: 3.2852706909179688
step 218, loss: 4.099540710449219
step 219, loss: 2.8147473335266113
step 220, loss: 3.9705398082733154
step 221, loss: 2.7513315677642822
step 222, loss: 3.988445997238159
step 223, loss: 3.7012438774108887
step 224, loss: 2.9119246006011963
step 225, loss: 4.085916042327881
step 226, loss: 3.1806259155273438
step 227, loss: 3.509294271469116
step 228, loss: 4.1189470291137695
step 229, loss: 4.176172256469727
step 230, loss: 4.097783088684082
step 231, loss: 3.9431185722351074
step 232, loss: 4.0364460945129395
step 233, loss: 5.304929733276367
step 234, loss: 3.984450578689575
step 235, loss: 3.735208034515381
step 236, loss: 3.4085493087768555
step 237, loss: 2.8603789806365967
step 238, loss: 3.019592761993408
step 239, loss: 3.876112699508667
step 240, loss: 2.5454800128936768
step 241, loss: 3.3402767181396484
step 242, loss: 3.307049512863159
step 243, loss: 3.3739845752716064
step 244, loss: 4.313615322113037
step 245, loss: 4.626351833343506
step 246, loss: 4.252574920654297
step 247, loss: 4.415500640869141
step 248, loss: 3.8057515621185303
step 249, loss: 4.099172592163086
step 250, loss: 3.750082015991211
step 251, loss: 4.917501449584961
step 252, loss: 3.639786720275879
step 253, loss: 3.498170852661133
step 254, loss: 3.1007421016693115
step 255, loss: 3.3063242435455322
step 256, loss: 2.8743700981140137
step 257, loss: 3.681741237640381
step 258, loss: 3.1848063468933105
step 259, loss: 3.7540392875671387
step 260, loss: 4.654567241668701
step 261, loss: 4.931147575378418
step 262, loss: 4.450892448425293
step 263, loss: 4.872677803039551
step 264, loss: 5.0091376304626465
step 265, loss: 4.526147842407227
step 266, loss: 3.739161968231201
step 267, loss: 3.8850815296173096
step 268, loss: 3.6804468631744385
step 269, loss: 3.017101287841797
step 270, loss: 2.8567893505096436
step 271, loss: 3.474177360534668
step 272, loss: 4.317761421203613
step 273, loss: 4.0497894287109375
step 274, loss: 3.8060193061828613
step 275, loss: 4.4831862449646
step 276, loss: 3.9969394207000732
step 277, loss: 4.3963446617126465
step 278, loss: 3.290599822998047
step 279, loss: 2.626439332962036
step 280, loss: 3.127066135406494
step 281, loss: 3.6804370880126953
step 282, loss: 3.720655679702759
step 283, loss: 3.879833698272705
step 284, loss: 3.5076100826263428
step 285, loss: 3.525977373123169
step 286, loss: 3.3021512031555176
step 287, loss: 3.5402650833129883
step 288, loss: 4.072795391082764
step 289, loss: 3.737495183944702
step 290, loss: 3.8621339797973633
step 291, loss: 3.5676441192626953
step 292, loss: 3.973714828491211
step 293, loss: 3.662916421890259
step 294, loss: 3.2198824882507324
step 295, loss: 2.936190128326416
step 296, loss: 3.6617186069488525
step 297, loss: 4.018409252166748
step 298, loss: 4.167963981628418
step 299, loss: 4.311342716217041
step 300, loss: 4.589818954467773
step 301, loss: 4.465315818786621
step 302, loss: 3.439811944961548
step 303, loss: 3.8910999298095703
step 304, loss: 3.767385721206665
step 305, loss: 3.937666893005371
step 306, loss: 3.0079097747802734
step 307, loss: 4.502983093261719
step 308, loss: 2.9773576259613037
step 309, loss: 4.370871543884277
step 310, loss: 3.1050357818603516
step 311, loss: 3.439723253250122
step 312, loss: 4.6638970375061035
step 313, loss: 3.731752872467041
step 314, loss: 4.031481742858887
step 315, loss: 4.08387565612793
step 316, loss: 3.520646333694458
step 317, loss: 4.76973295211792
step 318, loss: 3.9224867820739746
step 319, loss: 4.6481804847717285
step 320, loss: 2.91058349609375
step 321, loss: 4.120361804962158
step 322, loss: 3.721418619155884
step 323, loss: 4.987096786499023
step 324, loss: 5.253626346588135
step 325, loss: 4.203490257263184
step 326, loss: 4.682925701141357
step 327, loss: 4.447874069213867
step 328, loss: 3.996356248855591
step 329, loss: 3.6881372928619385
step 330, loss: 3.7828590869903564
step 331, loss: 3.6709063053131104
step 332, loss: 5.114378929138184
step 333, loss: 4.76134729385376
step 334, loss: 4.069720268249512
step 335, loss: 3.9389686584472656
step 336, loss: 4.436750411987305
step 337, loss: 4.58310604095459
step 338, loss: 4.374699115753174
step 339, loss: 4.334414005279541
step 340, loss: 3.632459878921509
step 341, loss: 3.549287796020508
step 342, loss: 3.5261270999908447
step 343, loss: 3.5885238647460938
step 344, loss: 3.6078033447265625
step 345, loss: 4.715045928955078
step 346, loss: 3.869692087173462
step 347, loss: 3.592564821243286
step 348, loss: 4.5709662437438965
step 349, loss: 3.7438952922821045
step 350, loss: 3.651099443435669
step 351, loss: 4.070511341094971
step 352, loss: 3.550736665725708
step 353, loss: 4.1856842041015625
step 354, loss: 4.361396789550781
step 355, loss: 3.9849231243133545
step 356, loss: 4.1613006591796875
step 357, loss: 3.5017457008361816
step 358, loss: 3.7916131019592285
step 359, loss: 4.107242584228516
step 360, loss: 3.2814974784851074
step 361, loss: 4.28082275390625
step 362, loss: 4.056552886962891
step 363, loss: 3.6892542839050293
step 364, loss: 4.2708611488342285
step 365, loss: 2.7676236629486084
step 366, loss: 3.8282177448272705
step 367, loss: 4.045273303985596
step 368, loss: 4.906007766723633
step 369, loss: 5.6511640548706055
step 370, loss: 4.939241886138916
step 371, loss: 5.33341121673584
step 372, loss: 4.414546489715576
step 373, loss: 4.726495265960693
step 374, loss: 4.381545543670654
step 375, loss: 4.748166084289551
step 376, loss: 4.934148788452148
step 377, loss: 4.260481834411621
step 378, loss: 3.7011466026306152
step 379, loss: 3.9345715045928955
step 380, loss: 3.8699212074279785
step 381, loss: 3.704514265060425
step 382, loss: 3.730137348175049
step 383, loss: 4.534610748291016
step 384, loss: 4.73938512802124
step 385, loss: 4.971164226531982
step 386, loss: 5.0023064613342285
step 387, loss: 4.127720832824707
step 388, loss: 3.6398911476135254
step 389, loss: 4.1964430809021
step 390, loss: 4.962125301361084
step 391, loss: 4.117194175720215
step 392, loss: 3.6983766555786133
step 393, loss: 3.5273051261901855
step 394, loss: 3.0169520378112793
step 395, loss: 3.5807971954345703
step 396, loss: 2.739307165145874
step 397, loss: 2.850781202316284
step 398, loss: 3.884840726852417
step 399, loss: 3.1287825107574463
step 400, loss: 2.714984893798828
step 401, loss: 2.2238094806671143
Saved best model with loss: 2.2238 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.2238.pt
step 402, loss: 3.491319179534912
step 403, loss: 5.0357136726379395
step 404, loss: 4.845607757568359
step 405, loss: 3.7347450256347656
step 406, loss: 2.844393491744995
step 407, loss: 2.0911366939544678
Saved best model with loss: 2.0911 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.0911.pt
step 408, loss: 3.292614221572876
step 409, loss: 4.4984211921691895
step 410, loss: 3.3012125492095947
step 411, loss: 4.265262126922607
step 412, loss: 4.97517204284668
step 413, loss: 4.540679454803467
step 414, loss: 4.182224750518799
step 415, loss: 4.065505027770996
step 416, loss: 4.205849647521973
step 417, loss: 4.1764960289001465
step 418, loss: 3.7097480297088623
step 419, loss: 4.430809497833252
step 420, loss: 4.446236610412598
step 421, loss: 4.126493453979492
step 422, loss: 4.281955718994141
step 423, loss: 3.818333387374878
step 424, loss: 4.0114898681640625
step 425, loss: 4.230896949768066
step 426, loss: 4.162247180938721
step 427, loss: 3.5909485816955566
step 428, loss: 4.040852069854736
step 429, loss: 3.7586429119110107
step 430, loss: 3.4807193279266357
step 431, loss: 4.204544544219971
step 432, loss: 3.595024585723877
step 433, loss: 4.623478412628174
step 434, loss: 3.473196506500244
step 435, loss: 4.862525463104248
step 436, loss: 4.378417015075684
step 437, loss: 3.9512312412261963
step 438, loss: 4.977168083190918
step 439, loss: 2.9510488510131836
step 440, loss: 3.1255741119384766
step 441, loss: 4.197854518890381
step 442, loss: 3.5867457389831543
step 443, loss: 3.4491147994995117
step 444, loss: 4.302112579345703
step 445, loss: 4.040221691131592
step 446, loss: 3.9855480194091797
step 447, loss: 3.283785820007324
step 448, loss: 3.1612792015075684
step 449, loss: 3.8110547065734863
step 450, loss: 3.72841739654541
step 451, loss: 4.5891432762146
step 452, loss: 3.8182971477508545
step 453, loss: 3.6215150356292725
step 454, loss: 4.092803955078125
step 455, loss: 4.85612678527832
step 456, loss: 4.667201519012451
step 457, loss: 3.870645761489868
step 458, loss: 5.0944085121154785
step 459, loss: 4.353532791137695
step 460, loss: 3.7487645149230957
step 461, loss: 4.463057518005371
step 462, loss: 3.180408477783203
step 463, loss: 3.7073733806610107
step 464, loss: 3.809396743774414
step 465, loss: 3.4267191886901855
step 466, loss: 3.1602256298065186
step 467, loss: 4.82466459274292
step 468, loss: 3.8931567668914795
step 469, loss: 3.667632818222046
step 470, loss: 2.7182669639587402
step 471, loss: 3.284097671508789
step 472, loss: 4.354432106018066
step 473, loss: 4.054354190826416
step 474, loss: 3.577991485595703
step 475, loss: 4.279624938964844
step 476, loss: 3.265139102935791
step 477, loss: 3.3999459743499756
step 478, loss: 3.5568959712982178
step 479, loss: 3.363164186477661
step 480, loss: 3.47202730178833
step 481, loss: 3.7000341415405273
step 482, loss: 3.4730281829833984
step 483, loss: 4.679019451141357
step 484, loss: 4.138748645782471
step 485, loss: 3.8747828006744385
step 486, loss: 4.28925895690918
step 487, loss: 4.220834255218506
step 488, loss: 3.9329521656036377
step 489, loss: 4.199253559112549
step 490, loss: 4.134780406951904
step 491, loss: 3.104179620742798
step 492, loss: 4.399191856384277
step 493, loss: 3.403259515762329
step 494, loss: 4.208755970001221
step 495, loss: 4.664793491363525
step 496, loss: 3.9878454208374023
step 497, loss: 3.754990577697754
step 498, loss: 4.010169982910156
step 499, loss: 4.1523284912109375
step 500, loss: 4.28994607925415
step 501, loss: 3.133570671081543
step 502, loss: 4.019163608551025
step 503, loss: 4.2672834396362305
step 504, loss: 4.366705894470215
step 505, loss: 2.753650188446045
step 506, loss: 3.950260877609253
step 507, loss: 4.492121696472168
step 508, loss: 4.113781452178955
step 509, loss: 3.8829243183135986
step 510, loss: 4.446620464324951
step 511, loss: 4.15023136138916
step 512, loss: 3.2781646251678467
step 513, loss: 4.1751203536987305
step 514, loss: 3.6307213306427
step 515, loss: 4.245625019073486
step 516, loss: 4.216914653778076
step 517, loss: 3.9222187995910645
step 518, loss: 4.056567192077637
step 519, loss: 3.476773262023926
step 520, loss: 3.856610059738159
step 521, loss: 3.3184056282043457
step 522, loss: 3.646591901779175
step 523, loss: 2.7132155895233154
step 524, loss: 2.8889894485473633
step 525, loss: 3.8756327629089355
step 526, loss: 4.401609420776367
step 527, loss: 2.8489270210266113
step 528, loss: 3.686551570892334
step 529, loss: 4.408660888671875
step 530, loss: 3.262535572052002
step 531, loss: 3.9052200317382812
step 532, loss: 4.030031204223633
step 533, loss: 4.5309977531433105
step 534, loss: 3.3112471103668213
step 535, loss: 3.622084856033325
step 536, loss: 3.477010726928711
step 537, loss: 3.4920341968536377
step 538, loss: 2.795522451400757
step 539, loss: 2.895267963409424
step 540, loss: 2.93875789642334
step 541, loss: 2.9100606441497803
step 542, loss: 3.6493935585021973
step 543, loss: 2.961418867111206
step 544, loss: 2.713832139968872
step 545, loss: 3.9115090370178223
step 546, loss: 3.390613079071045
step 547, loss: 4.223880767822266
step 548, loss: 3.6169469356536865
step 549, loss: 3.011730194091797
step 550, loss: 3.2210147380828857
step 551, loss: 3.9017679691314697
step 552, loss: 4.28018045425415
step 553, loss: 3.585510730743408
step 554, loss: 3.717381238937378
step 555, loss: 3.6675801277160645
step 556, loss: 3.0652761459350586
step 557, loss: 3.8528945446014404
step 558, loss: 3.50830340385437
step 559, loss: 4.0719170570373535
step 560, loss: 3.4644312858581543
step 561, loss: 3.7301836013793945
step 562, loss: 3.2399370670318604
step 563, loss: 2.836890459060669
step 564, loss: 4.473371505737305
step 565, loss: 4.264866352081299
step 566, loss: 3.4067163467407227
step 567, loss: 3.3662118911743164
step 568, loss: 3.176567316055298
step 569, loss: 3.4956648349761963
step 570, loss: 2.7210326194763184
step 571, loss: 3.530759572982788
step 572, loss: 3.928694486618042
step 573, loss: 3.358644485473633
step 574, loss: 3.8919336795806885
step 575, loss: 4.30203914642334
step 576, loss: 4.333463191986084
step 577, loss: 3.8974344730377197
step 578, loss: 3.816490650177002
step 579, loss: 3.8251891136169434
step 580, loss: 2.9080827236175537
step 581, loss: 4.407614231109619
step 582, loss: 4.0915985107421875
step 583, loss: 3.992323637008667
step 584, loss: 4.1116156578063965
step 585, loss: 3.7215046882629395
step 586, loss: 4.998836994171143
step 587, loss: 4.184415340423584
step 588, loss: 3.948599100112915
step 589, loss: 4.629083156585693
step 590, loss: 3.7345290184020996
step 591, loss: 4.334741592407227
step 592, loss: 4.259176731109619
step 593, loss: 4.474061965942383
step 594, loss: 3.9421188831329346
step 595, loss: 3.8812856674194336
step 596, loss: 3.4817748069763184
step 597, loss: 3.3787436485290527
step 598, loss: 4.233025074005127
step 599, loss: 3.61014986038208
step 600, loss: 4.539452075958252
step 601, loss: 3.3461403846740723
step 602, loss: 3.739741086959839
step 603, loss: 4.744901180267334
step 604, loss: 4.296107292175293
step 605, loss: 4.315402984619141
step 606, loss: 4.380451679229736
step 607, loss: 3.904862880706787
step 608, loss: 5.265620708465576
step 609, loss: 4.191117763519287
step 610, loss: 3.6311278343200684
step 611, loss: 3.7073287963867188
step 612, loss: 4.105147838592529
step 613, loss: 3.4885690212249756
step 614, loss: 2.5846121311187744
step 615, loss: 3.5649948120117188
step 616, loss: 3.283432960510254
step 617, loss: 3.2325546741485596
step 618, loss: 3.483463764190674
step 619, loss: 4.021677494049072
step 620, loss: 3.8198390007019043
step 621, loss: 4.1553521156311035
step 622, loss: 4.041908264160156
step 623, loss: 4.258021354675293
step 624, loss: 3.4311907291412354
step 625, loss: 4.113290309906006
step 626, loss: 4.234845161437988
step 627, loss: 3.4258673191070557
step 628, loss: 3.764629602432251
step 629, loss: 4.469753742218018
step 630, loss: 4.171108245849609
step 631, loss: 3.4634830951690674
step 632, loss: 4.406610012054443
step 633, loss: 3.740154266357422
step 634, loss: 3.8057048320770264
step 635, loss: 2.6556637287139893
step 636, loss: 3.3531594276428223
step 637, loss: 3.20513916015625
step 638, loss: 2.7433724403381348
step 639, loss: 2.8954153060913086
step 640, loss: 4.778416156768799
step 641, loss: 5.41848087310791
step 642, loss: 3.4458999633789062
step 643, loss: 3.2815818786621094
step 644, loss: 4.016055583953857
step 645, loss: 4.87970495223999
step 646, loss: 4.0791215896606445
step 647, loss: 4.435027122497559
step 648, loss: 3.145630359649658
step 649, loss: 4.229874610900879
step 650, loss: 3.7494308948516846
step 651, loss: 4.3323259353637695
step 652, loss: 3.7725472450256348
step 653, loss: 4.539317607879639
step 654, loss: 4.03127908706665
step 655, loss: 4.424129009246826
step 656, loss: 4.241631507873535
step 657, loss: 4.184957027435303
step 658, loss: 3.727393865585327
step 659, loss: 3.507469654083252
step 660, loss: 3.6686718463897705
step 661, loss: 3.331425666809082
step 662, loss: 3.7619335651397705
step 663, loss: 2.3343942165374756
step 664, loss: 3.338174343109131
step 665, loss: 4.116832733154297
step 666, loss: 2.675865650177002
step 667, loss: 3.964273452758789
step 668, loss: 3.2095212936401367
step 669, loss: 3.2218868732452393
step 670, loss: 3.346663236618042
step 671, loss: 4.547794818878174
step 672, loss: 3.5576210021972656
step 673, loss: 2.8349132537841797
step 674, loss: 3.3291821479797363
step 675, loss: 2.4406323432922363
step 676, loss: 2.1138954162597656
step 677, loss: 4.311446666717529
step 678, loss: 3.26688289642334
step 679, loss: 3.788295269012451
step 680, loss: 4.312050819396973
step 681, loss: 4.713177680969238
step 682, loss: 5.026395797729492
step 683, loss: 3.7700400352478027
step 684, loss: 2.449939727783203
step 685, loss: 2.514400005340576
step 686, loss: 2.7536120414733887
step 687, loss: 2.86897611618042
step 688, loss: 3.3688127994537354
step 689, loss: 2.7180216312408447
step 690, loss: 3.757908821105957
step 691, loss: 4.4476189613342285
step 692, loss: 4.737434387207031
step 693, loss: 3.1816306114196777
step 694, loss: 2.6728837490081787
step 695, loss: 3.9937939643859863
step 696, loss: 3.8514392375946045
step 697, loss: 3.245821237564087
step 698, loss: 3.2109861373901367
step 699, loss: 3.2400383949279785
step 700, loss: 3.412933349609375
step 701, loss: 3.2735090255737305
step 702, loss: 3.196040391921997
step 703, loss: 4.5808563232421875
step 704, loss: 4.175414562225342
step 705, loss: 4.115406513214111
step 706, loss: 3.832961320877075
step 707, loss: 4.4505486488342285
step 708, loss: 4.911070823669434
step 709, loss: 3.505342960357666
step 710, loss: 4.400944232940674
step 711, loss: 4.609832286834717
step 712, loss: 4.377687454223633
step 713, loss: 4.919832706451416
step 714, loss: 4.461450576782227
step 715, loss: 3.2221713066101074
step 716, loss: 4.234313488006592
step 717, loss: 4.460127830505371
step 718, loss: 3.963320255279541
step 719, loss: 3.2655320167541504
step 720, loss: 3.1622235774993896
step 721, loss: 3.420419931411743
step 722, loss: 3.3606438636779785
step 723, loss: 3.286412477493286
step 724, loss: 4.730571746826172
step 725, loss: 4.5209479331970215
step 726, loss: 4.361079692840576
step 727, loss: 4.5232834815979
step 728, loss: 4.795870304107666
step 729, loss: 4.297868251800537
step 730, loss: 3.667675256729126
step 731, loss: 2.6813106536865234
step 732, loss: 3.886021137237549
step 733, loss: 3.824861764907837
step 734, loss: 4.033288478851318
step 735, loss: 4.393589496612549
step 736, loss: 4.123799800872803
step 737, loss: 3.337150812149048
step 738, loss: 3.2679731845855713
step 739, loss: 3.866957664489746
step 740, loss: 4.115380764007568
step 741, loss: 3.44160532951355
step 742, loss: 3.5236763954162598
step 743, loss: 3.977174758911133
step 744, loss: 3.4473958015441895
step 745, loss: 4.454002380371094
step 746, loss: 4.622211933135986
step 747, loss: 4.583146095275879
step 748, loss: 4.011759281158447
step 749, loss: 4.761345386505127
step 750, loss: 4.238533973693848
step 751, loss: 4.347911357879639
step 752, loss: 4.581821918487549
step 753, loss: 4.590087890625
step 754, loss: 4.633242607116699
step 755, loss: 3.9410228729248047
step 756, loss: 4.354451656341553
step 757, loss: 4.555123329162598
step 758, loss: 3.4802212715148926
step 759, loss: 4.446186542510986
step 760, loss: 4.614465713500977
step 761, loss: 4.380127429962158
step 762, loss: 4.597034931182861
step 763, loss: 3.6351702213287354
step 764, loss: 4.456620693206787
step 765, loss: 4.304673194885254
step 766, loss: 4.267419815063477
step 767, loss: 4.339310646057129
step 768, loss: 4.342396259307861
step 769, loss: 4.723591327667236
step 770, loss: 4.36590051651001
step 771, loss: 4.30037784576416
step 772, loss: 3.805178642272949
step 773, loss: 4.3504133224487305
step 774, loss: 4.788215637207031
step 775, loss: 4.541546821594238
step 776, loss: 3.9976587295532227
step 777, loss: 3.8404018878936768
step 778, loss: 3.747218132019043
step 779, loss: 4.009719371795654
step 780, loss: 3.5348024368286133
step 781, loss: 3.7007975578308105
step 782, loss: 4.639502048492432
step 783, loss: 3.9270644187927246
step 784, loss: 4.471218585968018
step 785, loss: 3.5599753856658936
step 786, loss: 3.214348077774048
step 787, loss: 4.128941535949707
step 788, loss: 4.961302757263184
step 789, loss: 4.50939416885376
step 790, loss: 3.941518783569336
step 791, loss: 4.6112961769104
step 792, loss: 3.822355270385742
step 793, loss: 4.6675496101379395
step 794, loss: 3.1955931186676025
step 795, loss: 3.621765375137329
step 796, loss: 3.8432059288024902
step 797, loss: 4.280978679656982
step 798, loss: 3.869335174560547
step 799, loss: 4.4099578857421875
step 800, loss: 3.213017463684082
step 801, loss: 3.041574001312256
step 802, loss: 3.407406806945801
step 803, loss: 4.3535237312316895
step 804, loss: 4.798339366912842
step 805, loss: 4.446545124053955
step 806, loss: 3.6134698390960693
step 807, loss: 3.435065746307373
step 808, loss: 3.6970601081848145
step 809, loss: 4.5638604164123535
step 810, loss: 4.759339332580566
step 811, loss: 4.386575698852539
step 812, loss: 3.9406466484069824
step 813, loss: 3.7902610301971436
step 814, loss: 5.094252109527588
step 815, loss: 5.005681991577148
step 816, loss: 4.906641483306885
step 817, loss: 4.527762413024902
step 818, loss: 4.849986553192139
step 819, loss: 4.661928176879883
step 820, loss: 4.171956539154053
step 821, loss: 3.5475270748138428
step 822, loss: 3.1658804416656494
step 823, loss: 4.472476959228516
step 824, loss: 4.327812671661377
step 825, loss: 4.517474174499512
step 826, loss: 5.135284900665283
step 827, loss: 3.405377149581909
step 828, loss: 3.6777501106262207
step 829, loss: 4.415040493011475
step 830, loss: 4.076146125793457
step 831, loss: 4.038245677947998
step 832, loss: 3.825502634048462
step 833, loss: 4.699803352355957
step 834, loss: 3.7492563724517822
step 835, loss: 4.259727478027344
step 836, loss: 3.7361934185028076
step 837, loss: 4.172335147857666
step 838, loss: 4.476185321807861
step 839, loss: 3.957545042037964
step 840, loss: 4.029139995574951
step 841, loss: 3.621915578842163
step 842, loss: 5.1929030418396
step 843, loss: 4.395349502563477
step 844, loss: 3.9954488277435303
step 845, loss: 4.897364616394043
step 846, loss: 4.257055282592773
step 847, loss: 4.163295745849609
step 848, loss: 3.9071104526519775
step 849, loss: 3.744959592819214
step 850, loss: 3.7053229808807373
step 851, loss: 3.7859201431274414
step 852, loss: 3.9188597202301025
step 853, loss: 3.6706767082214355
step 854, loss: 3.82979154586792
step 855, loss: 4.278820514678955
step 856, loss: 3.974179983139038
step 857, loss: 3.9421122074127197
step 858, loss: 4.181091785430908
step 859, loss: 3.178274631500244
step 860, loss: 4.596285343170166
step 861, loss: 3.343066692352295
step 862, loss: 3.4132418632507324
step 863, loss: 2.914057731628418
step 864, loss: 4.127789497375488
step 865, loss: 3.441812515258789
step 866, loss: 3.4166347980499268
step 867, loss: 3.3110263347625732
step 868, loss: 3.4195072650909424
step 869, loss: 4.149170398712158
step 870, loss: 4.006034851074219
step 871, loss: 3.1381447315216064
step 872, loss: 4.106234550476074
step 873, loss: 3.9947214126586914
step 874, loss: 3.4095499515533447
step 875, loss: 3.711015224456787
step 876, loss: 3.697997570037842
step 877, loss: 3.7269861698150635
step 878, loss: 4.35459566116333
step 879, loss: 4.581291198730469
step 880, loss: 3.978705883026123
step 881, loss: 4.634490966796875
step 882, loss: 4.145846843719482
step 883, loss: 3.2237653732299805
step 884, loss: 3.9253835678100586
step 885, loss: 4.784151077270508
step 886, loss: 4.731626510620117
step 887, loss: 3.853627920150757
step 888, loss: 5.042149066925049
step 889, loss: 4.134183883666992
step 890, loss: 4.342909812927246
step 891, loss: 3.1938202381134033
step 892, loss: 3.7191967964172363
step 893, loss: 4.06013298034668
step 894, loss: 4.293679237365723
step 895, loss: 4.963592529296875
step 896, loss: 4.159843444824219
step 897, loss: 4.554893493652344
step 898, loss: 3.5649490356445312
step 899, loss: 4.261174201965332
step 900, loss: 4.697722434997559
step 901, loss: 5.120924472808838
step 902, loss: 3.8674259185791016
step 903, loss: 3.7137317657470703
step 904, loss: 4.15587854385376
step 905, loss: 3.811856508255005
step 906, loss: 3.514610528945923
step 907, loss: 3.493905782699585
step 908, loss: 3.0592293739318848
step 909, loss: 3.2413699626922607
step 910, loss: 4.711111545562744
step 911, loss: 4.615302562713623
step 912, loss: 4.339561939239502
step 913, loss: 3.85992169380188
step 914, loss: 4.352588176727295
step 915, loss: 4.691862106323242
step 916, loss: 4.056066036224365
step 917, loss: 4.17519998550415
step 918, loss: 3.9146299362182617
step 919, loss: 3.676705837249756
step 920, loss: 3.597198486328125
step 921, loss: 4.586338520050049
step 922, loss: 4.183982849121094
step 923, loss: 3.8975775241851807
step 924, loss: 3.1717982292175293
step 925, loss: 3.0422680377960205
step 926, loss: 3.642242670059204
step 927, loss: 2.987833023071289
step 928, loss: 3.717977523803711
step 929, loss: 3.587183713912964
step 930, loss: 4.789722919464111
step 931, loss: 4.538737773895264
step 932, loss: 4.155767440795898
step 933, loss: 4.6957011222839355
step 934, loss: 4.0028910636901855
step 935, loss: 4.130772113800049
step 936, loss: 4.144378662109375
step 937, loss: 4.108226776123047
step 938, loss: 3.836801052093506
step 939, loss: 3.7941906452178955
step 940, loss: 3.8820362091064453
step 941, loss: 3.536801338195801
step 942, loss: 3.564934253692627
step 943, loss: 3.1626815795898438
step 944, loss: 4.072943687438965
step 945, loss: 3.4221458435058594
step 946, loss: 4.108558654785156
step 947, loss: 3.746137857437134
step 948, loss: 3.869687080383301
step 949, loss: 3.626995801925659
step 950, loss: 4.224812030792236
step 951, loss: 4.081594944000244
step 952, loss: 4.6620025634765625
step 953, loss: 4.61893367767334
step 954, loss: 3.704824686050415
step 955, loss: 3.5665628910064697
step 956, loss: 4.494711399078369
step 957, loss: 3.6817314624786377
step 958, loss: 3.7906110286712646
step 959, loss: 3.0965185165405273
step 960, loss: 4.284238338470459
step 961, loss: 4.308709621429443
step 962, loss: 4.09541130065918
step 963, loss: 3.822425603866577
step 964, loss: 3.4393563270568848
step 965, loss: 4.376813888549805
step 966, loss: 2.7415807247161865
step 967, loss: 4.110584259033203
step 968, loss: 3.941910982131958
step 969, loss: 4.361449718475342
step 970, loss: 2.583887815475464
step 971, loss: 2.1519923210144043
step 972, loss: 3.4196109771728516
step 973, loss: 4.109097957611084
step 974, loss: 4.545749187469482
step 975, loss: 4.548696994781494
step 976, loss: 4.283502578735352
step 977, loss: 4.4767656326293945
step 978, loss: 3.7042276859283447
step 979, loss: 4.405768871307373
step 980, loss: 3.7740676403045654
step 981, loss: 3.9846928119659424
step 982, loss: 3.664881706237793
step 983, loss: 3.992448568344116
step 984, loss: 3.558596134185791
step 985, loss: 4.42715311050415
step 986, loss: 3.882368326187134
step 987, loss: 4.470682144165039
step 988, loss: 3.643437147140503
step 989, loss: 3.599155902862549
step 990, loss: 2.56355619430542
step 991, loss: 2.6116790771484375
step 992, loss: 2.728256940841675
step 993, loss: 2.6155152320861816
step 994, loss: 2.622422456741333
step 995, loss: 3.3149361610412598
step 996, loss: 3.045427083969116
step 997, loss: 3.331073045730591
step 998, loss: 3.509594440460205
step 999, loss: 4.011389255523682
step 1000, loss: 3.4936506748199463
step 1001, loss: 2.80444073677063
step 1002, loss: 2.6063640117645264
step 1003, loss: 2.902122974395752
step 1004, loss: 3.8797287940979004
step 1005, loss: 4.955042839050293
step 1006, loss: 3.056983470916748
step 1007, loss: 4.2292938232421875
step 1008, loss: 2.9743056297302246
step 1009, loss: 3.5626766681671143
step 1010, loss: 3.7155542373657227
step 1011, loss: 3.8060245513916016
step 1012, loss: 3.9364755153656006
step 1013, loss: 2.694895029067993
step 1014, loss: 3.649658203125
step 1015, loss: 3.5136609077453613
step 1016, loss: 3.980959415435791
step 1017, loss: 4.5177226066589355
step 1018, loss: 4.835628986358643
step 1019, loss: 4.49381685256958
step 1020, loss: 4.786460876464844
step 1021, loss: 4.8337321281433105
step 1022, loss: 4.018579483032227
step 1023, loss: 4.754700183868408
step 1024, loss: 3.8673460483551025
step 1025, loss: 3.780590295791626
step 1026, loss: 3.4809937477111816
step 1027, loss: 4.610998153686523
step 1028, loss: 3.7154805660247803
step 1029, loss: 3.920567035675049
step 1030, loss: 3.5235941410064697
step 1031, loss: 4.212216377258301
step 1032, loss: 4.015108585357666
step 1033, loss: 4.46442174911499
step 1034, loss: 4.191082000732422
step 1035, loss: 4.25638484954834
step 1036, loss: 3.749533176422119
step 1037, loss: 3.610342264175415
step 1038, loss: 3.4831180572509766
step 1039, loss: 2.942026138305664
step 1040, loss: 4.074771881103516
step 1041, loss: 4.14263391494751
step 1042, loss: 4.283478260040283
step 1043, loss: 4.90334939956665
step 1044, loss: 4.226531028747559
step 1045, loss: 4.7350335121154785
step 1046, loss: 4.612802982330322
step 1047, loss: 5.095382213592529
step 1048, loss: 4.0042524337768555
step 1049, loss: 4.0811285972595215
step 1050, loss: 3.7775676250457764
step 1051, loss: 3.6464285850524902
step 1052, loss: 4.60725212097168
step 1053, loss: 4.1667609214782715
step 1054, loss: 3.7120511531829834
step 1055, loss: 3.6819522380828857
step 1056, loss: 4.623528003692627
step 1057, loss: 4.077727317810059
step 1058, loss: 4.726665019989014
step 1059, loss: 3.6487388610839844
step 1060, loss: 4.2552103996276855
step 1061, loss: 5.314281463623047
step 1062, loss: 4.997409820556641
step 1063, loss: 4.958485126495361
step 1064, loss: 3.5550012588500977
step 1065, loss: 3.8769989013671875
step 1066, loss: 4.806397438049316
step 1067, loss: 4.02394437789917
step 1068, loss: 4.497560977935791
step 1069, loss: 4.135988712310791
step 1070, loss: 3.4803519248962402
step 1071, loss: 4.008012294769287
step 1072, loss: 4.544239044189453
step 1073, loss: 4.3893351554870605
step 1074, loss: 4.316739082336426
step 1075, loss: 4.310278415679932
step 1076, loss: 3.7557804584503174
step 1077, loss: 3.8721811771392822
step 1078, loss: 3.339702606201172
step 1079, loss: 5.029326438903809
step 1080, loss: 4.036915302276611
step 1081, loss: 3.8237805366516113
step 1082, loss: 4.434831619262695
step 1083, loss: 4.435068130493164
step 1084, loss: 4.374999046325684
step 1085, loss: 3.97331166267395
step 1086, loss: 4.018407821655273
step 1087, loss: 3.144815444946289
step 1088, loss: 5.088811874389648
step 1089, loss: 5.269428253173828
step 1090, loss: 5.328762054443359
step 1091, loss: 4.670266628265381
step 1092, loss: 3.9485583305358887
step 1093, loss: 4.52902364730835
step 1094, loss: 4.8935866355896
step 1095, loss: 4.476219654083252
step 1096, loss: 4.1761627197265625
step 1097, loss: 3.8945040702819824
step 1098, loss: 3.7000057697296143
step 1099, loss: 4.468814849853516
step 1100, loss: 3.4233222007751465
step 1101, loss: 4.176307678222656
step 1102, loss: 3.6415293216705322
step 1103, loss: 4.569817543029785
step 1104, loss: 4.074892520904541
step 1105, loss: 3.377741575241089
step 1106, loss: 3.3252854347229004
step 1107, loss: 3.45082950592041
step 1108, loss: 3.18405818939209
step 1109, loss: 3.4918460845947266
step 1110, loss: 4.936829566955566
step 1111, loss: 3.8139708042144775
step 1112, loss: 4.918306350708008
step 1113, loss: 4.042437553405762
step 1114, loss: 4.09277868270874
step 1115, loss: 3.9780824184417725
step 1116, loss: 4.317687511444092
step 1117, loss: 4.567562103271484
step 1118, loss: 3.56927752494812
step 1119, loss: 3.294013023376465
step 1120, loss: 4.177268028259277
step 1121, loss: 3.338505268096924
step 1122, loss: 3.7330121994018555
step 1123, loss: 3.644892454147339
step 1124, loss: 4.193673133850098
step 1125, loss: 3.882627248764038
step 1126, loss: 4.28557825088501
step 1127, loss: 3.2861695289611816
step 1128, loss: 3.6119914054870605
step 1129, loss: 3.0735387802124023
step 1130, loss: 3.724506139755249
step 1131, loss: 3.130470037460327
step 1132, loss: 4.116394996643066
step 1133, loss: 2.8846325874328613
step 1134, loss: 3.2712907791137695
step 1135, loss: 3.9230780601501465
step 1136, loss: 5.258622169494629
step 1137, loss: 4.803286075592041
step 1138, loss: 5.70693302154541
step 1139, loss: 5.030893802642822
step 1140, loss: 3.44148588180542
step 1141, loss: 4.534884452819824
step 1142, loss: 4.157214641571045
step 1143, loss: 4.576537132263184
step 1144, loss: 3.458003520965576
step 1145, loss: 3.9527339935302734
step 1146, loss: 3.2299835681915283
step 1147, loss: 3.86271333694458
step 1148, loss: 4.625007629394531
step 1149, loss: 5.035430908203125
step 1150, loss: 5.104976654052734
step 1151, loss: 4.489621639251709
step 1152, loss: 3.0322277545928955
step 1153, loss: 3.999976634979248
step 1154, loss: 3.4250223636627197
step 1155, loss: 4.021555423736572
step 1156, loss: 3.318049907684326
step 1157, loss: 3.3682334423065186
step 1158, loss: 3.321070671081543
step 1159, loss: 3.711449146270752
step 1160, loss: 2.985499382019043
step 1161, loss: 3.394554376602173
step 1162, loss: 4.396311283111572
step 1163, loss: 4.256988048553467
step 1164, loss: 4.173208236694336
step 1165, loss: 3.6116580963134766
step 1166, loss: 3.905355930328369
step 1167, loss: 3.4770607948303223
step 1168, loss: 4.574601173400879
step 1169, loss: 3.5637364387512207
step 1170, loss: 4.876235008239746
step 1171, loss: 3.9676570892333984
step 1172, loss: 3.663797378540039
step 1173, loss: 3.3246757984161377
step 1174, loss: 3.883784055709839
step 1175, loss: 3.4570233821868896
step 1176, loss: 3.9387736320495605
step 1177, loss: 3.481478452682495
step 1178, loss: 3.904874324798584
step 1179, loss: 4.740698337554932
step 1180, loss: 3.5851855278015137
step 1181, loss: 4.568048000335693
step 1182, loss: 3.515443801879883
step 1183, loss: 3.53674054145813
step 1184, loss: 4.590105056762695
step 1185, loss: 4.315898418426514
step 1186, loss: 3.181318759918213
step 1187, loss: 3.7376372814178467
step 1188, loss: 3.4144489765167236
step 1189, loss: 3.5987281799316406
step 1190, loss: 3.755932092666626
step 1191, loss: 3.668124198913574
step 1192, loss: 3.5223116874694824
step 1193, loss: 3.766561508178711
step 1194, loss: 4.032942295074463
step 1195, loss: 4.051477432250977
step 1196, loss: 3.565673589706421
step 1197, loss: 3.1329681873321533
step 1198, loss: 3.3191916942596436
step 1199, loss: 3.5565414428710938
step 1200, loss: 5.104219913482666
step 1201, loss: 4.37490701675415
step 1202, loss: 3.4539573192596436
step 1203, loss: 4.3821516036987305
step 1204, loss: 4.913293361663818
step 1205, loss: 4.932708740234375
step 1206, loss: 3.983534336090088
step 1207, loss: 3.71976375579834
step 1208, loss: 3.2975895404815674
step 1209, loss: 4.158834457397461
step 1210, loss: 4.040775299072266
step 1211, loss: 3.108276605606079
step 1212, loss: 4.471938133239746
step 1213, loss: 4.217845439910889
step 1214, loss: 4.028789043426514
step 1215, loss: 3.9457848072052
step 1216, loss: 4.188674449920654
step 1217, loss: 3.7112059593200684
step 1218, loss: 3.7689404487609863
step 1219, loss: 3.4557807445526123
step 1220, loss: 3.1091184616088867
step 1221, loss: 3.6940877437591553
step 1222, loss: 3.8423595428466797
step 1223, loss: 4.828103542327881
step 1224, loss: 4.168003082275391
step 1225, loss: 3.5700747966766357
step 1226, loss: 3.245914936065674
step 1227, loss: 3.961923360824585
step 1228, loss: 3.0361340045928955
step 1229, loss: 3.422572135925293
step 1230, loss: 3.8493590354919434
step 1231, loss: 4.287319183349609
step 1232, loss: 4.242611408233643
step 1233, loss: 4.561079502105713
step 1234, loss: 4.310940265655518
step 1235, loss: 3.9404735565185547
step 1236, loss: 3.4763705730438232
step 1237, loss: 3.9622585773468018
step 1238, loss: 3.685405969619751
step 1239, loss: 3.6205506324768066
step 1240, loss: 4.028257369995117
step 1241, loss: 3.691122055053711
step 1242, loss: 4.21468448638916
step 1243, loss: 3.6647114753723145
step 1244, loss: 3.786109209060669
step 1245, loss: 4.401010036468506
step 1246, loss: 2.852320671081543
step 1247, loss: 3.2631235122680664
step 1248, loss: 3.867872953414917
step 1249, loss: 2.601682186126709
step 1250, loss: 3.2211060523986816
step 1251, loss: 2.7845823764801025
step 1252, loss: 3.667288064956665
step 1253, loss: 3.391117572784424
step 1254, loss: 3.4992427825927734
step 1255, loss: 3.526620864868164
step 1256, loss: 4.212741851806641
step 1257, loss: 3.5616347789764404
step 1258, loss: 3.794996738433838
step 1259, loss: 3.8168022632598877
step 1260, loss: 4.178061485290527
step 1261, loss: 3.2904253005981445
step 1262, loss: 4.1934285163879395
step 1263, loss: 4.195574760437012
step 1264, loss: 3.378180742263794
step 1265, loss: 3.393455982208252
step 1266, loss: 3.879380702972412
step 1267, loss: 3.0489888191223145
step 1268, loss: 3.9150407314300537
step 1269, loss: 3.2319769859313965
step 1270, loss: 3.5514471530914307
step 1271, loss: 2.5009639263153076
step 1272, loss: 3.11491322517395
step 1273, loss: 3.312562942504883
step 1274, loss: 3.4565160274505615
step 1275, loss: 4.1086578369140625
step 1276, loss: 4.302939414978027
step 1277, loss: 4.122986793518066
step 1278, loss: 3.892087459564209
step 1279, loss: 5.1156744956970215
step 1280, loss: 4.346394062042236
step 1281, loss: 3.7141759395599365
step 1282, loss: 3.492633104324341
step 1283, loss: 3.1824495792388916
step 1284, loss: 3.6289069652557373
step 1285, loss: 3.483903646469116
step 1286, loss: 3.0438151359558105
step 1287, loss: 3.8141696453094482
step 1288, loss: 3.32922625541687
step 1289, loss: 3.6116394996643066
step 1290, loss: 4.19435977935791
step 1291, loss: 4.339762210845947
step 1292, loss: 4.398769378662109
step 1293, loss: 3.758466958999634
step 1294, loss: 3.5962979793548584
step 1295, loss: 3.423152446746826
step 1296, loss: 3.792769432067871
step 1297, loss: 3.9955978393554688
step 1298, loss: 3.431572914123535
step 1299, loss: 2.561556816101074
step 1300, loss: 3.1059398651123047
step 1301, loss: 3.4754574298858643
step 1302, loss: 3.5770626068115234
step 1303, loss: 3.83691143989563
step 1304, loss: 3.903707981109619
step 1305, loss: 4.217504501342773
step 1306, loss: 4.506051063537598
step 1307, loss: 3.8603017330169678
step 1308, loss: 4.289374828338623
step 1309, loss: 3.77360463142395
step 1310, loss: 4.788963794708252
step 1311, loss: 4.188967704772949
step 1312, loss: 4.141122817993164
step 1313, loss: 4.5002593994140625
step 1314, loss: 4.250863075256348
step 1315, loss: 3.3759546279907227
step 1316, loss: 4.800105094909668
step 1317, loss: 4.946651458740234
step 1318, loss: 4.341001510620117
step 1319, loss: 4.126399517059326
step 1320, loss: 3.8142337799072266
step 1321, loss: 3.9000604152679443
step 1322, loss: 3.9672749042510986
step 1323, loss: 3.846271276473999
step 1324, loss: 4.0711469650268555
step 1325, loss: 4.475818157196045
step 1326, loss: 4.239073276519775
step 1327, loss: 4.553969860076904
step 1328, loss: 3.647568464279175
step 1329, loss: 4.2461838722229
step 1330, loss: 3.8419106006622314
step 1331, loss: 3.7066707611083984
step 1332, loss: 4.1998114585876465
step 1333, loss: 3.9882540702819824
step 1334, loss: 4.532602310180664
step 1335, loss: 4.7079877853393555
step 1336, loss: 3.625922203063965
step 1337, loss: 2.263554334640503
step 1338, loss: 3.4636621475219727
step 1339, loss: 3.7355170249938965
step 1340, loss: 3.76825213432312
step 1341, loss: 3.825629711151123
step 1342, loss: 4.103952884674072
step 1343, loss: 3.477360486984253
step 1344, loss: 3.5850789546966553
step 1345, loss: 3.5363171100616455
step 1346, loss: 3.66536283493042
step 1347, loss: 4.068406581878662
step 1348, loss: 3.4657182693481445
step 1349, loss: 4.445282936096191
step 1350, loss: 4.282613277435303
step 1351, loss: 3.963073968887329
step 1352, loss: 3.3305017948150635
step 1353, loss: 3.464761972427368
step 1354, loss: 3.7104620933532715
step 1355, loss: 4.201223373413086
step 1356, loss: 4.47006368637085
step 1357, loss: 3.8124146461486816
step 1358, loss: 4.183014869689941
step 1359, loss: 4.137328624725342
step 1360, loss: 3.654583692550659
step 1361, loss: 4.753940582275391
step 1362, loss: 3.9378256797790527
step 1363, loss: 3.7239603996276855
step 1364, loss: 3.7416744232177734
step 1365, loss: 3.170118808746338
step 1366, loss: 3.7303974628448486
step 1367, loss: 3.540846109390259
step 1368, loss: 3.5823934078216553
step 1369, loss: 3.3500237464904785
step 1370, loss: 4.248578071594238
step 1371, loss: 2.790616035461426
step 1372, loss: 2.9538066387176514
step 1373, loss: 3.024819850921631
step 1374, loss: 3.596604108810425
step 1375, loss: 3.2262375354766846
step 1376, loss: 2.404707431793213
step 1377, loss: 3.3513870239257812
step 1378, loss: 3.53804087638855
step 1379, loss: 2.6696982383728027
step 1380, loss: 2.3939647674560547
step 1381, loss: 4.112884998321533
step 1382, loss: 2.934300422668457
step 1383, loss: 4.743750095367432
step 1384, loss: 4.123858451843262
step 1385, loss: 2.429187536239624
step 1386, loss: 3.562993288040161
step 1387, loss: 3.213088274002075
step 1388, loss: 3.1796369552612305
step 1389, loss: 3.124149799346924
step 1390, loss: 4.306750774383545
step 1391, loss: 4.187305927276611
step 1392, loss: 3.662358522415161
step 1393, loss: 4.132903099060059
step 1394, loss: 3.420192003250122
step 1395, loss: 3.752500057220459
step 1396, loss: 3.690399408340454
step 1397, loss: 3.524047374725342
step 1398, loss: 3.9284701347351074
step 1399, loss: 3.2789812088012695
step 1400, loss: 4.150233268737793
step 1401, loss: 4.5303778648376465
step 1402, loss: 4.682703495025635
step 1403, loss: 3.8398563861846924
step 1404, loss: 4.3649678230285645
step 1405, loss: 3.8159759044647217
step 1406, loss: 4.051800727844238
step 1407, loss: 3.940638780593872
step 1408, loss: 4.525813579559326
step 1409, loss: 3.9011237621307373
step 1410, loss: 3.8956658840179443
step 1411, loss: 3.730794906616211
step 1412, loss: 4.064240455627441
step 1413, loss: 4.854737758636475
step 1414, loss: 4.1429362297058105
step 1415, loss: 4.373793125152588
step 1416, loss: 4.096138000488281
step 1417, loss: 4.043277263641357
step 1418, loss: 3.069377899169922
step 1419, loss: 3.7444798946380615
step 1420, loss: 3.9009437561035156
step 1421, loss: 4.512960910797119
step 1422, loss: 4.104302406311035
step 1423, loss: 4.25421667098999
step 1424, loss: 3.6372275352478027
step 1425, loss: 4.349641799926758
step 1426, loss: 3.895432949066162
step 1427, loss: 4.5635223388671875
step 1428, loss: 3.6258041858673096
step 1429, loss: 3.9546382427215576
step 1430, loss: 4.555391311645508
step 1431, loss: 4.731454372406006
step 1432, loss: 4.453527450561523
step 1433, loss: 3.7122058868408203
step 1434, loss: 3.93072509765625
step 1435, loss: 4.0816969871521
step 1436, loss: 4.414503574371338
step 1437, loss: 3.9908719062805176
step 1438, loss: 4.173353672027588
step 1439, loss: 3.1806893348693848
step 1440, loss: 3.3560996055603027
step 1441, loss: 4.569234848022461
step 1442, loss: 4.410804271697998
step 1443, loss: 4.481398582458496
step 1444, loss: 4.177392959594727
step 1445, loss: 3.755854845046997
step 1446, loss: 3.743600606918335
step 1447, loss: 2.7885539531707764
step 1448, loss: 3.696617841720581
step 1449, loss: 3.252394437789917
step 1450, loss: 2.68611741065979
step 1451, loss: 3.1553313732147217
step 1452, loss: 2.72784423828125
step 1453, loss: 3.5263843536376953
step 1454, loss: 3.060922622680664
step 1455, loss: 4.572093486785889
step 1456, loss: 4.137082099914551
step 1457, loss: 4.492670059204102
step 1458, loss: 3.8261141777038574
step 1459, loss: 3.796748638153076
step 1460, loss: 3.9311914443969727
step 1461, loss: 4.191685676574707
step 1462, loss: 3.994931221008301
step 1463, loss: 3.708280563354492
step 1464, loss: 3.9625437259674072
step 1465, loss: 3.430478096008301
step 1466, loss: 4.262486934661865
step 1467, loss: 4.361758232116699
step 1468, loss: 4.231471061706543
step 1469, loss: 4.37193489074707
step 1470, loss: 4.8612470626831055
step 1471, loss: 4.42745304107666
step 1472, loss: 3.6199114322662354
step 1473, loss: 4.012077808380127
step 1474, loss: 3.792165994644165
step 1475, loss: 4.320244789123535
step 1476, loss: 3.630966901779175
step 1477, loss: 3.9960904121398926
step 1478, loss: 3.7988226413726807
step 1479, loss: 3.4732747077941895
step 1480, loss: 4.7363152503967285
step 1481, loss: 4.278520584106445
step 1482, loss: 3.8597989082336426
step 1483, loss: 4.217769145965576
step 1484, loss: 4.276795864105225
step 1485, loss: 3.826373815536499
step 1486, loss: 3.688070774078369
step 1487, loss: 3.3876471519470215
step 1488, loss: 4.160707473754883
step 1489, loss: 4.176549911499023
step 1490, loss: 3.334699869155884
step 1491, loss: 4.23457145690918
step 1492, loss: 3.914851188659668
step 1493, loss: 4.218144416809082
step 1494, loss: 3.8778860569000244
step 1495, loss: 4.18239688873291
step 1496, loss: 4.125723838806152
step 1497, loss: 2.6914000511169434
step 1498, loss: 3.7018120288848877
step 1499, loss: 2.9248549938201904
step 1500, loss: 3.8606925010681152
step 1501, loss: 2.9506208896636963
step 1502, loss: 3.52215838432312
step 1503, loss: 3.4141056537628174
step 1504, loss: 3.057847023010254
step 1505, loss: 2.929293632507324
step 1506, loss: 2.7952373027801514
step 1507, loss: 2.2885565757751465
step 1508, loss: 2.807847738265991
step 1509, loss: 2.867975950241089
step 1510, loss: 2.933755397796631
step 1511, loss: 2.4969048500061035
step 1512, loss: 2.668671131134033
step 1513, loss: 2.9385452270507812
step 1514, loss: 2.5398993492126465
step 1515, loss: 3.4873647689819336
step 1516, loss: 4.452308177947998
step 1517, loss: 4.070843696594238
step 1518, loss: 4.3676652908325195
step 1519, loss: 4.299393177032471
step 1520, loss: 4.308687686920166
step 1521, loss: 4.212225437164307
step 1522, loss: 4.390408515930176
step 1523, loss: 4.479430675506592
step 1524, loss: 4.147347450256348
step 1525, loss: 4.176314353942871
step 1526, loss: 3.5925590991973877
step 1527, loss: 3.549842119216919
step 1528, loss: 3.5082507133483887
step 1529, loss: 5.268011569976807
step 1530, loss: 3.7318344116210938
step 1531, loss: 4.481520175933838
step 1532, loss: 4.316068649291992
step 1533, loss: 3.2897658348083496
step 1534, loss: 3.6626830101013184
step 1535, loss: 4.125843048095703
step 1536, loss: 3.7647464275360107
step 1537, loss: 4.083173751831055
step 1538, loss: 3.9247567653656006
step 1539, loss: 3.2031476497650146
step 1540, loss: 3.7307496070861816
step 1541, loss: 4.147764682769775
step 1542, loss: 3.684664726257324
step 1543, loss: 3.790799379348755
step 1544, loss: 3.0533735752105713
step 1545, loss: 3.698300361633301
step 1546, loss: 3.688175916671753
step 1547, loss: 3.623129367828369
step 1548, loss: 4.124351501464844
step 1549, loss: 3.1902263164520264
step 1550, loss: 3.300490140914917
step 1551, loss: 3.273968458175659
step 1552, loss: 3.5103116035461426
step 1553, loss: 3.826333999633789
step 1554, loss: 3.8317861557006836
step 1555, loss: 3.762712001800537
step 1556, loss: 3.715430736541748
step 1557, loss: 3.7689743041992188
step 1558, loss: 3.305586338043213
step 1559, loss: 3.494112253189087
step 1560, loss: 3.629570960998535
step 1561, loss: 4.01720666885376
step 1562, loss: 3.7049031257629395
step 1563, loss: 3.9943089485168457
step 1564, loss: 2.855201244354248
step 1565, loss: 3.8721933364868164
step 1566, loss: 4.9368696212768555
step 1567, loss: 3.499741315841675
step 1568, loss: 3.5357534885406494
step 1569, loss: 3.833828926086426
step 1570, loss: 3.1698105335235596
step 1571, loss: 3.578603506088257
step 1572, loss: 3.762000560760498
step 1573, loss: 3.5556275844573975
step 1574, loss: 2.738790273666382
step 1575, loss: 3.5933797359466553
step 1576, loss: 4.235395908355713
step 1577, loss: 3.570268154144287
step 1578, loss: 3.998405933380127
step 1579, loss: 3.5161495208740234
step 1580, loss: 2.6716132164001465
step 1581, loss: 3.3420228958129883
step 1582, loss: 4.440859794616699
step 1583, loss: 4.029382705688477
step 1584, loss: 3.9902806282043457
step 1585, loss: 3.6187620162963867
step 1586, loss: 3.606733798980713
step 1587, loss: 3.2343719005584717
step 1588, loss: 3.888909101486206
step 1589, loss: 3.565063238143921
step 1590, loss: 4.147151470184326
step 1591, loss: 3.0320181846618652
step 1592, loss: 4.155968189239502
step 1593, loss: 2.965320110321045
step 1594, loss: 3.397041082382202
step 1595, loss: 3.233675241470337
step 1596, loss: 3.222414970397949
step 1597, loss: 3.3384532928466797
step 1598, loss: 3.5004546642303467
step 1599, loss: 3.4800961017608643
step 1600, loss: 4.075765132904053
step 1601, loss: 3.306720018386841
step 1602, loss: 4.319707870483398
step 1603, loss: 3.195638656616211
step 1604, loss: 2.564107656478882
step 1605, loss: 4.050211429595947
step 1606, loss: 3.8626551628112793
step 1607, loss: 4.257378578186035
step 1608, loss: 3.542560577392578
step 1609, loss: 3.244330406188965
step 1610, loss: 3.825209140777588
step 1611, loss: 2.892665147781372
step 1612, loss: 3.2642552852630615
step 1613, loss: 3.2949588298797607
step 1614, loss: 4.142164707183838
step 1615, loss: 3.0990662574768066
step 1616, loss: 3.034379243850708
step 1617, loss: 4.147481441497803
step 1618, loss: 4.190699100494385
step 1619, loss: 3.1618027687072754
step 1620, loss: 3.1365041732788086
step 1621, loss: 3.4399750232696533
step 1622, loss: 4.545562744140625
step 1623, loss: 4.141633033752441
step 1624, loss: 3.612645149230957
step 1625, loss: 3.5544731616973877
step 1626, loss: 3.4954030513763428
step 1627, loss: 4.033622741699219
step 1628, loss: 3.5594515800476074
step 1629, loss: 4.596317768096924
step 1630, loss: 4.853719711303711
step 1631, loss: 4.3600592613220215
step 1632, loss: 4.367802619934082
step 1633, loss: 3.8414034843444824
step 1634, loss: 2.921929121017456
step 1635, loss: 3.4323573112487793
step 1636, loss: 3.8370935916900635
step 1637, loss: 3.0795493125915527
step 1638, loss: 3.513660430908203
step 1639, loss: 3.497284173965454
step 1640, loss: 2.958723306655884
step 1641, loss: 3.645677328109741
step 1642, loss: 2.3589837551116943
step 1643, loss: 3.1959128379821777
step 1644, loss: 3.8308820724487305
step 1645, loss: 2.9086880683898926
step 1646, loss: 3.235720634460449
step 1647, loss: 2.8863627910614014
step 1648, loss: 3.134303569793701
step 1649, loss: 3.664750576019287
step 1650, loss: 4.278973579406738
step 1651, loss: 3.467172384262085
step 1652, loss: 3.4862656593322754
step 1653, loss: 5.024254322052002
step 1654, loss: 3.2694919109344482
step 1655, loss: 4.190279483795166
step 1656, loss: 4.315976142883301
step 1657, loss: 4.132534980773926
step 1658, loss: 4.1632914543151855
step 1659, loss: 4.374462127685547
step 1660, loss: 2.8695309162139893
step 1661, loss: 3.6347126960754395
step 1662, loss: 5.07472038269043
step 1663, loss: 5.001006603240967
step 1664, loss: 5.095470905303955
step 1665, loss: 5.002006530761719
step 1666, loss: 4.416806697845459
step 1667, loss: 5.252553462982178
step 1668, loss: 4.848891258239746
step 1669, loss: 4.759152412414551
step 1670, loss: 4.26510763168335
step 1671, loss: 4.477893352508545
step 1672, loss: 4.333444118499756
step 1673, loss: 4.326839923858643
step 1674, loss: 4.296018600463867
step 1675, loss: 4.781900882720947
step 1676, loss: 4.53188419342041
step 1677, loss: 3.829169511795044
step 1678, loss: 4.544305801391602
step 1679, loss: 3.9153785705566406
step 1680, loss: 4.467805862426758
step 1681, loss: 5.243114471435547
step 1682, loss: 4.078434944152832
step 1683, loss: 4.565701961517334
step 1684, loss: 3.9482414722442627
step 1685, loss: 4.82194709777832
step 1686, loss: 4.174631118774414
step 1687, loss: 4.57816219329834
step 1688, loss: 4.892731189727783
step 1689, loss: 3.792924404144287
step 1690, loss: 3.8003737926483154
step 1691, loss: 4.503911972045898
step 1692, loss: 3.4872384071350098
step 1693, loss: 4.495286464691162
step 1694, loss: 3.9626591205596924
step 1695, loss: 4.157662868499756
step 1696, loss: 4.684443950653076
step 1697, loss: 4.817289352416992
step 1698, loss: 4.976292133331299
step 1699, loss: 3.5890352725982666
step 1700, loss: 4.605713844299316
step 1701, loss: 4.309084892272949
step 1702, loss: 4.6203718185424805
step 1703, loss: 4.035244464874268
step 1704, loss: 3.4345345497131348
step 1705, loss: 4.907566547393799
step 1706, loss: 3.557873249053955
step 1707, loss: 4.1239399909973145
step 1708, loss: 4.090895652770996
step 1709, loss: 4.279356002807617
step 1710, loss: 3.854407548904419
step 1711, loss: 2.8914666175842285
step 1712, loss: 4.358354091644287
step 1713, loss: 4.289380073547363
step 1714, loss: 4.306107044219971
step 1715, loss: 4.598341941833496
step 1716, loss: 3.7659621238708496
step 1717, loss: 3.0870566368103027
step 1718, loss: 3.727384567260742
step 1719, loss: 3.9972264766693115
step 1720, loss: 2.8388190269470215
step 1721, loss: 3.8946499824523926
step 1722, loss: 4.71946907043457
step 1723, loss: 3.9761078357696533
step 1724, loss: 3.3523736000061035
step 1725, loss: 4.943288326263428
step 1726, loss: 3.7103381156921387
step 1727, loss: 4.909368515014648
step 1728, loss: 4.052816390991211
step 1729, loss: 4.550168991088867
step 1730, loss: 3.6851513385772705
step 1731, loss: 3.6286416053771973
step 1732, loss: 3.8748135566711426
step 1733, loss: 4.424833297729492
step 1734, loss: 4.009310245513916
step 1735, loss: 4.491117000579834
step 1736, loss: 4.074754238128662
step 1737, loss: 4.6790289878845215
step 1738, loss: 3.93405818939209
step 1739, loss: 3.9582605361938477
step 1740, loss: 4.056864261627197
step 1741, loss: 4.2762250900268555
step 1742, loss: 4.306661605834961
step 1743, loss: 4.468788146972656
step 1744, loss: 3.684804677963257
step 1745, loss: 3.5403764247894287
step 1746, loss: 3.9781672954559326
step 1747, loss: 4.024982929229736
step 1748, loss: 3.909764528274536
step 1749, loss: 4.011043071746826
step 1750, loss: 2.875783920288086
step 1751, loss: 3.5035674571990967
step 1752, loss: 3.5395050048828125
step 1753, loss: 3.874788761138916
step 1754, loss: 3.900693893432617
step 1755, loss: 3.52898907661438
step 1756, loss: 4.007696151733398
step 1757, loss: 4.5420050621032715
step 1758, loss: 3.4611330032348633
step 1759, loss: 3.8788344860076904
step 1760, loss: 3.6005332469940186
step 1761, loss: 4.042022228240967
step 1762, loss: 3.5841619968414307
step 1763, loss: 4.169046401977539
step 1764, loss: 3.5417447090148926
step 1765, loss: 4.434978485107422
step 1766, loss: 4.773306846618652
step 1767, loss: 3.816502094268799
step 1768, loss: 4.5769500732421875
step 1769, loss: 4.631963729858398
step 1770, loss: 4.795139789581299
step 1771, loss: 4.011898994445801
step 1772, loss: 4.497737884521484
step 1773, loss: 4.413944721221924
step 1774, loss: 4.43413782119751
step 1775, loss: 3.599487066268921
step 1776, loss: 4.254575729370117
step 1777, loss: 3.3987324237823486
step 1778, loss: 4.365509986877441
step 1779, loss: 4.862760066986084
step 1780, loss: 4.476467609405518
step 1781, loss: 3.6675474643707275
step 1782, loss: 3.201237201690674
step 1783, loss: 2.7210114002227783
step 1784, loss: 3.777276039123535
step 1785, loss: 4.700158596038818
step 1786, loss: 4.688767910003662
step 1787, loss: 5.4728827476501465
step 1788, loss: 4.89219856262207
step 1789, loss: 4.089217662811279
step 1790, loss: 4.35177755355835
step 1791, loss: 3.383312702178955
step 1792, loss: 3.7555205821990967
step 1793, loss: 4.1139817237854
step 1794, loss: 4.35020112991333
step 1795, loss: 3.6883652210235596
step 1796, loss: 4.9594807624816895
step 1797, loss: 4.842873573303223
step 1798, loss: 4.571622848510742
step 1799, loss: 4.711950778961182
step 1800, loss: 5.5037007331848145
step 1801, loss: 4.844646453857422
step 1802, loss: 3.9894134998321533
step 1803, loss: 4.814039707183838
step 1804, loss: 4.57544469833374
step 1805, loss: 4.697498798370361
step 1806, loss: 4.273347854614258
step 1807, loss: 3.968597173690796
step 1808, loss: 4.8038010597229
step 1809, loss: 4.942145347595215
step 1810, loss: 5.0786356925964355
step 1811, loss: 4.28194522857666
step 1812, loss: 4.545538425445557
step 1813, loss: 5.023248195648193
step 1814, loss: 4.230413436889648
step 1815, loss: 4.075809001922607
step 1816, loss: 4.0729451179504395
step 1817, loss: 4.856351852416992
step 1818, loss: 4.988836765289307
step 1819, loss: 5.405519962310791
step 1820, loss: 4.982843399047852
step 1821, loss: 4.902467250823975
step 1822, loss: 3.9083614349365234
step 1823, loss: 3.52346134185791
step 1824, loss: 2.955436944961548
step 1825, loss: 3.818349838256836
step 1826, loss: 4.32371187210083
step 1827, loss: 4.292875289916992
step 1828, loss: 3.150519847869873
step 1829, loss: 3.9114925861358643
step 1830, loss: 4.927152633666992
step 1831, loss: 4.770104885101318
step 1832, loss: 4.582746982574463
step 1833, loss: 4.636335372924805
step 1834, loss: 3.8256657123565674
step 1835, loss: 4.411146640777588
step 1836, loss: 4.66691255569458
step 1837, loss: 4.136537075042725
step 1838, loss: 4.454726696014404
step 1839, loss: 4.234221458435059
step 1840, loss: 4.702349662780762
step 1841, loss: 4.742627143859863
step 1842, loss: 5.101099491119385
step 1843, loss: 3.8632442951202393
step 1844, loss: 4.232033729553223
step 1845, loss: 4.130611419677734
step 1846, loss: 3.897416830062866
step 1847, loss: 3.5633461475372314
step 1848, loss: 3.93237566947937
step 1849, loss: 4.947783470153809
step 1850, loss: 5.300245761871338
step 1851, loss: 4.479167461395264
step 1852, loss: 3.8686389923095703
step 1853, loss: 5.078954219818115
step 1854, loss: 3.97860050201416
step 1855, loss: 4.496209621429443
step 1856, loss: 3.439645528793335
step 1857, loss: 3.555453062057495
step 1858, loss: 4.014534950256348
step 1859, loss: 3.806464910507202
step 1860, loss: 3.341094493865967
step 1861, loss: 2.864633798599243
step 1862, loss: 2.7042317390441895
step 1863, loss: 4.537848472595215
step 1864, loss: 4.240298748016357
step 1865, loss: 4.091999530792236
step 1866, loss: 3.8534555435180664
step 1867, loss: 4.573892593383789
step 1868, loss: 4.368469715118408
step 1869, loss: 3.729868173599243
step 1870, loss: 3.539781332015991
step 1871, loss: 3.257795810699463
step 1872, loss: 3.987076759338379
step 1873, loss: 3.945329189300537
step 1874, loss: 2.570592164993286
step 1875, loss: 4.171372413635254
step 1876, loss: 4.777200698852539
step 1877, loss: 4.583169937133789
step 1878, loss: 3.9273061752319336
step 1879, loss: 4.385992527008057
step 1880, loss: 3.793640613555908
step 1881, loss: 3.5521438121795654
step 1882, loss: 4.35752010345459
step 1883, loss: 4.328475475311279
step 1884, loss: 3.89709210395813
step 1885, loss: 3.389646053314209
step 1886, loss: 4.279075622558594
step 1887, loss: 3.8319225311279297
step 1888, loss: 4.295835494995117
step 1889, loss: 3.7960662841796875
step 1890, loss: 3.9751694202423096
step 1891, loss: 3.7107362747192383
step 1892, loss: 3.5136287212371826
step 1893, loss: 3.675581455230713
step 1894, loss: 5.558213233947754
step 1895, loss: 5.123699188232422
step 1896, loss: 4.096446514129639
step 1897, loss: 2.867234230041504
step 1898, loss: 4.220346450805664
step 1899, loss: 3.363956928253174
step 1900, loss: 3.5054306983947754
step 1901, loss: 2.523087739944458
step 1902, loss: 4.987675666809082
step 1903, loss: 3.97619891166687
step 1904, loss: 3.7315003871917725
step 1905, loss: 3.310948610305786
step 1906, loss: 3.6153857707977295
step 1907, loss: 4.0677385330200195
step 1908, loss: 4.384264945983887
step 1909, loss: 3.3866710662841797
step 1910, loss: 3.626985788345337
step 1911, loss: 3.52337384223938
step 1912, loss: 4.469731330871582
step 1913, loss: 4.1162495613098145
step 1914, loss: 5.150106430053711
step 1915, loss: 4.391594409942627
step 1916, loss: 3.014254331588745
step 1917, loss: 3.299788475036621
step 1918, loss: 3.7020201683044434
step 1919, loss: 4.338705539703369
step 1920, loss: 4.482752799987793
step 1921, loss: 3.910128116607666
step 1922, loss: 4.3665900230407715
step 1923, loss: 4.498601913452148
step 1924, loss: 4.28736686706543
step 1925, loss: 4.215439319610596
step 1926, loss: 4.22429084777832
step 1927, loss: 3.2373037338256836
step 1928, loss: 3.730323553085327
step 1929, loss: 3.937493324279785
step 1930, loss: 4.385696887969971
step 1931, loss: 3.9795687198638916
step 1932, loss: 3.7075183391571045
step 1933, loss: 4.53385066986084
step 1934, loss: 4.413722038269043
step 1935, loss: 4.573367595672607
step 1936, loss: 4.197123050689697
step 1937, loss: 4.545747756958008
step 1938, loss: 4.249222278594971
step 1939, loss: 4.08671760559082
step 1940, loss: 3.9205899238586426
step 1941, loss: 2.918764352798462
step 1942, loss: 4.200781345367432
step 1943, loss: 3.5923945903778076
step 1944, loss: 3.706423044204712
step 1945, loss: 4.568185806274414
step 1946, loss: 4.811176300048828
step 1947, loss: 4.468579292297363
step 1948, loss: 4.3152995109558105
step 1949, loss: 4.904822826385498
step 1950, loss: 4.788809776306152
step 1951, loss: 4.522418975830078
step 1952, loss: 4.610086917877197
step 1953, loss: 4.646368503570557
step 1954, loss: 4.899321556091309
step 1955, loss: 4.513829708099365
step 1956, loss: 4.595536231994629
step 1957, loss: 4.326501846313477
step 1958, loss: 3.1649415493011475
step 1959, loss: 3.28519344329834
step 1960, loss: 3.0907235145568848
step 1961, loss: 3.3356192111968994
step 1962, loss: 3.7747998237609863
step 1963, loss: 4.120743751525879
step 1964, loss: 4.5863847732543945
step 1965, loss: 3.755556583404541
step 1966, loss: 4.37576961517334
step 1967, loss: 4.098357200622559
step 1968, loss: 3.3005223274230957
step 1969, loss: 3.7145674228668213
step 1970, loss: 3.8800618648529053
step 1971, loss: 3.7933311462402344
step 1972, loss: 3.6206064224243164
step 1973, loss: 4.7666544914245605
step 1974, loss: 3.2778851985931396
step 1975, loss: 3.9910671710968018
step 1976, loss: 4.74428653717041
step 1977, loss: 3.8437817096710205
step 1978, loss: 4.199256420135498
step 1979, loss: 4.735333442687988
step 1980, loss: 4.846710681915283
step 1981, loss: 4.4306745529174805
step 1982, loss: 5.068162441253662
step 1983, loss: 4.359621524810791
step 1984, loss: 3.974388837814331
step 1985, loss: 4.3208842277526855
step 1986, loss: 3.440865993499756
step 1987, loss: 4.333793640136719
step 1988, loss: 3.9709737300872803
step 1989, loss: 3.712111234664917
step 1990, loss: 4.400705337524414
step 1991, loss: 3.449748992919922
step 1992, loss: 4.355422019958496
step 1993, loss: 4.116179466247559
step 1994, loss: 3.6617913246154785
step 1995, loss: 3.8691186904907227
step 1996, loss: 3.813861131668091
step 1997, loss: 3.7523391246795654
step 1998, loss: 3.969818353652954
step 1999, loss: 4.301666259765625
step 2000, loss: 4.80532169342041
step 2001, loss: 3.1664369106292725
step 2002, loss: 4.805002689361572
step 2003, loss: 4.875154495239258
step 2004, loss: 4.128880500793457
step 2005, loss: 4.492619514465332
step 2006, loss: 3.675485134124756
step 2007, loss: 4.630589485168457
step 2008, loss: 4.041731357574463
step 2009, loss: 4.800642967224121
step 2010, loss: 4.219773769378662
step 2011, loss: 4.689845085144043
step 2012, loss: 4.97020149230957
step 2013, loss: 3.805495262145996
step 2014, loss: 3.8481898307800293
step 2015, loss: 4.172545909881592
step 2016, loss: 4.681247711181641
step 2017, loss: 3.6974668502807617
step 2018, loss: 4.9898529052734375
step 2019, loss: 3.682027816772461
step 2020, loss: 3.7182068824768066
step 2021, loss: 2.9635531902313232
step 2022, loss: 4.220370292663574
step 2023, loss: 4.576808929443359
step 2024, loss: 3.874293327331543
step 2025, loss: 3.6369454860687256
step 2026, loss: 4.591183662414551
step 2027, loss: 3.648998498916626
step 2028, loss: 3.23907208442688
step 2029, loss: 3.592716693878174
step 2030, loss: 3.2141172885894775
step 2031, loss: 4.109504222869873
step 2032, loss: 4.186150550842285
step 2033, loss: 3.5454795360565186
step 2034, loss: 3.241811752319336
step 2035, loss: 4.098503589630127
step 2036, loss: 3.39775013923645
step 2037, loss: 2.4532008171081543
step 2038, loss: 2.8632516860961914
step 2039, loss: 3.1300604343414307
step 2040, loss: 3.871826171875
step 2041, loss: 3.8484437465667725
step 2042, loss: 2.942840576171875
step 2043, loss: 3.663764715194702
step 2044, loss: 2.792384147644043
step 2045, loss: 3.4472219944000244
step 2046, loss: 2.879856586456299
step 2047, loss: 4.174984931945801
step 2048, loss: 3.9577972888946533
step 2049, loss: 3.2868902683258057
step 2050, loss: 3.523707866668701
step 2051, loss: 2.4660942554473877
step 2052, loss: 3.5479986667633057
step 2053, loss: 4.119640827178955
step 2054, loss: 3.71601939201355
step 2055, loss: 3.2462499141693115
step 2056, loss: 3.127406597137451
step 2057, loss: 3.512878179550171
step 2058, loss: 2.7224960327148438
step 2059, loss: 3.910930871963501
step 2060, loss: 3.042349338531494
step 2061, loss: 3.991663694381714
step 2062, loss: 3.772362470626831
step 2063, loss: 4.080572605133057
step 2064, loss: 3.7702441215515137
step 2065, loss: 5.4093523025512695
step 2066, loss: 3.570236921310425
step 2067, loss: 3.496011972427368
step 2068, loss: 3.219170093536377
step 2069, loss: 2.827958822250366
step 2070, loss: 4.448737621307373
step 2071, loss: 4.1287336349487305
step 2072, loss: 3.6784584522247314
step 2073, loss: 3.778092384338379
step 2074, loss: 2.7745492458343506
step 2075, loss: 3.0421760082244873
step 2076, loss: 3.318465232849121
step 2077, loss: 3.916893482208252
step 2078, loss: 5.038541316986084
step 2079, loss: 4.121471881866455
step 2080, loss: 3.4639902114868164
step 2081, loss: 3.2603299617767334
step 2082, loss: 4.251362323760986
step 2083, loss: 3.1200270652770996
step 2084, loss: 3.1641881465911865
step 2085, loss: 3.6738393306732178
step 2086, loss: 3.476640224456787
step 2087, loss: 4.154770374298096
step 2088, loss: 3.4797022342681885
step 2089, loss: 3.369138240814209
step 2090, loss: 4.192558288574219
step 2091, loss: 3.6258909702301025
step 2092, loss: 2.776386022567749
step 2093, loss: 4.3170166015625
step 2094, loss: 4.514590740203857
step 2095, loss: 4.391199588775635
step 2096, loss: 3.9442827701568604
step 2097, loss: 4.323277950286865
step 2098, loss: 4.337812900543213
step 2099, loss: 4.596827983856201
step 2100, loss: 4.491890907287598
step 2101, loss: 2.767896890640259
step 2102, loss: 3.293520927429199
step 2103, loss: 2.9378740787506104
step 2104, loss: 3.475322723388672
step 2105, loss: 4.095357894897461
step 2106, loss: 4.606265068054199
step 2107, loss: 3.2994155883789062
step 2108, loss: 2.9609601497650146
step 2109, loss: 3.819707155227661
step 2110, loss: 4.724284648895264
step 2111, loss: 3.912860870361328
step 2112, loss: 3.220250368118286
step 2113, loss: 2.9680304527282715
step 2114, loss: 4.578416347503662
step 2115, loss: 3.2556824684143066
step 2116, loss: 4.209364891052246
step 2117, loss: 4.155882835388184
step 2118, loss: 4.373993873596191
step 2119, loss: 4.555430889129639
step 2120, loss: 4.528937339782715
step 2121, loss: 4.114866733551025
step 2122, loss: 4.122212886810303
step 2123, loss: 4.864999294281006
step 2124, loss: 4.012133598327637
step 2125, loss: 3.5692038536071777
step 2126, loss: 4.372932434082031
step 2127, loss: 3.6018872261047363
step 2128, loss: 3.9435060024261475
step 2129, loss: 3.791647434234619
step 2130, loss: 4.508261680603027
step 2131, loss: 3.6777303218841553
step 2132, loss: 3.19094181060791
step 2133, loss: 3.419450521469116
step 2134, loss: 3.2428910732269287
step 2135, loss: 2.6503124237060547
step 2136, loss: 3.329784870147705
step 2137, loss: 3.883267402648926
step 2138, loss: 3.712125301361084
step 2139, loss: 3.138648271560669
step 2140, loss: 3.37255597114563
step 2141, loss: 3.3740527629852295
step 2142, loss: 3.975303888320923
step 2143, loss: 2.9960973262786865
step 2144, loss: 3.0417423248291016
step 2145, loss: 4.114492416381836
step 2146, loss: 4.2557878494262695
step 2147, loss: 3.5663909912109375
step 2148, loss: 3.843837022781372
step 2149, loss: 2.9346799850463867
step 2150, loss: 3.654033899307251
step 2151, loss: 4.0583415031433105
step 2152, loss: 4.170350074768066
step 2153, loss: 4.043254852294922
step 2154, loss: 4.22180700302124
step 2155, loss: 5.749344348907471
step 2156, loss: 4.252413272857666
step 2157, loss: 3.4839887619018555
step 2158, loss: 3.936753273010254
step 2159, loss: 4.160962104797363
step 2160, loss: 2.687218427658081
step 2161, loss: 4.294384479522705
step 2162, loss: 2.910207986831665
step 2163, loss: 3.4411585330963135
step 2164, loss: 4.012099266052246
step 2165, loss: 3.6740074157714844
step 2166, loss: 3.8737306594848633
step 2167, loss: 4.143723011016846
step 2168, loss: 3.785250186920166
step 2169, loss: 3.5034537315368652
step 2170, loss: 3.7832014560699463
step 2171, loss: 2.653291702270508
step 2172, loss: 3.8779337406158447
step 2173, loss: 3.3521041870117188
step 2174, loss: 3.1773321628570557
step 2175, loss: 3.0375242233276367
step 2176, loss: 3.5767970085144043
step 2177, loss: 3.9871981143951416
step 2178, loss: 4.074426174163818
step 2179, loss: 3.7154526710510254
step 2180, loss: 3.651834726333618
step 2181, loss: 2.974773645401001
step 2182, loss: 3.562837839126587
step 2183, loss: 4.049341201782227
step 2184, loss: 4.309670448303223
step 2185, loss: 5.646754264831543
step 2186, loss: 4.859975814819336
step 2187, loss: 2.931990385055542
step 2188, loss: 2.758404493331909
step 2189, loss: 3.2576165199279785
step 2190, loss: 3.2696640491485596
step 2191, loss: 2.8043220043182373
step 2192, loss: 3.9405970573425293
step 2193, loss: 3.608898401260376
step 2194, loss: 4.141238689422607
step 2195, loss: 3.2032053470611572
step 2196, loss: 3.14662504196167
step 2197, loss: 2.600966453552246
step 2198, loss: 4.155918121337891
step 2199, loss: 3.367034673690796
step 2200, loss: 3.780571460723877
step 2201, loss: 3.816660165786743
step 2202, loss: 2.6466493606567383
step 2203, loss: 2.9832119941711426
step 2204, loss: 4.2639899253845215
step 2205, loss: 3.870163679122925
step 2206, loss: 4.680784225463867
step 2207, loss: 4.292637348175049
step 2208, loss: 4.148809432983398
step 2209, loss: 3.302985429763794
step 2210, loss: 3.5909290313720703
step 2211, loss: 3.578928232192993
step 2212, loss: 4.228278636932373
step 2213, loss: 3.7067484855651855
step 2214, loss: 3.2410876750946045
step 2215, loss: 3.0196709632873535
step 2216, loss: 3.7979209423065186
step 2217, loss: 3.7574191093444824
step 2218, loss: 3.686765432357788
step 2219, loss: 2.626782178878784
step 2220, loss: 2.6953439712524414
step 2221, loss: 3.275940179824829
step 2222, loss: 3.793972969055176
step 2223, loss: 4.042232036590576
step 2224, loss: 4.266634941101074
step 2225, loss: 3.4120664596557617
step 2226, loss: 3.588796377182007
step 2227, loss: 3.7020652294158936
step 2228, loss: 4.0746259689331055
step 2229, loss: 3.5088250637054443
step 2230, loss: 1.8615942001342773
Saved best model with loss: 1.8616 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.8616.pt
step 2231, loss: 2.843229293823242
step 2232, loss: 2.98380970954895
step 2233, loss: 2.8999404907226562
step 2234, loss: 3.4724855422973633
step 2235, loss: 3.891106128692627
step 2236, loss: 4.095462799072266
step 2237, loss: 4.241629123687744
step 2238, loss: 3.3923163414001465
step 2239, loss: 4.384880065917969
step 2240, loss: 2.8479714393615723
step 2241, loss: 2.9402315616607666
step 2242, loss: 2.642613172531128
step 2243, loss: 3.4548752307891846
step 2244, loss: 3.8062760829925537
step 2245, loss: 4.470736503601074
step 2246, loss: 2.923659324645996
step 2247, loss: 2.747283935546875
step 2248, loss: 3.217499256134033
step 2249, loss: 3.5757763385772705
step 2250, loss: 3.213038444519043
step 2251, loss: 3.4365413188934326
step 2252, loss: 3.2074389457702637
step 2253, loss: 4.092085838317871
step 2254, loss: 3.8571596145629883
step 2255, loss: 5.1386284828186035
step 2256, loss: 3.520805597305298
step 2257, loss: 2.5571494102478027
step 2258, loss: 3.692981719970703
step 2259, loss: 3.309077501296997
step 2260, loss: 3.427436113357544
step 2261, loss: 3.055009126663208
step 2262, loss: 3.280353546142578
step 2263, loss: 2.9758412837982178
step 2264, loss: 3.42354154586792
step 2265, loss: 3.2034268379211426
step 2266, loss: 3.250697135925293
step 2267, loss: 4.278157711029053
step 2268, loss: 4.767677307128906
step 2269, loss: 4.454222202301025
step 2270, loss: 4.968336582183838
step 2271, loss: 3.847212314605713
step 2272, loss: 4.162345886230469
step 2273, loss: 4.941168785095215
step 2274, loss: 4.460485935211182
step 2275, loss: 4.011634826660156
step 2276, loss: 3.4177534580230713
step 2277, loss: 4.181262493133545
step 2278, loss: 4.176617622375488
step 2279, loss: 4.633391857147217
step 2280, loss: 4.706436634063721
step 2281, loss: 4.434943199157715
step 2282, loss: 4.360662937164307
step 2283, loss: 4.806578636169434
step 2284, loss: 4.786157131195068
step 2285, loss: 4.868941307067871
step 2286, loss: 4.7164835929870605
step 2287, loss: 4.454472064971924
step 2288, loss: 3.682851552963257
step 2289, loss: 3.8144989013671875
step 2290, loss: 3.7971553802490234
step 2291, loss: 4.709353923797607
step 2292, loss: 3.192744731903076
step 2293, loss: 3.619112014770508
step 2294, loss: 4.066197872161865
step 2295, loss: 4.279922008514404
step 2296, loss: 3.8913278579711914
step 2297, loss: 5.37071418762207
step 2298, loss: 5.122952461242676
step 2299, loss: 5.58644962310791
step 2300, loss: 4.333556175231934
step 2301, loss: 4.334861755371094
step 2302, loss: 4.5060906410217285
step 2303, loss: 4.374830722808838
step 2304, loss: 4.321855068206787
step 2305, loss: 4.360135078430176
step 2306, loss: 4.299812316894531
step 2307, loss: 4.621870517730713
step 2308, loss: 3.7778377532958984
step 2309, loss: 4.339416027069092
step 2310, loss: 4.557493686676025
step 2311, loss: 3.866603374481201
step 2312, loss: 3.8893356323242188
step 2313, loss: 4.304935455322266
step 2314, loss: 3.916825532913208
step 2315, loss: 3.9764366149902344
step 2316, loss: 3.049919366836548
step 2317, loss: 4.527043342590332
step 2318, loss: 3.9405558109283447
step 2319, loss: 3.7972609996795654
step 2320, loss: 3.8080620765686035
step 2321, loss: 3.4599926471710205
step 2322, loss: 3.6955909729003906
step 2323, loss: 3.5934033393859863
step 2324, loss: 3.8995699882507324
step 2325, loss: 3.5798797607421875
step 2326, loss: 4.74055290222168
step 2327, loss: 4.4685282707214355
step 2328, loss: 3.7412991523742676
step 2329, loss: 4.142612934112549
step 2330, loss: 3.4262330532073975
step 2331, loss: 4.845791816711426
step 2332, loss: 4.180685520172119
step 2333, loss: 3.8218061923980713
step 2334, loss: 3.4743473529815674
step 2335, loss: 4.208395957946777
step 2336, loss: 3.999936103820801
step 2337, loss: 3.913684129714966
step 2338, loss: 3.4191784858703613
step 2339, loss: 3.917707920074463
step 2340, loss: 3.0318713188171387
step 2341, loss: 4.001780986785889
step 2342, loss: 3.8657402992248535
step 2343, loss: 3.043548107147217
step 2344, loss: 3.0775320529937744
step 2345, loss: 4.5879950523376465
step 2346, loss: 3.3819632530212402
step 2347, loss: 2.737971305847168
step 2348, loss: 2.676352024078369
step 2349, loss: 2.52237868309021
step 2350, loss: 3.9666943550109863
step 2351, loss: 2.9776611328125
step 2352, loss: 3.6724648475646973
step 2353, loss: 3.8197898864746094
step 2354, loss: 3.861020803451538
step 2355, loss: 3.8495750427246094
step 2356, loss: 3.3932695388793945
step 2357, loss: 3.833989381790161
step 2358, loss: 3.288665294647217
step 2359, loss: 2.656717538833618
step 2360, loss: 3.537149429321289
step 2361, loss: 3.031538724899292
step 2362, loss: 2.893348455429077
step 2363, loss: 4.294775485992432
step 2364, loss: 3.285804271697998
step 2365, loss: 3.9127368927001953
step 2366, loss: 3.7547104358673096
step 2367, loss: 3.3564724922180176
step 2368, loss: 3.8209776878356934
step 2369, loss: 3.5364675521850586
step 2370, loss: 2.7259089946746826
step 2371, loss: 4.493001937866211
step 2372, loss: 3.0678601264953613
step 2373, loss: 3.9086484909057617
step 2374, loss: 3.539933681488037
step 2375, loss: 4.879960536956787
step 2376, loss: 2.5973851680755615
step 2377, loss: 3.0327935218811035
step 2378, loss: 3.291977643966675
step 2379, loss: 2.4700894355773926
step 2380, loss: 3.1077804565429688
step 2381, loss: 2.067718982696533
step 2382, loss: 2.3899426460266113
step 2383, loss: 4.360833644866943
step 2384, loss: 3.9031848907470703
step 2385, loss: 2.9983901977539062
step 2386, loss: 4.0999436378479
step 2387, loss: 2.955819606781006
step 2388, loss: 3.8808443546295166
step 2389, loss: 3.249911069869995
step 2390, loss: 4.431632995605469
step 2391, loss: 3.1848154067993164
step 2392, loss: 3.783012866973877
step 2393, loss: 3.784637212753296
step 2394, loss: 3.717611789703369
step 2395, loss: 5.581360340118408
step 2396, loss: 3.903869390487671
step 2397, loss: 3.9026036262512207
step 2398, loss: 3.4778854846954346
step 2399, loss: 3.2206833362579346
step 2400, loss: 3.952068328857422
step 2401, loss: 4.22852087020874
step 2402, loss: 3.9405293464660645
step 2403, loss: 3.923067808151245
step 2404, loss: 4.471817493438721
step 2405, loss: 4.729060649871826
step 2406, loss: 4.439912796020508
step 2407, loss: 3.80370831489563
step 2408, loss: 3.578108072280884
step 2409, loss: 3.590137481689453
step 2410, loss: 3.197969913482666
step 2411, loss: 4.037230014801025
step 2412, loss: 3.932810068130493
step 2413, loss: 3.388380527496338
step 2414, loss: 2.1204960346221924
step 2415, loss: 4.846760272979736
step 2416, loss: 5.957024574279785
step 2417, loss: 4.626539707183838
step 2418, loss: 3.0604424476623535
step 2419, loss: 2.6117711067199707
step 2420, loss: 2.8957314491271973
step 2421, loss: 3.8901188373565674
step 2422, loss: 3.7963736057281494
step 2423, loss: 3.56679105758667
step 2424, loss: 3.61769437789917
step 2425, loss: 3.8550899028778076
step 2426, loss: 3.3272643089294434
step 2427, loss: 3.136282444000244
step 2428, loss: 4.105996608734131
step 2429, loss: 4.746825695037842
step 2430, loss: 3.716346263885498
step 2431, loss: 3.270484447479248
step 2432, loss: 1.8898040056228638
step 2433, loss: 2.7285995483398438
step 2434, loss: 2.9509880542755127
step 2435, loss: 3.9435107707977295
step 2436, loss: 4.303819179534912
step 2437, loss: 2.685112714767456
step 2438, loss: 3.6840360164642334
step 2439, loss: 4.586297035217285
step 2440, loss: 4.197662353515625
step 2441, loss: 4.131105422973633
step 2442, loss: 3.9502322673797607
step 2443, loss: 4.369253635406494
step 2444, loss: 3.461303234100342
step 2445, loss: 3.523827075958252
step 2446, loss: 3.793012857437134
step 2447, loss: 4.253950119018555
step 2448, loss: 3.743455410003662
step 2449, loss: 3.457380533218384
step 2450, loss: 3.7713470458984375
step 2451, loss: 3.530728816986084
step 2452, loss: 4.5158491134643555
step 2453, loss: 3.9249935150146484
step 2454, loss: 4.107666015625
step 2455, loss: 3.178114414215088
step 2456, loss: 3.7715113162994385
step 2457, loss: 4.089496612548828
step 2458, loss: 3.4672391414642334
step 2459, loss: 4.479588985443115
step 2460, loss: 4.562192440032959
step 2461, loss: 3.1280953884124756
step 2462, loss: 3.626164674758911
step 2463, loss: 3.393207311630249
step 2464, loss: 3.5429558753967285
step 2465, loss: 3.460897207260132
step 2466, loss: 3.1925625801086426
step 2467, loss: 2.9032351970672607
step 2468, loss: 3.1923611164093018
step 2469, loss: 3.561359167098999
step 2470, loss: 3.1871094703674316
step 2471, loss: 3.4518561363220215
step 2472, loss: 3.112323045730591
step 2473, loss: 3.702404737472534
step 2474, loss: 3.2992682456970215
step 2475, loss: 4.362199306488037
step 2476, loss: 3.127263307571411
step 2477, loss: 2.907327651977539
step 2478, loss: 3.236956834793091
step 2479, loss: 3.293729543685913
step 2480, loss: 3.0526928901672363
step 2481, loss: 3.137998580932617
step 2482, loss: 3.8585011959075928
step 2483, loss: 3.552290439605713
step 2484, loss: 3.72220516204834
step 2485, loss: 3.2412054538726807
step 2486, loss: 4.711388111114502
step 2487, loss: 3.0570220947265625
step 2488, loss: 3.728792190551758
step 2489, loss: 3.042954921722412
step 2490, loss: 3.1354119777679443
step 2491, loss: 2.5048670768737793
step 2492, loss: 3.0384647846221924
step 2493, loss: 4.5128493309021
step 2494, loss: 3.4325413703918457
step 2495, loss: 3.30551815032959
step 2496, loss: 2.9983086585998535
step 2497, loss: 3.345400810241699
step 2498, loss: 4.2419047355651855
step 2499, loss: 3.74143648147583
step 2500, loss: 3.8181419372558594
step 2501, loss: 3.865884780883789
step 2502, loss: 2.5617191791534424
step 2503, loss: 2.99723482131958
step 2504, loss: 3.593989133834839
step 2505, loss: 3.7243130207061768
step 2506, loss: 3.473693370819092
step 2507, loss: 2.745995044708252
step 2508, loss: 2.4933385848999023
step 2509, loss: 3.4781715869903564
step 2510, loss: 3.8748116493225098
step 2511, loss: 3.467758893966675
step 2512, loss: 3.6795501708984375
step 2513, loss: 3.3123130798339844
step 2514, loss: 3.7965645790100098
step 2515, loss: 3.0598363876342773
step 2516, loss: 3.08115553855896
step 2517, loss: 3.296983242034912
step 2518, loss: 2.970069169998169
step 2519, loss: 3.371201753616333
step 2520, loss: 3.0288665294647217
step 2521, loss: 2.9170634746551514
step 2522, loss: 3.7235090732574463
step 2523, loss: 2.9892666339874268
step 2524, loss: 3.3874385356903076
step 2525, loss: 2.5171189308166504
step 2526, loss: 2.84989333152771
step 2527, loss: 3.111140489578247
step 2528, loss: 3.3573920726776123
step 2529, loss: 2.7967917919158936
step 2530, loss: 2.6068625450134277
step 2531, loss: 3.2147302627563477
step 2532, loss: 3.4912400245666504
step 2533, loss: 2.814434766769409
step 2534, loss: 3.0808494091033936
step 2535, loss: 3.064823627471924
step 2536, loss: 2.8653934001922607
step 2537, loss: 3.1743526458740234
step 2538, loss: 3.5946240425109863
step 2539, loss: 2.89628529548645
step 2540, loss: 3.0564467906951904
step 2541, loss: 2.1429378986358643
step 2542, loss: 2.624221086502075
step 2543, loss: 2.751352310180664
step 2544, loss: 2.254242181777954
step 2545, loss: 2.626011848449707
step 2546, loss: 3.35250186920166
step 2547, loss: 3.9374022483825684
step 2548, loss: 3.014292001724243
step 2549, loss: 4.342888832092285
step 2550, loss: 4.531365871429443
step 2551, loss: 4.393959999084473
step 2552, loss: 4.492416858673096
step 2553, loss: 2.9965009689331055
step 2554, loss: 2.946709394454956
step 2555, loss: 4.961450576782227
step 2556, loss: 4.437275409698486
step 2557, loss: 4.186797142028809
step 2558, loss: 4.781224727630615
step 2559, loss: 4.353338241577148
step 2560, loss: 3.693765640258789
step 2561, loss: 4.2282562255859375
step 2562, loss: 4.821060657501221
step 2563, loss: 4.340752124786377
step 2564, loss: 4.273187160491943
step 2565, loss: 4.530483722686768
step 2566, loss: 3.9521923065185547
step 2567, loss: 3.912402629852295
step 2568, loss: 3.6373374462127686
step 2569, loss: 3.988043785095215
step 2570, loss: 4.647706508636475
step 2571, loss: 4.2326979637146
step 2572, loss: 5.180253982543945
step 2573, loss: 4.326800346374512
step 2574, loss: 4.438080787658691
step 2575, loss: 4.654394149780273
step 2576, loss: 4.144320964813232
step 2577, loss: 4.669172763824463
step 2578, loss: 4.167651176452637
step 2579, loss: 4.552528381347656
step 2580, loss: 3.755178451538086
step 2581, loss: 4.795931339263916
step 2582, loss: 4.160912036895752
step 2583, loss: 5.039548397064209
step 2584, loss: 4.393490791320801
step 2585, loss: 3.9671764373779297
step 2586, loss: 4.305509090423584
step 2587, loss: 4.297478675842285
step 2588, loss: 3.2960152626037598
step 2589, loss: 3.80942440032959
step 2590, loss: 3.317531108856201
step 2591, loss: 4.194085121154785
step 2592, loss: 5.0382819175720215
step 2593, loss: 4.643973350524902
step 2594, loss: 3.665170907974243
step 2595, loss: 3.6335017681121826
step 2596, loss: 3.6049678325653076
step 2597, loss: 4.460343360900879
step 2598, loss: 4.351992130279541
step 2599, loss: 4.467405796051025
step 2600, loss: 3.7877352237701416
step 2601, loss: 4.48604154586792
step 2602, loss: 4.454709529876709
step 2603, loss: 4.499867916107178
step 2604, loss: 4.1482415199279785
step 2605, loss: 4.9404754638671875
step 2606, loss: 3.702488899230957
step 2607, loss: 4.169976234436035
step 2608, loss: 3.4141783714294434
step 2609, loss: 3.3625998497009277
step 2610, loss: 4.003820419311523
step 2611, loss: 3.8012499809265137
step 2612, loss: 3.9266560077667236
step 2613, loss: 3.0795350074768066
step 2614, loss: 3.5250890254974365
step 2615, loss: 3.713637113571167
step 2616, loss: 3.731339931488037
step 2617, loss: 3.3930304050445557
step 2618, loss: 3.4189977645874023
step 2619, loss: 2.889744758605957
step 2620, loss: 3.1742336750030518
step 2621, loss: 3.4170570373535156
step 2622, loss: 3.1051573753356934
step 2623, loss: 4.151579856872559
step 2624, loss: 3.8946075439453125
step 2625, loss: 3.522510290145874
step 2626, loss: 3.123948335647583
step 2627, loss: 3.069263458251953
step 2628, loss: 4.593227386474609
step 2629, loss: 4.285057544708252
step 2630, loss: 3.977254629135132
step 2631, loss: 3.0275397300720215
step 2632, loss: 3.189753770828247
step 2633, loss: 4.069242477416992
step 2634, loss: 4.018171310424805
step 2635, loss: 2.868379831314087
step 2636, loss: 3.6754181385040283
step 2637, loss: 2.8950841426849365
step 2638, loss: 3.311678647994995
step 2639, loss: 3.7752673625946045
step 2640, loss: 3.3157153129577637
step 2641, loss: 4.024270534515381
step 2642, loss: 3.754455804824829
step 2643, loss: 3.908071279525757
step 2644, loss: 3.580782651901245
step 2645, loss: 3.5224249362945557
step 2646, loss: 4.322994232177734
step 2647, loss: 4.26381778717041
step 2648, loss: 3.455364465713501
step 2649, loss: 4.033283233642578
step 2650, loss: 3.905444860458374
step 2651, loss: 3.474217653274536
step 2652, loss: 3.652618885040283
step 2653, loss: 3.638960361480713
step 2654, loss: 3.7795770168304443
step 2655, loss: 3.6960933208465576
step 2656, loss: 3.885136365890503
step 2657, loss: 4.729678153991699
step 2658, loss: 3.4983978271484375
step 2659, loss: 4.036190032958984
step 2660, loss: 3.9101202487945557
step 2661, loss: 3.7941362857818604
step 2662, loss: 2.7149510383605957
step 2663, loss: 3.4963300228118896
step 2664, loss: 3.1992123126983643
step 2665, loss: 3.1295907497406006
step 2666, loss: 3.165310859680176
step 2667, loss: 3.701862335205078
step 2668, loss: 3.6493592262268066
step 2669, loss: 3.4688527584075928
step 2670, loss: 3.876986026763916
step 2671, loss: 3.7478480339050293
step 2672, loss: 3.6644210815429688
step 2673, loss: 2.9742932319641113
step 2674, loss: 4.583812236785889
step 2675, loss: 3.9551732540130615
step 2676, loss: 3.238372802734375
step 2677, loss: 3.97786283493042
step 2678, loss: 2.95237135887146
step 2679, loss: 3.1152102947235107
step 2680, loss: 4.017414093017578
step 2681, loss: 2.5984253883361816
step 2682, loss: 2.8201377391815186
step 2683, loss: 3.179415225982666
step 2684, loss: 3.0816078186035156
step 2685, loss: 2.972381591796875
step 2686, loss: 2.5451667308807373
step 2687, loss: 3.079024314880371
step 2688, loss: 3.582674741744995
step 2689, loss: 3.533869981765747
step 2690, loss: 4.006491184234619
step 2691, loss: 4.034082412719727
step 2692, loss: 2.8612220287323
step 2693, loss: 3.7988994121551514
step 2694, loss: 3.932840347290039
step 2695, loss: 3.2868831157684326
step 2696, loss: 3.5994811058044434
step 2697, loss: 3.6976053714752197
step 2698, loss: 2.9442641735076904
step 2699, loss: 3.4118199348449707
step 2700, loss: 3.178908586502075
step 2701, loss: 3.0045433044433594
step 2702, loss: 3.108041524887085
step 2703, loss: 2.856513023376465
step 2704, loss: 3.8305022716522217
step 2705, loss: 4.2025861740112305
step 2706, loss: 3.5858044624328613
step 2707, loss: 3.039729356765747
step 2708, loss: 3.3907151222229004
step 2709, loss: 3.3577306270599365
step 2710, loss: 3.5581138134002686
step 2711, loss: 3.347336530685425
step 2712, loss: 3.335012435913086
step 2713, loss: 3.616553783416748
step 2714, loss: 4.268905162811279
step 2715, loss: 3.644606828689575
step 2716, loss: 3.4160873889923096
step 2717, loss: 2.9321439266204834
step 2718, loss: 3.1279702186584473
step 2719, loss: 2.605241537094116
step 2720, loss: 3.6312551498413086
step 2721, loss: 4.081850051879883
step 2722, loss: 3.5646257400512695
step 2723, loss: 2.6023244857788086
step 2724, loss: 2.9144468307495117
step 2725, loss: 3.080730438232422
step 2726, loss: 3.0464529991149902
step 2727, loss: 3.5706255435943604
step 2728, loss: 3.746220827102661
step 2729, loss: 4.206167221069336
step 2730, loss: 3.7621190547943115
step 2731, loss: 3.5779647827148438
step 2732, loss: 3.927107810974121
step 2733, loss: 3.3010942935943604
step 2734, loss: 2.659627914428711
step 2735, loss: 3.1492083072662354
step 2736, loss: 2.9178247451782227
step 2737, loss: 3.420417070388794
step 2738, loss: 2.597287893295288
step 2739, loss: 3.264819622039795
step 2740, loss: 3.6943447589874268
step 2741, loss: 2.491739511489868
step 2742, loss: 3.2204787731170654
step 2743, loss: 3.6401004791259766
step 2744, loss: 3.0201900005340576
step 2745, loss: 3.885929822921753
step 2746, loss: 4.550891399383545
step 2747, loss: 3.363112211227417
step 2748, loss: 3.2681102752685547
step 2749, loss: 3.128481864929199
step 2750, loss: 3.9940826892852783
step 2751, loss: 3.3306679725646973
step 2752, loss: 2.967273712158203
step 2753, loss: 3.802750825881958
step 2754, loss: 4.0297064781188965
step 2755, loss: 3.808685541152954
step 2756, loss: 3.388230085372925
step 2757, loss: 3.5194568634033203
step 2758, loss: 3.0768721103668213
step 2759, loss: 3.0718884468078613
step 2760, loss: 3.3708877563476562
step 2761, loss: 3.5958797931671143
step 2762, loss: 4.239077568054199
step 2763, loss: 4.275839328765869
step 2764, loss: 4.375787734985352
step 2765, loss: 2.9660189151763916
step 2766, loss: 2.458069086074829
step 2767, loss: 2.8836214542388916
step 2768, loss: 3.1918556690216064
step 2769, loss: 2.757988691329956
step 2770, loss: 3.5255680084228516
step 2771, loss: 3.8130416870117188
step 2772, loss: 3.4983439445495605
step 2773, loss: 3.3786587715148926
step 2774, loss: 2.9547977447509766
step 2775, loss: 2.912594795227051
step 2776, loss: 2.3598644733428955
step 2777, loss: 2.89626407623291
step 2778, loss: 2.8279802799224854
step 2779, loss: 2.7456448078155518
step 2780, loss: 3.4603002071380615
step 2781, loss: 3.457531213760376
step 2782, loss: 4.149905681610107
step 2783, loss: 2.5686144828796387
step 2784, loss: 2.597761631011963
step 2785, loss: 2.3558554649353027
step 2786, loss: 2.7506022453308105
step 2787, loss: 2.981680154800415
step 2788, loss: 3.151749610900879
step 2789, loss: 3.7505178451538086
step 2790, loss: 3.6584737300872803
step 2791, loss: 3.4674322605133057
step 2792, loss: 3.6910018920898438
step 2793, loss: 3.5627543926239014
step 2794, loss: 3.8495688438415527
step 2795, loss: 3.82743501663208
step 2796, loss: 3.4238553047180176
step 2797, loss: 2.9921483993530273
step 2798, loss: 2.7748360633850098
step 2799, loss: 2.7837278842926025
step 2800, loss: 2.1797285079956055
step 2801, loss: 2.6257925033569336
step 2802, loss: 3.2304229736328125
step 2803, loss: 2.313791275024414
step 2804, loss: 2.9151711463928223
step 2805, loss: 3.087733268737793
step 2806, loss: 3.467024564743042
step 2807, loss: 2.8995912075042725
step 2808, loss: 2.758061170578003
step 2809, loss: 3.7543675899505615
step 2810, loss: 3.799738645553589
step 2811, loss: 2.8503429889678955
step 2812, loss: 3.2809696197509766
step 2813, loss: 4.032731533050537
step 2814, loss: 3.4246602058410645
step 2815, loss: 4.136778354644775
step 2816, loss: 3.100904703140259
step 2817, loss: 2.9759881496429443
step 2818, loss: 2.563535213470459
step 2819, loss: 2.656130075454712
step 2820, loss: 2.641167640686035
step 2821, loss: 2.263157606124878
step 2822, loss: 3.024296760559082
step 2823, loss: 2.585064649581909
step 2824, loss: 2.3153505325317383
step 2825, loss: 2.4647319316864014
step 2826, loss: 2.7485032081604004
step 2827, loss: 3.2127084732055664
step 2828, loss: 3.040499448776245
step 2829, loss: 2.884218454360962
step 2830, loss: 2.4080705642700195
step 2831, loss: 2.065553665161133
step 2832, loss: 3.3276491165161133
step 2833, loss: 3.252669095993042
step 2834, loss: 2.907074213027954
step 2835, loss: 3.5328633785247803
step 2836, loss: 3.037231922149658
step 2837, loss: 2.42073392868042
step 2838, loss: 3.8855059146881104
step 2839, loss: 3.0430054664611816
step 2840, loss: 2.610600471496582
step 2841, loss: 2.8030202388763428
step 2842, loss: 3.0647575855255127
step 2843, loss: 2.907925844192505
step 2844, loss: 3.8198087215423584
step 2845, loss: 3.2845847606658936
step 2846, loss: 4.110085487365723
step 2847, loss: 3.372203826904297
step 2848, loss: 2.5285885334014893
step 2849, loss: 3.259345531463623
step 2850, loss: 3.805323839187622
step 2851, loss: 3.7177562713623047
step 2852, loss: 3.519932270050049
step 2853, loss: 2.818187952041626
step 2854, loss: 3.049677610397339
step 2855, loss: 2.786390781402588
step 2856, loss: 3.1197736263275146
step 2857, loss: 2.822983741760254
step 2858, loss: 3.240128755569458
step 2859, loss: 2.3959603309631348
step 2860, loss: 3.36868953704834
step 2861, loss: 2.436265707015991
step 2862, loss: 3.1783998012542725
step 2863, loss: 3.0233185291290283
step 2864, loss: 2.460993766784668
step 2865, loss: 3.2066264152526855
step 2866, loss: 2.522315740585327
step 2867, loss: 2.9295918941497803
step 2868, loss: 3.4169673919677734
step 2869, loss: 3.4503426551818848
step 2870, loss: 3.4410247802734375
step 2871, loss: 3.2017035484313965
step 2872, loss: 3.3461716175079346
step 2873, loss: 4.151407241821289
step 2874, loss: 3.3314340114593506
step 2875, loss: 3.1125526428222656
step 2876, loss: 2.9048562049865723
step 2877, loss: 2.491556167602539
step 2878, loss: 2.343043088912964
step 2879, loss: 3.256377935409546
step 2880, loss: 2.1744394302368164
step 2881, loss: 2.8445043563842773
step 2882, loss: 2.804489850997925
step 2883, loss: 2.719414234161377
step 2884, loss: 3.1544456481933594
step 2885, loss: 3.617830514907837
step 2886, loss: 3.2816672325134277
step 2887, loss: 3.547530174255371
step 2888, loss: 3.13747239112854
step 2889, loss: 3.4339754581451416
step 2890, loss: 3.0571508407592773
step 2891, loss: 4.0545806884765625
step 2892, loss: 2.6551616191864014
step 2893, loss: 2.72731351852417
step 2894, loss: 2.400524616241455
step 2895, loss: 2.5141282081604004
step 2896, loss: 2.150892972946167
step 2897, loss: 2.8343231678009033
step 2898, loss: 2.7367780208587646
step 2899, loss: 2.994389533996582
step 2900, loss: 3.7613816261291504
step 2901, loss: 3.8297412395477295
step 2902, loss: 3.5089194774627686
step 2903, loss: 3.976043462753296
step 2904, loss: 4.120464324951172
step 2905, loss: 3.51033616065979
step 2906, loss: 2.8898022174835205
step 2907, loss: 3.332610607147217
step 2908, loss: 3.0646016597747803
step 2909, loss: 2.5909993648529053
step 2910, loss: 2.410193681716919
step 2911, loss: 2.8623404502868652
step 2912, loss: 3.400277853012085
step 2913, loss: 3.2393290996551514
step 2914, loss: 3.0595035552978516
step 2915, loss: 3.437352180480957
step 2916, loss: 3.193849802017212
step 2917, loss: 3.6363484859466553
step 2918, loss: 2.67244291305542
step 2919, loss: 2.229315996170044
step 2920, loss: 2.42336106300354
step 2921, loss: 3.0010931491851807
step 2922, loss: 2.78023362159729
step 2923, loss: 3.1975882053375244
step 2924, loss: 2.8205063343048096
step 2925, loss: 2.8413777351379395
step 2926, loss: 2.720700979232788
step 2927, loss: 2.8541054725646973
step 2928, loss: 3.155916213989258
step 2929, loss: 3.0766026973724365
step 2930, loss: 3.2908105850219727
step 2931, loss: 3.119448184967041
step 2932, loss: 3.3045895099639893
step 2933, loss: 2.887636184692383
step 2934, loss: 2.8199245929718018
step 2935, loss: 2.3694655895233154
step 2936, loss: 3.1394569873809814
step 2937, loss: 3.3601737022399902
step 2938, loss: 3.4411401748657227
step 2939, loss: 3.5733911991119385
step 2940, loss: 3.883474588394165
step 2941, loss: 3.681004047393799
step 2942, loss: 2.8717422485351562
step 2943, loss: 3.2892041206359863
step 2944, loss: 3.0461556911468506
step 2945, loss: 3.189176559448242
step 2946, loss: 2.5690619945526123
step 2947, loss: 3.599801778793335
step 2948, loss: 2.4591362476348877
step 2949, loss: 3.638767957687378
step 2950, loss: 2.6303675174713135
step 2951, loss: 2.9155638217926025
step 2952, loss: 3.731870412826538
step 2953, loss: 3.117443561553955
step 2954, loss: 3.438255548477173
step 2955, loss: 3.215601921081543
step 2956, loss: 2.9965741634368896
step 2957, loss: 3.8061821460723877
step 2958, loss: 3.1838650703430176
step 2959, loss: 3.7674944400787354
step 2960, loss: 2.547194719314575
step 2961, loss: 3.5815486907958984
step 2962, loss: 3.098597288131714
step 2963, loss: 3.882279396057129
step 2964, loss: 4.278684139251709
step 2965, loss: 3.5715396404266357
step 2966, loss: 3.7971742153167725
step 2967, loss: 3.5509121417999268
step 2968, loss: 3.1864306926727295
step 2969, loss: 3.0780656337738037
step 2970, loss: 3.0532736778259277
step 2971, loss: 3.0654780864715576
step 2972, loss: 4.39638614654541
step 2973, loss: 3.8203442096710205
step 2974, loss: 3.3812503814697266
step 2975, loss: 3.3110146522521973
step 2976, loss: 3.5284359455108643
step 2977, loss: 3.602023124694824
step 2978, loss: 3.6773414611816406
step 2979, loss: 3.82609224319458
step 2980, loss: 3.0918161869049072
step 2981, loss: 3.030174970626831
step 2982, loss: 3.066887617111206
step 2983, loss: 2.8163106441497803
step 2984, loss: 2.5928378105163574
step 2985, loss: 3.613114595413208
step 2986, loss: 3.1254003047943115
step 2987, loss: 2.994246482849121
step 2988, loss: 3.3362152576446533
step 2989, loss: 2.9487497806549072
step 2990, loss: 2.931001901626587
step 2991, loss: 3.203962802886963
step 2992, loss: 2.86464524269104
step 2993, loss: 3.2859015464782715
step 2994, loss: 3.367738962173462
step 2995, loss: 3.256469964981079
step 2996, loss: 3.3234024047851562
step 2997, loss: 2.8609707355499268
step 2998, loss: 2.9903013706207275
step 2999, loss: 3.2188949584960938
step 3000, loss: 2.5086607933044434
step 3001, loss: 3.4556663036346436
step 3002, loss: 3.3190107345581055
step 3003, loss: 3.0053093433380127
step 3004, loss: 3.418630838394165
step 3005, loss: 2.2824087142944336
step 3006, loss: 3.1569554805755615
step 3007, loss: 3.2048678398132324
step 3008, loss: 3.7931840419769287
step 3009, loss: 3.9974679946899414
step 3010, loss: 3.833811044692993
step 3011, loss: 3.940633535385132
step 3012, loss: 2.9832496643066406
step 3013, loss: 3.6379969120025635
step 3014, loss: 3.39591646194458
step 3015, loss: 3.7220234870910645
step 3016, loss: 3.355581760406494
step 3017, loss: 3.1255054473876953
step 3018, loss: 2.7730555534362793
step 3019, loss: 3.0188472270965576
step 3020, loss: 2.8297243118286133
step 3021, loss: 2.888430595397949
step 3022, loss: 2.865002393722534
step 3023, loss: 3.5589635372161865
step 3024, loss: 3.615072011947632
step 3025, loss: 3.3684356212615967
step 3026, loss: 3.5989346504211426
step 3027, loss: 3.008998394012451
step 3028, loss: 2.6285927295684814
step 3029, loss: 3.0787014961242676
step 3030, loss: 3.557819128036499
step 3031, loss: 3.255216360092163
step 3032, loss: 2.7375714778900146
step 3033, loss: 2.7045953273773193
step 3034, loss: 2.3177592754364014
step 3035, loss: 2.7020914554595947
step 3036, loss: 2.0997071266174316
step 3037, loss: 2.280825138092041
step 3038, loss: 2.791226387023926
step 3039, loss: 2.441354751586914
step 3040, loss: 1.8490138053894043
Saved best model with loss: 1.8490 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.8490.pt
step 3041, loss: 1.5739890336990356
Saved best model with loss: 1.5740 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.5740.pt
step 3042, loss: 2.6344175338745117
step 3043, loss: 3.5956525802612305
step 3044, loss: 3.570481061935425
step 3045, loss: 2.706800937652588
step 3046, loss: 2.114136219024658
step 3047, loss: 1.7829043865203857
step 3048, loss: 2.5849123001098633
step 3049, loss: 3.410288095474243
step 3050, loss: 2.389927864074707
step 3051, loss: 3.645007610321045
step 3052, loss: 3.8980612754821777
step 3053, loss: 3.749614715576172
step 3054, loss: 3.0204765796661377
step 3055, loss: 2.6524417400360107
step 3056, loss: 3.034207344055176
step 3057, loss: 3.0025992393493652
step 3058, loss: 2.72528338432312
step 3059, loss: 3.6099050045013428
step 3060, loss: 3.4993956089019775
step 3061, loss: 3.2635703086853027
step 3062, loss: 3.4558358192443848
step 3063, loss: 3.039885997772217
step 3064, loss: 3.105419158935547
step 3065, loss: 3.1555898189544678
step 3066, loss: 3.1142868995666504
step 3067, loss: 2.9365007877349854
step 3068, loss: 3.002363443374634
step 3069, loss: 2.9660208225250244
step 3070, loss: 2.9294917583465576
step 3071, loss: 3.3615102767944336
step 3072, loss: 2.6721534729003906
step 3073, loss: 3.629432201385498
step 3074, loss: 2.8179306983947754
step 3075, loss: 3.7914741039276123
step 3076, loss: 3.2790474891662598
step 3077, loss: 3.095435380935669
step 3078, loss: 3.902808427810669
step 3079, loss: 2.3544559478759766
step 3080, loss: 2.5798535346984863
step 3081, loss: 3.2833704948425293
step 3082, loss: 2.8142056465148926
step 3083, loss: 2.7728590965270996
step 3084, loss: 3.2395808696746826
step 3085, loss: 3.2118849754333496
step 3086, loss: 3.2439897060394287
step 3087, loss: 2.499230146408081
step 3088, loss: 2.5878186225891113
step 3089, loss: 2.9678118228912354
step 3090, loss: 2.9032633304595947
step 3091, loss: 3.566758394241333
step 3092, loss: 2.956976890563965
step 3093, loss: 2.8875732421875
step 3094, loss: 2.8313345909118652
step 3095, loss: 3.582392930984497
step 3096, loss: 3.3479106426239014
step 3097, loss: 2.9578442573547363
step 3098, loss: 3.847616195678711
step 3099, loss: 3.3217527866363525
step 3100, loss: 3.103200674057007
step 3101, loss: 3.419527292251587
step 3102, loss: 2.366802930831909
step 3103, loss: 2.7844598293304443
step 3104, loss: 2.7178499698638916
step 3105, loss: 2.6429781913757324
step 3106, loss: 2.405524253845215
step 3107, loss: 3.6604340076446533
step 3108, loss: 3.0235981941223145
step 3109, loss: 2.8083484172821045
step 3110, loss: 2.2503550052642822
step 3111, loss: 2.6177732944488525
step 3112, loss: 3.3920578956604004
step 3113, loss: 3.1708168983459473
step 3114, loss: 2.850067377090454
step 3115, loss: 3.312837600708008
step 3116, loss: 2.714992046356201
step 3117, loss: 2.6631977558135986
step 3118, loss: 2.8561882972717285
step 3119, loss: 2.785717487335205
step 3120, loss: 2.70451021194458
step 3121, loss: 3.057464599609375
step 3122, loss: 2.6700620651245117
step 3123, loss: 3.261354684829712
step 3124, loss: 3.1899306774139404
step 3125, loss: 2.8203213214874268
step 3126, loss: 3.203843116760254
step 3127, loss: 3.4026052951812744
step 3128, loss: 2.969067096710205
step 3129, loss: 3.3571815490722656
step 3130, loss: 3.070605993270874
step 3131, loss: 2.475477457046509
step 3132, loss: 3.387521266937256
step 3133, loss: 2.7191154956817627
step 3134, loss: 3.2298989295959473
step 3135, loss: 3.6321804523468018
step 3136, loss: 3.3498594760894775
step 3137, loss: 2.6552891731262207
step 3138, loss: 3.001911163330078
step 3139, loss: 2.8929104804992676
step 3140, loss: 3.2116007804870605
step 3141, loss: 2.363787889480591
step 3142, loss: 3.2416930198669434
step 3143, loss: 3.4378316402435303
step 3144, loss: 3.251997947692871
step 3145, loss: 2.158534288406372
step 3146, loss: 2.8138058185577393
step 3147, loss: 3.5245277881622314
step 3148, loss: 3.1355197429656982
step 3149, loss: 2.765643358230591
step 3150, loss: 3.52183198928833
step 3151, loss: 3.32602858543396
step 3152, loss: 2.5788638591766357
step 3153, loss: 3.2549684047698975
step 3154, loss: 2.674621820449829
step 3155, loss: 3.1231253147125244
step 3156, loss: 3.436002731323242
step 3157, loss: 3.0387320518493652
step 3158, loss: 3.304521083831787
step 3159, loss: 2.6317622661590576
step 3160, loss: 2.853667974472046
step 3161, loss: 2.580413579940796
step 3162, loss: 2.726633071899414
step 3163, loss: 2.0748744010925293
step 3164, loss: 2.049905776977539
step 3165, loss: 3.1021363735198975
step 3166, loss: 3.5286507606506348
step 3167, loss: 2.313472270965576
step 3168, loss: 2.984346866607666
step 3169, loss: 3.561269760131836
step 3170, loss: 2.5602362155914307
step 3171, loss: 3.015038013458252
step 3172, loss: 3.232131004333496
step 3173, loss: 3.5110161304473877
step 3174, loss: 2.646085500717163
step 3175, loss: 2.7404940128326416
step 3176, loss: 2.6891865730285645
step 3177, loss: 2.743042469024658
step 3178, loss: 2.18874192237854
step 3179, loss: 2.3886871337890625
step 3180, loss: 2.3867416381835938
step 3181, loss: 2.3593671321868896
step 3182, loss: 2.874415874481201
step 3183, loss: 2.2734055519104004
step 3184, loss: 2.4181597232818604
step 3185, loss: 2.9026761054992676
step 3186, loss: 2.694614887237549
step 3187, loss: 3.222402334213257
step 3188, loss: 2.876652956008911
step 3189, loss: 2.418175220489502
step 3190, loss: 2.3333914279937744
step 3191, loss: 2.8147132396698
step 3192, loss: 3.473314046859741
step 3193, loss: 2.6842172145843506
step 3194, loss: 2.9531242847442627
step 3195, loss: 2.901869058609009
step 3196, loss: 2.624823570251465
step 3197, loss: 2.992476224899292
step 3198, loss: 2.717127561569214
step 3199, loss: 3.1199240684509277
step 3200, loss: 2.6805918216705322
step 3201, loss: 2.8379368782043457
step 3202, loss: 2.6555025577545166
step 3203, loss: 2.2435059547424316
step 3204, loss: 3.2552990913391113
step 3205, loss: 3.3384180068969727
step 3206, loss: 2.5254316329956055
step 3207, loss: 2.6180012226104736
step 3208, loss: 2.5753540992736816
step 3209, loss: 2.963080644607544
step 3210, loss: 2.0287749767303467
step 3211, loss: 2.879026412963867
step 3212, loss: 3.0032036304473877
step 3213, loss: 2.7939705848693848
step 3214, loss: 3.0378060340881348
step 3215, loss: 3.101949453353882
step 3216, loss: 3.3265233039855957
step 3217, loss: 2.761382818222046
step 3218, loss: 3.063816547393799
step 3219, loss: 2.881638288497925
step 3220, loss: 2.2841968536376953
step 3221, loss: 3.390184164047241
step 3222, loss: 3.4145374298095703
step 3223, loss: 3.201542854309082
step 3224, loss: 3.484036445617676
step 3225, loss: 2.8939859867095947
step 3226, loss: 4.119661331176758
step 3227, loss: 3.2077863216400146
step 3228, loss: 2.8032615184783936
step 3229, loss: 3.5445265769958496
step 3230, loss: 2.8641297817230225
step 3231, loss: 3.1236329078674316
step 3232, loss: 3.3909173011779785
step 3233, loss: 3.69393253326416
step 3234, loss: 3.009577989578247
step 3235, loss: 3.1501851081848145
step 3236, loss: 2.883251190185547
step 3237, loss: 2.880345106124878
step 3238, loss: 3.209528923034668
step 3239, loss: 2.899442672729492
step 3240, loss: 3.2905263900756836
step 3241, loss: 2.657503128051758
step 3242, loss: 2.935673236846924
step 3243, loss: 3.692572832107544
step 3244, loss: 3.3243613243103027
step 3245, loss: 3.5567190647125244
step 3246, loss: 3.604886293411255
step 3247, loss: 3.1733860969543457
step 3248, loss: 3.8497190475463867
step 3249, loss: 3.110822916030884
step 3250, loss: 2.7654874324798584
step 3251, loss: 2.9741146564483643
step 3252, loss: 3.271815776824951
step 3253, loss: 2.7985241413116455
step 3254, loss: 2.0052623748779297
step 3255, loss: 2.808539390563965
step 3256, loss: 2.7088663578033447
step 3257, loss: 2.7403953075408936
step 3258, loss: 2.7109663486480713
step 3259, loss: 3.1428472995758057
step 3260, loss: 3.0377039909362793
step 3261, loss: 3.314702272415161
step 3262, loss: 3.3399925231933594
step 3263, loss: 3.4871504306793213
step 3264, loss: 2.9100568294525146
step 3265, loss: 3.1179986000061035
step 3266, loss: 2.8874902725219727
step 3267, loss: 2.585423231124878
step 3268, loss: 3.003857374191284
step 3269, loss: 3.3650684356689453
step 3270, loss: 3.0989325046539307
step 3271, loss: 2.540404796600342
step 3272, loss: 3.5075299739837646
step 3273, loss: 2.4878077507019043
step 3274, loss: 2.849344253540039
step 3275, loss: 2.080946922302246
step 3276, loss: 2.6669228076934814
step 3277, loss: 2.5453786849975586
step 3278, loss: 2.0866987705230713
step 3279, loss: 2.2746968269348145
step 3280, loss: 3.7101449966430664
step 3281, loss: 3.9530370235443115
step 3282, loss: 2.8119657039642334
step 3283, loss: 2.6372427940368652
step 3284, loss: 3.0515425205230713
step 3285, loss: 3.6461217403411865
step 3286, loss: 2.9698410034179688
step 3287, loss: 3.378847122192383
step 3288, loss: 2.415207862854004
step 3289, loss: 3.3180627822875977
step 3290, loss: 2.71724796295166
step 3291, loss: 3.367105722427368
step 3292, loss: 3.14562726020813
step 3293, loss: 3.5557548999786377
step 3294, loss: 3.0802040100097656
step 3295, loss: 3.4836347103118896
step 3296, loss: 3.1517951488494873
step 3297, loss: 3.311650276184082
step 3298, loss: 2.9711618423461914
step 3299, loss: 2.84537672996521
step 3300, loss: 3.070103883743286
step 3301, loss: 2.7331299781799316
step 3302, loss: 2.905527114868164
step 3303, loss: 2.0550758838653564
step 3304, loss: 2.548391342163086
step 3305, loss: 3.145651340484619
step 3306, loss: 2.225407600402832
step 3307, loss: 3.0808658599853516
step 3308, loss: 2.581791877746582
step 3309, loss: 2.5738532543182373
step 3310, loss: 2.7008655071258545
step 3311, loss: 3.6224231719970703
step 3312, loss: 2.783458709716797
step 3313, loss: 2.2986135482788086
step 3314, loss: 2.701234817504883
step 3315, loss: 2.0043911933898926
step 3316, loss: 1.7752859592437744
step 3317, loss: 3.244356393814087
step 3318, loss: 2.6933772563934326
step 3319, loss: 3.0914204120635986
step 3320, loss: 3.4972574710845947
step 3321, loss: 3.759838342666626
step 3322, loss: 3.982459545135498
step 3323, loss: 2.9657323360443115
step 3324, loss: 1.901046633720398
step 3325, loss: 2.0357604026794434
step 3326, loss: 2.1265978813171387
step 3327, loss: 2.2911767959594727
step 3328, loss: 2.6839487552642822
step 3329, loss: 2.0979859828948975
step 3330, loss: 2.8909120559692383
step 3331, loss: 3.3985745906829834
step 3332, loss: 3.7802820205688477
step 3333, loss: 2.5046372413635254
step 3334, loss: 2.1936755180358887
step 3335, loss: 2.996426820755005
step 3336, loss: 3.0255789756774902
step 3337, loss: 2.599925994873047
step 3338, loss: 2.5350446701049805
step 3339, loss: 2.4864871501922607
step 3340, loss: 2.6310296058654785
step 3341, loss: 2.4972000122070312
step 3342, loss: 2.557847261428833
step 3343, loss: 3.4912607669830322
step 3344, loss: 2.995528221130371
step 3345, loss: 2.921825885772705
step 3346, loss: 2.937274217605591
step 3347, loss: 3.407125473022461
step 3348, loss: 3.188872814178467
step 3349, loss: 2.684732675552368
step 3350, loss: 3.157322645187378
step 3351, loss: 3.6179850101470947
step 3352, loss: 3.1755120754241943
step 3353, loss: 3.5835084915161133
step 3354, loss: 3.3548359870910645
step 3355, loss: 2.3299591541290283
step 3356, loss: 3.057000160217285
step 3357, loss: 2.9765594005584717
step 3358, loss: 2.956843614578247
step 3359, loss: 2.53490948677063
step 3360, loss: 2.3093223571777344
step 3361, loss: 2.3849823474884033
step 3362, loss: 2.530568838119507
step 3363, loss: 2.6832659244537354
step 3364, loss: 3.62992000579834
step 3365, loss: 3.395806074142456
step 3366, loss: 3.3600080013275146
step 3367, loss: 2.8440568447113037
step 3368, loss: 3.856600522994995
step 3369, loss: 3.101433753967285
step 3370, loss: 2.847336530685425
step 3371, loss: 2.1046040058135986
step 3372, loss: 2.9302103519439697
step 3373, loss: 2.8945889472961426
step 3374, loss: 3.145073175430298
step 3375, loss: 3.5812668800354004
step 3376, loss: 3.1655385494232178
step 3377, loss: 2.543689250946045
step 3378, loss: 2.5338387489318848
step 3379, loss: 3.007819175720215
step 3380, loss: 3.1457602977752686
step 3381, loss: 2.6134965419769287
step 3382, loss: 2.4409749507904053
step 3383, loss: 3.2022199630737305
step 3384, loss: 2.635247230529785
step 3385, loss: 3.3575334548950195
step 3386, loss: 3.7793219089508057
step 3387, loss: 3.7069520950317383
step 3388, loss: 2.7899551391601562
step 3389, loss: 3.2020342350006104
step 3390, loss: 3.113274335861206
step 3391, loss: 3.378772735595703
step 3392, loss: 3.763190269470215
step 3393, loss: 3.4952151775360107
step 3394, loss: 3.5183699131011963
step 3395, loss: 3.211872100830078
step 3396, loss: 3.602698564529419
step 3397, loss: 3.5809507369995117
step 3398, loss: 2.9026856422424316
step 3399, loss: 3.467597246170044
step 3400, loss: 3.6688342094421387
step 3401, loss: 3.3451731204986572
step 3402, loss: 3.621506690979004
step 3403, loss: 2.8712470531463623
step 3404, loss: 3.454463481903076
step 3405, loss: 3.390129327774048
step 3406, loss: 3.2537381649017334
step 3407, loss: 3.4517111778259277
step 3408, loss: 3.5705249309539795
step 3409, loss: 3.5725207328796387
step 3410, loss: 3.5120139122009277
step 3411, loss: 3.4575579166412354
step 3412, loss: 3.081073522567749
step 3413, loss: 3.422311305999756
step 3414, loss: 3.9147868156433105
step 3415, loss: 3.2994611263275146
step 3416, loss: 2.989238977432251
step 3417, loss: 3.1097936630249023
step 3418, loss: 2.7775661945343018
step 3419, loss: 3.100712299346924
step 3420, loss: 2.7631001472473145
step 3421, loss: 2.984890937805176
step 3422, loss: 3.5713248252868652
step 3423, loss: 3.0427284240722656
step 3424, loss: 3.535639524459839
step 3425, loss: 2.6988933086395264
step 3426, loss: 2.3035459518432617
step 3427, loss: 3.41979718208313
step 3428, loss: 3.6987154483795166
step 3429, loss: 3.446603775024414
step 3430, loss: 3.0407216548919678
step 3431, loss: 3.4208483695983887
step 3432, loss: 2.9886999130249023
step 3433, loss: 3.438023090362549
step 3434, loss: 2.507347822189331
step 3435, loss: 2.9345827102661133
step 3436, loss: 2.96864914894104
step 3437, loss: 3.3874075412750244
step 3438, loss: 3.1697938442230225
step 3439, loss: 3.667257785797119
step 3440, loss: 2.6972873210906982
step 3441, loss: 2.3462491035461426
step 3442, loss: 2.577284812927246
step 3443, loss: 3.435415506362915
step 3444, loss: 3.5410852432250977
step 3445, loss: 3.5770459175109863
step 3446, loss: 2.8555679321289062
step 3447, loss: 2.705876111984253
step 3448, loss: 2.800675630569458
step 3449, loss: 3.3721070289611816
step 3450, loss: 3.795593500137329
step 3451, loss: 3.637842893600464
step 3452, loss: 2.844289541244507
step 3453, loss: 3.058961868286133
step 3454, loss: 3.848390817642212
step 3455, loss: 3.848982810974121
step 3456, loss: 3.7158892154693604
step 3457, loss: 3.4351513385772705
step 3458, loss: 3.6055190563201904
step 3459, loss: 3.7104077339172363
step 3460, loss: 3.297745943069458
step 3461, loss: 2.99112606048584
step 3462, loss: 2.584615707397461
step 3463, loss: 3.518094778060913
step 3464, loss: 3.219222068786621
step 3465, loss: 3.5577046871185303
step 3466, loss: 4.008808135986328
step 3467, loss: 2.8298754692077637
step 3468, loss: 2.7276253700256348
step 3469, loss: 3.478766918182373
step 3470, loss: 3.194523334503174
step 3471, loss: 3.3564751148223877
step 3472, loss: 2.945875406265259
step 3473, loss: 3.6041531562805176
step 3474, loss: 2.8723106384277344
step 3475, loss: 3.0348501205444336
step 3476, loss: 2.7620627880096436
step 3477, loss: 3.238354444503784
step 3478, loss: 3.116321325302124
step 3479, loss: 2.911675453186035
step 3480, loss: 3.1536705493927
step 3481, loss: 2.84480619430542
step 3482, loss: 3.580878973007202
step 3483, loss: 3.177515983581543
step 3484, loss: 3.2513749599456787
step 3485, loss: 3.7350995540618896
step 3486, loss: 3.3750112056732178
step 3487, loss: 3.439389228820801
step 3488, loss: 3.2198033332824707
step 3489, loss: 2.6704485416412354
step 3490, loss: 2.6200788021087646
step 3491, loss: 2.9930834770202637
step 3492, loss: 3.2351768016815186
step 3493, loss: 3.037222146987915
step 3494, loss: 3.2219114303588867
step 3495, loss: 3.3862860202789307
step 3496, loss: 3.1082494258880615
step 3497, loss: 2.926320791244507
step 3498, loss: 3.216205358505249
step 3499, loss: 2.5523619651794434
step 3500, loss: 3.6863322257995605
step 3501, loss: 2.488676071166992
step 3502, loss: 2.542915105819702
step 3503, loss: 2.306450366973877
step 3504, loss: 3.2500388622283936
step 3505, loss: 2.725393295288086
step 3506, loss: 2.7848875522613525
step 3507, loss: 2.6251511573791504
step 3508, loss: 2.8674542903900146
step 3509, loss: 3.265155553817749
step 3510, loss: 3.28352952003479
step 3511, loss: 2.571626663208008
step 3512, loss: 3.349010705947876
step 3513, loss: 3.166384696960449
step 3514, loss: 2.7736740112304688
step 3515, loss: 3.2417047023773193
step 3516, loss: 2.972961902618408
step 3517, loss: 2.9016029834747314
step 3518, loss: 3.3514537811279297
step 3519, loss: 3.6483027935028076
step 3520, loss: 3.2450971603393555
step 3521, loss: 3.602200746536255
step 3522, loss: 3.1643879413604736
step 3523, loss: 2.7267537117004395
step 3524, loss: 3.1032612323760986
step 3525, loss: 3.9154212474823
step 3526, loss: 3.4216527938842773
step 3527, loss: 2.9031169414520264
step 3528, loss: 3.7043163776397705
step 3529, loss: 3.36277437210083
step 3530, loss: 3.434455394744873
step 3531, loss: 2.517624616622925
step 3532, loss: 2.9962213039398193
step 3533, loss: 2.9558181762695312
step 3534, loss: 3.1878185272216797
step 3535, loss: 3.647658348083496
step 3536, loss: 3.185959815979004
step 3537, loss: 3.406696319580078
step 3538, loss: 2.536626100540161
step 3539, loss: 3.3387386798858643
step 3540, loss: 3.577692747116089
step 3541, loss: 4.0777668952941895
step 3542, loss: 3.0432236194610596
step 3543, loss: 2.879321336746216
step 3544, loss: 3.2661514282226562
step 3545, loss: 2.9336016178131104
step 3546, loss: 2.749222993850708
step 3547, loss: 2.7374284267425537
step 3548, loss: 2.406122922897339
step 3549, loss: 2.4027507305145264
step 3550, loss: 3.617544651031494
step 3551, loss: 3.6391594409942627
step 3552, loss: 3.463550329208374
step 3553, loss: 3.0068106651306152
step 3554, loss: 3.479060173034668
step 3555, loss: 3.5175890922546387
step 3556, loss: 3.11259388923645
step 3557, loss: 3.4442951679229736
step 3558, loss: 3.065141201019287
step 3559, loss: 2.954224109649658
step 3560, loss: 2.5678999423980713
step 3561, loss: 3.6617889404296875
step 3562, loss: 3.5062520503997803
step 3563, loss: 2.9504332542419434
step 3564, loss: 2.437628746032715
step 3565, loss: 2.531783103942871
step 3566, loss: 3.03230619430542
step 3567, loss: 2.389263153076172
step 3568, loss: 2.9847233295440674
step 3569, loss: 2.909032106399536
step 3570, loss: 3.644319534301758
step 3571, loss: 3.4037485122680664
step 3572, loss: 3.206940174102783
step 3573, loss: 3.558913230895996
step 3574, loss: 3.092363119125366
step 3575, loss: 3.3363025188446045
step 3576, loss: 3.247464656829834
step 3577, loss: 3.471921920776367
step 3578, loss: 3.0078792572021484
step 3579, loss: 3.052729368209839
step 3580, loss: 3.118269443511963
step 3581, loss: 2.6678004264831543
step 3582, loss: 2.93184232711792
step 3583, loss: 2.4259121417999268
step 3584, loss: 3.4744176864624023
step 3585, loss: 2.5052523612976074
step 3586, loss: 3.1894876956939697
step 3587, loss: 3.1372733116149902
step 3588, loss: 2.9189329147338867
step 3589, loss: 2.806663751602173
step 3590, loss: 3.214775800704956
step 3591, loss: 3.2440128326416016
step 3592, loss: 3.807386636734009
step 3593, loss: 3.60072922706604
step 3594, loss: 3.0035855770111084
step 3595, loss: 2.801570415496826
step 3596, loss: 3.742123603820801
step 3597, loss: 3.1606040000915527
step 3598, loss: 3.091362953186035
step 3599, loss: 2.345828056335449
step 3600, loss: 3.318016529083252
step 3601, loss: 3.59648060798645
step 3602, loss: 3.2139973640441895
step 3603, loss: 2.9939074516296387
step 3604, loss: 2.7854745388031006
step 3605, loss: 3.550137996673584
step 3606, loss: 2.0997283458709717
step 3607, loss: 3.2346103191375732
step 3608, loss: 2.9963274002075195
step 3609, loss: 3.5269899368286133
step 3610, loss: 2.1723339557647705
step 3611, loss: 1.8521677255630493
step 3612, loss: 2.575538396835327
step 3613, loss: 3.260143995285034
step 3614, loss: 3.7231943607330322
step 3615, loss: 3.695805788040161
step 3616, loss: 3.528461456298828
step 3617, loss: 3.6040523052215576
step 3618, loss: 2.971609354019165
step 3619, loss: 3.536001205444336
step 3620, loss: 2.976269006729126
step 3621, loss: 3.143941879272461
step 3622, loss: 2.900831699371338
step 3623, loss: 3.3293819427490234
step 3624, loss: 3.0972750186920166
step 3625, loss: 3.31380033493042
step 3626, loss: 3.2216854095458984
step 3627, loss: 3.665130376815796
step 3628, loss: 2.9572954177856445
step 3629, loss: 2.7613701820373535
step 3630, loss: 2.232740640640259
step 3631, loss: 2.0059216022491455
step 3632, loss: 2.2033329010009766
step 3633, loss: 2.012716293334961
step 3634, loss: 2.1268255710601807
step 3635, loss: 2.752228021621704
step 3636, loss: 2.4507062435150146
step 3637, loss: 2.671393871307373
step 3638, loss: 2.8873322010040283
step 3639, loss: 3.0376882553100586
step 3640, loss: 2.7226102352142334
step 3641, loss: 2.1279261112213135
step 3642, loss: 2.1279876232147217
step 3643, loss: 2.3650832176208496
step 3644, loss: 3.1478188037872314
step 3645, loss: 3.636502742767334
step 3646, loss: 2.5445168018341064
step 3647, loss: 3.602856397628784
step 3648, loss: 2.4298901557922363
step 3649, loss: 3.0022737979888916
step 3650, loss: 2.945373058319092
step 3651, loss: 2.9842562675476074
step 3652, loss: 3.165126085281372
step 3653, loss: 2.177915573120117
step 3654, loss: 3.048530101776123
step 3655, loss: 2.7604591846466064
step 3656, loss: 3.1319971084594727
step 3657, loss: 3.446585178375244
step 3658, loss: 3.8887135982513428
step 3659, loss: 3.525095224380493
step 3660, loss: 3.80407977104187
step 3661, loss: 3.6798431873321533
step 3662, loss: 3.1837949752807617
step 3663, loss: 3.6292243003845215
step 3664, loss: 3.110598087310791
step 3665, loss: 2.9673938751220703
step 3666, loss: 2.714656114578247
step 3667, loss: 3.4365830421447754
step 3668, loss: 2.9077975749969482
step 3669, loss: 2.868959426879883
step 3670, loss: 2.8499865531921387
step 3671, loss: 3.3447160720825195
step 3672, loss: 3.212815046310425
step 3673, loss: 3.5134477615356445
step 3674, loss: 3.0395877361297607
step 3675, loss: 3.188260316848755
step 3676, loss: 2.999640941619873
step 3677, loss: 2.825928211212158
step 3678, loss: 2.522935628890991
step 3679, loss: 2.306570529937744
step 3680, loss: 2.818419933319092
step 3681, loss: 3.088313579559326
step 3682, loss: 2.9871482849121094
step 3683, loss: 3.5240705013275146
step 3684, loss: 3.2429606914520264
step 3685, loss: 3.6709234714508057
step 3686, loss: 3.2377994060516357
step 3687, loss: 3.810349225997925
step 3688, loss: 3.010895252227783
step 3689, loss: 3.321261405944824
step 3690, loss: 2.7150063514709473
step 3691, loss: 2.830773115158081
step 3692, loss: 3.6248888969421387
step 3693, loss: 3.2520864009857178
step 3694, loss: 2.916841745376587
step 3695, loss: 2.8650333881378174
step 3696, loss: 3.6066980361938477
step 3697, loss: 3.1820895671844482
step 3698, loss: 3.704205274581909
step 3699, loss: 2.990694522857666
step 3700, loss: 3.4571893215179443
step 3701, loss: 4.117811679840088
step 3702, loss: 3.9901304244995117
step 3703, loss: 3.5373518466949463
step 3704, loss: 2.816089153289795
step 3705, loss: 2.946794033050537
step 3706, loss: 3.1687612533569336
step 3707, loss: 2.959090232849121
step 3708, loss: 3.5557754039764404
step 3709, loss: 3.1378493309020996
step 3710, loss: 2.503915786743164
step 3711, loss: 2.871394395828247
step 3712, loss: 3.5409770011901855
step 3713, loss: 3.4369258880615234
step 3714, loss: 3.3238110542297363
step 3715, loss: 3.0605430603027344
step 3716, loss: 2.748448610305786
step 3717, loss: 2.9374492168426514
step 3718, loss: 2.6986303329467773
step 3719, loss: 3.8219716548919678
step 3720, loss: 3.2199103832244873
step 3721, loss: 2.8524224758148193
step 3722, loss: 3.515022039413452
step 3723, loss: 3.464235782623291
step 3724, loss: 3.199979543685913
step 3725, loss: 3.287022113800049
step 3726, loss: 3.3117363452911377
step 3727, loss: 2.3528971672058105
step 3728, loss: 3.66683292388916
step 3729, loss: 3.903031826019287
step 3730, loss: 3.7555830478668213
step 3731, loss: 3.5489892959594727
step 3732, loss: 3.028137445449829
step 3733, loss: 3.324154853820801
step 3734, loss: 3.4960238933563232
step 3735, loss: 3.4435346126556396
step 3736, loss: 3.297272205352783
step 3737, loss: 2.911768913269043
step 3738, loss: 2.8181371688842773
step 3739, loss: 3.6147286891937256
step 3740, loss: 2.781625747680664
step 3741, loss: 3.4336743354797363
step 3742, loss: 2.986609935760498
step 3743, loss: 3.7439260482788086
step 3744, loss: 3.172201633453369
step 3745, loss: 2.8190009593963623
step 3746, loss: 2.8154172897338867
step 3747, loss: 2.855820655822754
step 3748, loss: 2.5720250606536865
step 3749, loss: 2.8770525455474854
step 3750, loss: 3.9876770973205566
step 3751, loss: 2.90677547454834
step 3752, loss: 3.7178432941436768
step 3753, loss: 3.2116734981536865
step 3754, loss: 3.360926866531372
step 3755, loss: 3.191328287124634
step 3756, loss: 3.515937566757202
step 3757, loss: 3.74863338470459
step 3758, loss: 2.9196274280548096
step 3759, loss: 2.67010235786438
step 3760, loss: 3.336120128631592
step 3761, loss: 2.789996385574341
step 3762, loss: 3.1034419536590576
step 3763, loss: 3.0157785415649414
step 3764, loss: 3.2810781002044678
step 3765, loss: 3.311492919921875
step 3766, loss: 3.3595898151397705
step 3767, loss: 2.66499924659729
step 3768, loss: 2.977219343185425
step 3769, loss: 2.6229238510131836
step 3770, loss: 3.092373847961426
step 3771, loss: 2.678830146789551
step 3772, loss: 3.1352810859680176
step 3773, loss: 2.3121235370635986
step 3774, loss: 2.857980489730835
step 3775, loss: 2.9345831871032715
step 3776, loss: 3.6994218826293945
step 3777, loss: 3.832794189453125
step 3778, loss: 4.5330424308776855
step 3779, loss: 3.735281229019165
step 3780, loss: 2.7216720581054688
step 3781, loss: 3.5250167846679688
step 3782, loss: 3.2210936546325684
step 3783, loss: 3.5860135555267334
step 3784, loss: 2.788296937942505
step 3785, loss: 3.232844114303589
step 3786, loss: 2.543384552001953
step 3787, loss: 3.0073418617248535
step 3788, loss: 3.4356307983398438
step 3789, loss: 3.827852249145508
step 3790, loss: 3.8104755878448486
step 3791, loss: 3.447495937347412
step 3792, loss: 2.5224111080169678
step 3793, loss: 3.008164167404175
step 3794, loss: 2.7009036540985107
step 3795, loss: 2.9689040184020996
step 3796, loss: 2.667186975479126
step 3797, loss: 2.6063690185546875
step 3798, loss: 2.590791702270508
step 3799, loss: 2.937762498855591
step 3800, loss: 2.4124958515167236
step 3801, loss: 2.7288472652435303
step 3802, loss: 3.441011428833008
step 3803, loss: 3.348161458969116
step 3804, loss: 3.242321252822876
step 3805, loss: 2.845142126083374
step 3806, loss: 3.0144317150115967
step 3807, loss: 2.8885934352874756
step 3808, loss: 3.691910982131958
step 3809, loss: 2.6792209148406982
step 3810, loss: 3.8503987789154053
step 3811, loss: 3.1807892322540283
step 3812, loss: 2.762526273727417
step 3813, loss: 2.7100536823272705
step 3814, loss: 3.0642762184143066
step 3815, loss: 2.532644748687744
step 3816, loss: 3.080989122390747
step 3817, loss: 2.613555669784546
step 3818, loss: 2.96846342086792
step 3819, loss: 3.758146047592163
step 3820, loss: 2.964952230453491
step 3821, loss: 3.557817220687866
step 3822, loss: 2.875563621520996
step 3823, loss: 2.869014024734497
step 3824, loss: 3.5290911197662354
step 3825, loss: 3.087942123413086
step 3826, loss: 2.6311326026916504
step 3827, loss: 2.759192943572998
step 3828, loss: 2.797654867172241
step 3829, loss: 2.8999998569488525
step 3830, loss: 2.8755455017089844
step 3831, loss: 2.9660415649414062
step 3832, loss: 2.6700844764709473
step 3833, loss: 2.890467643737793
step 3834, loss: 3.2892704010009766
step 3835, loss: 3.2340588569641113
step 3836, loss: 2.7596638202667236
step 3837, loss: 2.60387921333313
step 3838, loss: 2.5336239337921143
step 3839, loss: 2.900477170944214
step 3840, loss: 3.8351423740386963
step 3841, loss: 3.2778491973876953
step 3842, loss: 2.8025412559509277
step 3843, loss: 3.2789227962493896
step 3844, loss: 3.7759711742401123
step 3845, loss: 3.85579776763916
step 3846, loss: 3.2093567848205566
step 3847, loss: 2.699850082397461
step 3848, loss: 2.649167060852051
step 3849, loss: 3.4109280109405518
step 3850, loss: 3.1910529136657715
step 3851, loss: 2.371591567993164
step 3852, loss: 3.380967140197754
step 3853, loss: 3.046780824661255
step 3854, loss: 3.27841854095459
step 3855, loss: 3.1356594562530518
step 3856, loss: 3.085118532180786
step 3857, loss: 2.820671796798706
step 3858, loss: 3.0963385105133057
step 3859, loss: 2.7406694889068604
step 3860, loss: 2.552544355392456
step 3861, loss: 2.628347158432007
step 3862, loss: 2.9350357055664062
step 3863, loss: 3.900244951248169
step 3864, loss: 3.1743552684783936
step 3865, loss: 2.5189712047576904
step 3866, loss: 2.6389241218566895
step 3867, loss: 3.0987651348114014
step 3868, loss: 2.369575262069702
step 3869, loss: 2.6302731037139893
step 3870, loss: 2.9754700660705566
step 3871, loss: 3.2236554622650146
step 3872, loss: 3.1749470233917236
step 3873, loss: 3.2709338665008545
step 3874, loss: 3.3924026489257812
step 3875, loss: 3.090165853500366
step 3876, loss: 2.8572006225585938
step 3877, loss: 3.2764945030212402
step 3878, loss: 2.916576385498047
step 3879, loss: 2.8556060791015625
step 3880, loss: 3.0385923385620117
step 3881, loss: 2.8962221145629883
step 3882, loss: 2.919088840484619
step 3883, loss: 2.8313915729522705
step 3884, loss: 3.009094715118408
step 3885, loss: 3.467057466506958
step 3886, loss: 2.5059900283813477
step 3887, loss: 2.6603260040283203
step 3888, loss: 3.058286666870117
step 3889, loss: 2.1443490982055664
step 3890, loss: 2.575669765472412
step 3891, loss: 2.3303656578063965
step 3892, loss: 3.051020622253418
step 3893, loss: 2.70065975189209
step 3894, loss: 2.535264015197754
step 3895, loss: 2.7445521354675293
step 3896, loss: 3.3586037158966064
step 3897, loss: 2.976585865020752
step 3898, loss: 2.8713645935058594
step 3899, loss: 2.8848068714141846
step 3900, loss: 3.2782480716705322
step 3901, loss: 2.7061688899993896
step 3902, loss: 3.234682083129883
step 3903, loss: 3.429288864135742
step 3904, loss: 2.572173595428467
step 3905, loss: 2.7702202796936035
step 3906, loss: 3.0564732551574707
step 3907, loss: 2.597022533416748
step 3908, loss: 2.9909749031066895
step 3909, loss: 2.4550106525421143
step 3910, loss: 2.9456756114959717
step 3911, loss: 1.970159888267517
step 3912, loss: 2.6384735107421875
step 3913, loss: 2.7470476627349854
step 3914, loss: 2.7153921127319336
step 3915, loss: 3.5275895595550537
step 3916, loss: 3.629054307937622
step 3917, loss: 3.1655373573303223
step 3918, loss: 2.963937997817993
step 3919, loss: 4.0108771324157715
step 3920, loss: 3.4192469120025635
step 3921, loss: 3.0452404022216797
step 3922, loss: 2.633451223373413
step 3923, loss: 2.6218650341033936
step 3924, loss: 2.7059967517852783
step 3925, loss: 2.711510419845581
step 3926, loss: 2.3376314640045166
step 3927, loss: 3.0237772464752197
step 3928, loss: 2.6977362632751465
step 3929, loss: 2.850517749786377
step 3930, loss: 3.509812355041504
step 3931, loss: 3.350507974624634
step 3932, loss: 3.406813144683838
step 3933, loss: 2.8105573654174805
step 3934, loss: 2.753709316253662
step 3935, loss: 2.6524505615234375
step 3936, loss: 2.933077335357666
step 3937, loss: 3.1357057094573975
step 3938, loss: 2.5761783123016357
step 3939, loss: 2.0844247341156006
step 3940, loss: 2.549055576324463
step 3941, loss: 2.9342997074127197
step 3942, loss: 2.747105836868286
step 3943, loss: 2.7783384323120117
step 3944, loss: 3.1831867694854736
step 3945, loss: 3.470656156539917
step 3946, loss: 3.2615468502044678
step 3947, loss: 3.0296335220336914
step 3948, loss: 2.9006917476654053
step 3949, loss: 2.6119227409362793
step 3950, loss: 3.4080166816711426
step 3951, loss: 2.7045114040374756
step 3952, loss: 3.2074103355407715
step 3953, loss: 3.480593204498291
step 3954, loss: 3.145822763442993
step 3955, loss: 2.381700277328491
step 3956, loss: 3.586183786392212
step 3957, loss: 3.6039795875549316
step 3958, loss: 3.1256773471832275
step 3959, loss: 3.1255321502685547
step 3960, loss: 2.944915533065796
step 3961, loss: 3.127387762069702
step 3962, loss: 3.1157948970794678
step 3963, loss: 3.0441858768463135
step 3964, loss: 3.0157089233398438
step 3965, loss: 3.7407066822052
step 3966, loss: 3.2464792728424072
step 3967, loss: 3.536228656768799
step 3968, loss: 2.911681890487671
step 3969, loss: 3.4052295684814453
step 3970, loss: 3.1543545722961426
step 3971, loss: 2.9888460636138916
step 3972, loss: 3.462169647216797
step 3973, loss: 3.3500328063964844
step 3974, loss: 3.5013256072998047
step 3975, loss: 3.813908576965332
step 3976, loss: 2.7706334590911865
step 3977, loss: 1.9904857873916626
step 3978, loss: 2.731720447540283
step 3979, loss: 3.170905828475952
step 3980, loss: 3.083804130554199
step 3981, loss: 3.218374490737915
step 3982, loss: 3.368060350418091
step 3983, loss: 2.8710789680480957
step 3984, loss: 2.66510009765625
step 3985, loss: 2.8577325344085693
step 3986, loss: 3.083322763442993
step 3987, loss: 3.4837753772735596
step 3988, loss: 2.687786102294922
step 3989, loss: 3.537414789199829
step 3990, loss: 3.2980754375457764
step 3991, loss: 3.157395362854004
step 3992, loss: 2.6914238929748535
step 3993, loss: 2.751507043838501
step 3994, loss: 3.1281206607818604
step 3995, loss: 3.496861696243286
step 3996, loss: 3.077899932861328
step 3997, loss: 2.842954397201538
step 3998, loss: 3.1137948036193848
step 3999, loss: 3.3719232082366943
step 4000, loss: 2.8809680938720703
step 4001, loss: 3.641066789627075
step 4002, loss: 2.9564671516418457
step 4003, loss: 2.85168719291687
step 4004, loss: 2.8585526943206787
step 4005, loss: 2.3228538036346436
step 4006, loss: 2.7795846462249756
step 4007, loss: 2.83674955368042
step 4008, loss: 2.4427528381347656
step 4009, loss: 2.7154200077056885
step 4010, loss: 3.355506420135498
step 4011, loss: 2.049044132232666
step 4012, loss: 2.2455408573150635
step 4013, loss: 2.229429244995117
step 4014, loss: 2.7406883239746094
step 4015, loss: 2.602442502975464
step 4016, loss: 1.9815800189971924
step 4017, loss: 2.6310172080993652
step 4018, loss: 2.793665885925293
step 4019, loss: 2.1141560077667236
step 4020, loss: 2.0022668838500977
step 4021, loss: 3.2012617588043213
step 4022, loss: 2.1763479709625244
step 4023, loss: 3.6436619758605957
step 4024, loss: 3.2655484676361084
step 4025, loss: 2.0285558700561523
step 4026, loss: 3.004080057144165
step 4027, loss: 2.474395513534546
step 4028, loss: 2.561854124069214
step 4029, loss: 2.4334168434143066
step 4030, loss: 3.390042304992676
step 4031, loss: 3.4563589096069336
step 4032, loss: 3.0531723499298096
step 4033, loss: 3.1396124362945557
step 4034, loss: 2.75266695022583
step 4035, loss: 2.6716132164001465
step 4036, loss: 2.886803150177002
step 4037, loss: 3.0136876106262207
step 4038, loss: 3.260793924331665
step 4039, loss: 2.7398695945739746
step 4040, loss: 2.8820152282714844
step 4041, loss: 3.3648874759674072
step 4042, loss: 3.484402656555176
step 4043, loss: 3.0650296211242676
step 4044, loss: 3.3899576663970947
step 4045, loss: 3.052746534347534
step 4046, loss: 3.3328075408935547
step 4047, loss: 2.9143924713134766
step 4048, loss: 3.7270894050598145
step 4049, loss: 3.2289674282073975
step 4050, loss: 3.1865475177764893
step 4051, loss: 3.1030588150024414
step 4052, loss: 3.0730881690979004
step 4053, loss: 3.6701529026031494
step 4054, loss: 3.2212510108947754
step 4055, loss: 3.3954718112945557
step 4056, loss: 3.2309412956237793
step 4057, loss: 3.54154372215271
step 4058, loss: 2.6144111156463623
step 4059, loss: 2.6898837089538574
step 4060, loss: 2.9873178005218506
step 4061, loss: 3.5657107830047607
step 4062, loss: 3.1556057929992676
step 4063, loss: 3.31241774559021
step 4064, loss: 2.9177045822143555
step 4065, loss: 3.306309700012207
step 4066, loss: 3.0933310985565186
step 4067, loss: 3.4831719398498535
step 4068, loss: 2.9014604091644287
step 4069, loss: 3.17008113861084
step 4070, loss: 3.554716110229492
step 4071, loss: 3.586132764816284
step 4072, loss: 3.460667371749878
step 4073, loss: 3.024557590484619
step 4074, loss: 3.0836410522460938
step 4075, loss: 3.0362493991851807
step 4076, loss: 3.4544315338134766
step 4077, loss: 3.056556224822998
step 4078, loss: 3.2347443103790283
step 4079, loss: 2.474604845046997
step 4080, loss: 2.537330389022827
step 4081, loss: 3.4848852157592773
step 4082, loss: 3.6329705715179443
step 4083, loss: 3.529141902923584
step 4084, loss: 3.4074695110321045
step 4085, loss: 3.0326452255249023
step 4086, loss: 3.047905206680298
step 4087, loss: 2.3032991886138916
step 4088, loss: 2.7565224170684814
step 4089, loss: 2.7719709873199463
step 4090, loss: 2.064462661743164
step 4091, loss: 2.467798948287964
step 4092, loss: 2.1369645595550537
step 4093, loss: 2.949838399887085
step 4094, loss: 2.3862051963806152
step 4095, loss: 3.222805976867676
step 4096, loss: 3.1942391395568848
step 4097, loss: 3.5815298557281494
step 4098, loss: 3.0525400638580322
step 4099, loss: 2.860236883163452
step 4100, loss: 3.043874740600586
step 4101, loss: 3.4180586338043213
step 4102, loss: 2.9867656230926514
step 4103, loss: 3.0045971870422363
step 4104, loss: 3.099466323852539
step 4105, loss: 2.7376744747161865
step 4106, loss: 3.27081561088562
step 4107, loss: 3.41501784324646
step 4108, loss: 3.073451519012451
step 4109, loss: 3.160977840423584
step 4110, loss: 3.6675708293914795
step 4111, loss: 3.615243911743164
step 4112, loss: 3.0791661739349365
step 4113, loss: 3.10886812210083
step 4114, loss: 2.9342939853668213
step 4115, loss: 3.356077194213867
step 4116, loss: 2.7710719108581543
step 4117, loss: 3.1247341632843018
step 4118, loss: 2.8693439960479736
step 4119, loss: 2.7570552825927734
step 4120, loss: 3.537365674972534
step 4121, loss: 3.289280891418457
step 4122, loss: 2.9231348037719727
step 4123, loss: 3.146991014480591
step 4124, loss: 3.6595726013183594
step 4125, loss: 2.9892988204956055
step 4126, loss: 2.945199489593506
step 4127, loss: 2.7905735969543457
step 4128, loss: 3.2248876094818115
step 4129, loss: 3.159817695617676
step 4130, loss: 2.751781463623047
step 4131, loss: 3.198113203048706
step 4132, loss: 3.2149271965026855
step 4133, loss: 3.175553560256958
step 4134, loss: 2.9951088428497314
step 4135, loss: 3.411142349243164
step 4136, loss: 3.285184383392334
step 4137, loss: 2.180379867553711
step 4138, loss: 2.9604082107543945
step 4139, loss: 2.2847020626068115
step 4140, loss: 3.099409818649292
step 4141, loss: 2.370450735092163
step 4142, loss: 2.7292325496673584
step 4143, loss: 2.2337746620178223
step 4144, loss: 1.9880459308624268
step 4145, loss: 2.00663161277771
step 4146, loss: 2.025113821029663
step 4147, loss: 1.7675992250442505
step 4148, loss: 2.2800703048706055
step 4149, loss: 2.2090704441070557
step 4150, loss: 2.365501880645752
step 4151, loss: 1.9318835735321045
step 4152, loss: 2.1605637073516846
step 4153, loss: 2.4525625705718994
step 4154, loss: 2.005002975463867
step 4155, loss: 2.788008213043213
step 4156, loss: 3.4714715480804443
step 4157, loss: 3.2700159549713135
step 4158, loss: 3.485208034515381
step 4159, loss: 3.34619402885437
step 4160, loss: 3.355468273162842
step 4161, loss: 3.0298147201538086
step 4162, loss: 2.960690498352051
step 4163, loss: 3.3602747917175293
step 4164, loss: 3.149345636367798
step 4165, loss: 3.2613842487335205
step 4166, loss: 2.722900152206421
step 4167, loss: 2.6710286140441895
step 4168, loss: 2.6100642681121826
step 4169, loss: 4.12301778793335
step 4170, loss: 2.8289973735809326
step 4171, loss: 3.595766067504883
step 4172, loss: 3.42875075340271
step 4173, loss: 2.512871742248535
step 4174, loss: 2.9197795391082764
step 4175, loss: 3.1642467975616455
step 4176, loss: 2.7335448265075684
step 4177, loss: 3.384946584701538
step 4178, loss: 3.1021978855133057
step 4179, loss: 2.466543197631836
step 4180, loss: 2.9563121795654297
step 4181, loss: 3.4725210666656494
step 4182, loss: 2.884079694747925
step 4183, loss: 3.064180374145508
step 4184, loss: 2.478606939315796
step 4185, loss: 2.672100067138672
step 4186, loss: 2.7293124198913574
step 4187, loss: 2.890385627746582
step 4188, loss: 3.312795400619507
step 4189, loss: 2.609482526779175
step 4190, loss: 2.479567050933838
step 4191, loss: 2.6870839595794678
step 4192, loss: 2.7375710010528564
step 4193, loss: 2.9941248893737793
step 4194, loss: 3.0615944862365723
step 4195, loss: 3.020359754562378
step 4196, loss: 3.2116811275482178
step 4197, loss: 3.165956735610962
step 4198, loss: 2.631601095199585
step 4199, loss: 2.4319570064544678
step 4200, loss: 2.702932357788086
step 4201, loss: 3.140266180038452
step 4202, loss: 2.966902256011963
step 4203, loss: 3.2165544033050537
step 4204, loss: 2.218390941619873
step 4205, loss: 3.159374475479126
step 4206, loss: 3.544951915740967
step 4207, loss: 2.9744417667388916
step 4208, loss: 2.812927722930908
step 4209, loss: 3.188079357147217
step 4210, loss: 2.583118200302124
step 4211, loss: 2.664034366607666
step 4212, loss: 3.2002313137054443
step 4213, loss: 2.693049430847168
step 4214, loss: 2.195624589920044
step 4215, loss: 2.9795589447021484
step 4216, loss: 3.327364921569824
step 4217, loss: 2.965967893600464
step 4218, loss: 3.1354923248291016
step 4219, loss: 2.72587251663208
step 4220, loss: 1.9977513551712036
step 4221, loss: 2.617474317550659
step 4222, loss: 3.5296268463134766
step 4223, loss: 3.3588311672210693
step 4224, loss: 3.1610488891601562
step 4225, loss: 2.974268674850464
step 4226, loss: 2.972174882888794
step 4227, loss: 2.6628315448760986
step 4228, loss: 3.0863406658172607
step 4229, loss: 2.6163158416748047
step 4230, loss: 3.2759058475494385
step 4231, loss: 2.6027731895446777
step 4232, loss: 3.027578830718994
step 4233, loss: 2.314038038253784
step 4234, loss: 2.852564573287964
step 4235, loss: 2.6513607501983643
step 4236, loss: 2.574457883834839
step 4237, loss: 2.5735549926757812
step 4238, loss: 2.943248748779297
step 4239, loss: 2.7814769744873047
step 4240, loss: 3.089170455932617
step 4241, loss: 2.3934340476989746
step 4242, loss: 3.209486722946167
step 4243, loss: 2.568147659301758
step 4244, loss: 2.021873950958252
step 4245, loss: 2.9203941822052
step 4246, loss: 3.0664215087890625
step 4247, loss: 2.986346483230591
step 4248, loss: 2.5246365070343018
step 4249, loss: 2.4824843406677246
step 4250, loss: 2.920308828353882
step 4251, loss: 2.4333527088165283
step 4252, loss: 2.7050063610076904
step 4253, loss: 2.6618940830230713
step 4254, loss: 3.283936023712158
step 4255, loss: 2.566711902618408
step 4256, loss: 2.3338112831115723
step 4257, loss: 3.4175336360931396
step 4258, loss: 3.408355951309204
step 4259, loss: 2.7389700412750244
step 4260, loss: 2.6008243560791016
step 4261, loss: 2.882108211517334
step 4262, loss: 3.334141254425049
step 4263, loss: 3.3951408863067627
step 4264, loss: 2.9431960582733154
step 4265, loss: 2.848694324493408
step 4266, loss: 2.7300100326538086
step 4267, loss: 3.121875286102295
step 4268, loss: 2.767218589782715
step 4269, loss: 3.5299017429351807
step 4270, loss: 3.703017234802246
step 4271, loss: 3.4864213466644287
step 4272, loss: 3.4576151371002197
step 4273, loss: 3.135327100753784
step 4274, loss: 2.415670394897461
step 4275, loss: 2.663501501083374
step 4276, loss: 3.111895799636841
step 4277, loss: 2.3313851356506348
step 4278, loss: 2.7580034732818604
step 4279, loss: 2.7804219722747803
step 4280, loss: 2.3240952491760254
step 4281, loss: 2.872114658355713
step 4282, loss: 2.005948781967163
step 4283, loss: 2.529071092605591
step 4284, loss: 3.0755808353424072
step 4285, loss: 2.475023031234741
step 4286, loss: 2.6131441593170166
step 4287, loss: 2.4897069931030273
step 4288, loss: 2.530780553817749
step 4289, loss: 2.64139986038208
step 4290, loss: 3.2840497493743896
step 4291, loss: 2.7981791496276855
step 4292, loss: 2.762575387954712
step 4293, loss: 3.577155590057373
step 4294, loss: 2.708249568939209
step 4295, loss: 3.2898194789886475
step 4296, loss: 3.4393982887268066
step 4297, loss: 3.548037528991699
step 4298, loss: 3.0854036808013916
step 4299, loss: 3.341496467590332
step 4300, loss: 2.3280842304229736
step 4301, loss: 2.7426884174346924
step 4302, loss: 3.40313720703125
step 4303, loss: 3.52717924118042
step 4304, loss: 3.5529322624206543
step 4305, loss: 3.512117624282837
step 4306, loss: 3.3203017711639404
step 4307, loss: 3.8362956047058105
step 4308, loss: 3.208223819732666
step 4309, loss: 3.56558895111084
step 4310, loss: 3.3565711975097656
step 4311, loss: 3.5300817489624023
step 4312, loss: 3.4602746963500977
step 4313, loss: 3.4550251960754395
step 4314, loss: 3.3452374935150146
step 4315, loss: 3.6265079975128174
step 4316, loss: 3.7488369941711426
step 4317, loss: 3.079711675643921
step 4318, loss: 3.8218586444854736
step 4319, loss: 3.163233518600464
step 4320, loss: 3.5153539180755615
step 4321, loss: 4.274295806884766
step 4322, loss: 3.2288661003112793
step 4323, loss: 3.493678331375122
step 4324, loss: 3.2088735103607178
step 4325, loss: 3.8773422241210938
step 4326, loss: 3.461479425430298
step 4327, loss: 3.860832691192627
step 4328, loss: 4.047008991241455
step 4329, loss: 3.011878728866577
step 4330, loss: 3.100651741027832
step 4331, loss: 3.7129175662994385
step 4332, loss: 2.849630117416382
step 4333, loss: 3.576514959335327
step 4334, loss: 3.2561166286468506
step 4335, loss: 3.4979114532470703
step 4336, loss: 3.5445969104766846
step 4337, loss: 3.7099156379699707
step 4338, loss: 3.647390604019165
step 4339, loss: 2.7517952919006348
step 4340, loss: 3.6066408157348633
step 4341, loss: 3.3549652099609375
step 4342, loss: 3.6531081199645996
step 4343, loss: 3.2352638244628906
step 4344, loss: 2.82905650138855
step 4345, loss: 3.80175518989563
step 4346, loss: 2.7474520206451416
step 4347, loss: 3.2973828315734863
step 4348, loss: 3.2841622829437256
step 4349, loss: 3.2406837940216064
step 4350, loss: 3.11344838142395
step 4351, loss: 2.3817660808563232
step 4352, loss: 3.422811508178711
step 4353, loss: 3.29259991645813
step 4354, loss: 3.579211473464966
step 4355, loss: 3.751462697982788
step 4356, loss: 3.144442081451416
step 4357, loss: 2.504519462585449
step 4358, loss: 2.782090425491333
step 4359, loss: 3.2981321811676025
step 4360, loss: 2.5377860069274902
step 4361, loss: 3.145087480545044
step 4362, loss: 3.9294753074645996
step 4363, loss: 3.366567850112915
step 4364, loss: 2.8012452125549316
step 4365, loss: 3.961834192276001
step 4366, loss: 3.0884180068969727
step 4367, loss: 3.8669466972351074
step 4368, loss: 3.3020291328430176
step 4369, loss: 3.6680939197540283
step 4370, loss: 3.2444300651550293
step 4371, loss: 3.1074323654174805
step 4372, loss: 2.981670379638672
step 4373, loss: 3.6698896884918213
step 4374, loss: 3.214564085006714
step 4375, loss: 3.538459539413452
step 4376, loss: 3.1411101818084717
step 4377, loss: 3.634026288986206
step 4378, loss: 3.1573843955993652
step 4379, loss: 3.026637315750122
step 4380, loss: 3.1762049198150635
step 4381, loss: 3.4238016605377197
step 4382, loss: 3.5414810180664062
step 4383, loss: 3.5823378562927246
step 4384, loss: 3.1099886894226074
step 4385, loss: 3.0245373249053955
step 4386, loss: 3.2264115810394287
step 4387, loss: 3.295833110809326
step 4388, loss: 3.3587088584899902
step 4389, loss: 3.1789491176605225
step 4390, loss: 2.192911386489868
step 4391, loss: 2.8841419219970703
step 4392, loss: 3.0181679725646973
step 4393, loss: 3.049881935119629
step 4394, loss: 3.048255205154419
step 4395, loss: 2.906223773956299
step 4396, loss: 3.3606317043304443
step 4397, loss: 3.649421453475952
step 4398, loss: 2.807093381881714
step 4399, loss: 3.1296303272247314
step 4400, loss: 3.046508550643921
step 4401, loss: 3.5690019130706787
step 4402, loss: 2.9646992683410645
step 4403, loss: 3.357452154159546
step 4404, loss: 2.8746681213378906
step 4405, loss: 3.6868181228637695
step 4406, loss: 3.596034288406372
step 4407, loss: 3.0329959392547607
step 4408, loss: 3.2929091453552246
step 4409, loss: 3.4211010932922363
step 4410, loss: 3.773754119873047
step 4411, loss: 3.1692090034484863
step 4412, loss: 3.493257761001587
step 4413, loss: 3.5476760864257812
step 4414, loss: 3.522765874862671
step 4415, loss: 3.0055582523345947
step 4416, loss: 3.7535853385925293
step 4417, loss: 2.8299880027770996
step 4418, loss: 3.5184519290924072
step 4419, loss: 3.977647304534912
step 4420, loss: 3.508622884750366
step 4421, loss: 2.93530011177063
step 4422, loss: 2.445661783218384
step 4423, loss: 2.329177141189575
step 4424, loss: 2.94404935836792
step 4425, loss: 3.9588723182678223
step 4426, loss: 3.7355458736419678
step 4427, loss: 4.17389440536499
step 4428, loss: 4.009668827056885
step 4429, loss: 3.3369343280792236
step 4430, loss: 3.4293265342712402
step 4431, loss: 2.854024887084961
step 4432, loss: 3.2911107540130615
step 4433, loss: 3.296635389328003
step 4434, loss: 3.5288307666778564
step 4435, loss: 3.1064164638519287
step 4436, loss: 3.90844464302063
step 4437, loss: 3.627667188644409
step 4438, loss: 3.88043475151062
step 4439, loss: 3.576040506362915
step 4440, loss: 4.126352787017822
step 4441, loss: 3.4481282234191895
step 4442, loss: 3.1012349128723145
step 4443, loss: 3.546112537384033
step 4444, loss: 3.4070916175842285
step 4445, loss: 3.6158628463745117
step 4446, loss: 3.2464075088500977
step 4447, loss: 3.2062435150146484
step 4448, loss: 3.7626595497131348
step 4449, loss: 4.130681991577148
step 4450, loss: 3.925790786743164
step 4451, loss: 3.2473740577697754
step 4452, loss: 3.694837808609009
step 4453, loss: 3.8967175483703613
step 4454, loss: 3.2883355617523193
step 4455, loss: 3.3171770572662354
step 4456, loss: 2.8624842166900635
step 4457, loss: 3.6634578704833984
step 4458, loss: 3.984107255935669
step 4459, loss: 3.9210450649261475
step 4460, loss: 3.608793258666992
step 4461, loss: 3.457148551940918
step 4462, loss: 2.927342414855957
step 4463, loss: 2.5339550971984863
step 4464, loss: 2.248776912689209
step 4465, loss: 2.9776406288146973
step 4466, loss: 3.087606906890869
step 4467, loss: 3.159749746322632
step 4468, loss: 2.537597417831421
step 4469, loss: 2.8731026649475098
step 4470, loss: 3.4677574634552
step 4471, loss: 3.7439653873443604
step 4472, loss: 3.794677972793579
step 4473, loss: 3.4574902057647705
step 4474, loss: 3.0727860927581787
step 4475, loss: 3.373809337615967
step 4476, loss: 3.517529249191284
step 4477, loss: 3.0904576778411865
step 4478, loss: 3.3774192333221436
step 4479, loss: 3.21130633354187
step 4480, loss: 3.664668321609497
step 4481, loss: 3.8018290996551514
step 4482, loss: 3.7774293422698975
step 4483, loss: 3.0859034061431885
step 4484, loss: 3.440088987350464
step 4485, loss: 3.3756093978881836
step 4486, loss: 2.881269693374634
step 4487, loss: 2.810868263244629
step 4488, loss: 3.2737109661102295
step 4489, loss: 3.664355754852295
step 4490, loss: 3.921164035797119
step 4491, loss: 3.400902509689331
step 4492, loss: 3.2533798217773438
step 4493, loss: 3.4028899669647217
step 4494, loss: 2.982072114944458
step 4495, loss: 3.1503400802612305
step 4496, loss: 2.72160005569458
step 4497, loss: 2.8090412616729736
step 4498, loss: 3.1553516387939453
step 4499, loss: 3.1318509578704834
step 4500, loss: 2.512144088745117
step 4501, loss: 2.374455451965332
step 4502, loss: 2.379607677459717
step 4503, loss: 3.4763121604919434
step 4504, loss: 3.0510616302490234
step 4505, loss: 3.10916805267334
step 4506, loss: 3.258023738861084
step 4507, loss: 3.616819381713867
step 4508, loss: 3.60764217376709
step 4509, loss: 3.1904664039611816
step 4510, loss: 3.0932209491729736
step 4511, loss: 2.7845075130462646
step 4512, loss: 3.191551923751831
step 4513, loss: 3.489185333251953
step 4514, loss: 2.1381571292877197
step 4515, loss: 3.5897443294525146
step 4516, loss: 3.93560528755188
step 4517, loss: 3.6669905185699463
step 4518, loss: 3.3099606037139893
step 4519, loss: 3.6169776916503906
step 4520, loss: 3.206102132797241
step 4521, loss: 2.8862054347991943
step 4522, loss: 3.570441961288452
step 4523, loss: 3.47438645362854
step 4524, loss: 3.230543375015259
step 4525, loss: 2.821993112564087
step 4526, loss: 3.439771890640259
step 4527, loss: 3.1044623851776123
step 4528, loss: 3.5310559272766113
step 4529, loss: 3.089160680770874
step 4530, loss: 3.352844476699829
step 4531, loss: 3.094512701034546
step 4532, loss: 2.8300509452819824
step 4533, loss: 2.950984001159668
step 4534, loss: 3.8904964923858643
step 4535, loss: 4.012139320373535
step 4536, loss: 3.1078200340270996
step 4537, loss: 2.418083429336548
step 4538, loss: 3.2781357765197754
step 4539, loss: 2.6300854682922363
step 4540, loss: 2.8277244567871094
step 4541, loss: 2.0167648792266846
step 4542, loss: 3.6956894397735596
step 4543, loss: 3.204059600830078
step 4544, loss: 3.0423011779785156
step 4545, loss: 2.74902081489563
step 4546, loss: 2.7752509117126465
step 4547, loss: 3.0144100189208984
step 4548, loss: 3.386920213699341
step 4549, loss: 2.6690773963928223
step 4550, loss: 2.722632646560669
step 4551, loss: 2.8801817893981934
step 4552, loss: 3.4356436729431152
step 4553, loss: 3.481489896774292
step 4554, loss: 3.763427257537842
step 4555, loss: 3.456645965576172
step 4556, loss: 2.5311057567596436
step 4557, loss: 2.7467663288116455
step 4558, loss: 3.2226455211639404
step 4559, loss: 3.4659628868103027
step 4560, loss: 3.6681652069091797
step 4561, loss: 3.4779675006866455
step 4562, loss: 3.6442978382110596
step 4563, loss: 3.6089158058166504
step 4564, loss: 3.4631199836730957
step 4565, loss: 3.5789527893066406
step 4566, loss: 3.5069446563720703
step 4567, loss: 2.600632905960083
step 4568, loss: 3.3822436332702637
step 4569, loss: 3.198298692703247
step 4570, loss: 3.600444793701172
step 4571, loss: 3.280571937561035
step 4572, loss: 3.1531805992126465
step 4573, loss: 3.807269811630249
step 4574, loss: 3.8064136505126953
step 4575, loss: 3.795315980911255
step 4576, loss: 3.3923654556274414
step 4577, loss: 3.656078338623047
step 4578, loss: 3.5197978019714355
step 4579, loss: 3.537243604660034
step 4580, loss: 3.1785342693328857
step 4581, loss: 2.3923020362854004
step 4582, loss: 3.5785062313079834
step 4583, loss: 2.950321674346924
step 4584, loss: 3.169816493988037
step 4585, loss: 3.700371503829956
step 4586, loss: 3.6881203651428223
step 4587, loss: 3.4946658611297607
step 4588, loss: 3.440300226211548
step 4589, loss: 3.71811580657959
step 4590, loss: 3.614656448364258
step 4591, loss: 3.3749241828918457
step 4592, loss: 3.79178524017334
step 4593, loss: 3.5789682865142822
step 4594, loss: 3.7899258136749268
step 4595, loss: 3.532313585281372
step 4596, loss: 3.619748115539551
step 4597, loss: 3.4971694946289062
step 4598, loss: 2.555903911590576
step 4599, loss: 2.599587917327881
step 4600, loss: 2.396939516067505
step 4601, loss: 2.8092565536499023
step 4602, loss: 3.04492449760437
step 4603, loss: 3.3896334171295166
step 4604, loss: 3.781618118286133
step 4605, loss: 3.0700011253356934
step 4606, loss: 3.559558153152466
step 4607, loss: 3.365788698196411
step 4608, loss: 2.775554656982422
step 4609, loss: 3.0789170265197754
step 4610, loss: 3.1665356159210205
step 4611, loss: 2.9822187423706055
step 4612, loss: 2.9873995780944824
step 4613, loss: 3.8702962398529053
step 4614, loss: 2.5581915378570557
step 4615, loss: 3.2226722240448
step 4616, loss: 4.044077396392822
step 4617, loss: 3.268202781677246
step 4618, loss: 3.3212692737579346
step 4619, loss: 3.398165225982666
step 4620, loss: 3.759439468383789
step 4621, loss: 3.084717035293579
step 4622, loss: 3.9358856678009033
step 4623, loss: 3.3491060733795166
step 4624, loss: 3.226649045944214
step 4625, loss: 3.6287574768066406
step 4626, loss: 2.8859894275665283
step 4627, loss: 3.1651952266693115
step 4628, loss: 3.3319246768951416
step 4629, loss: 2.8673017024993896
step 4630, loss: 3.2982351779937744
step 4631, loss: 2.682777166366577
step 4632, loss: 3.0636186599731445
step 4633, loss: 2.420858144760132
step 4634, loss: 2.637115955352783
step 4635, loss: 2.4535434246063232
step 4636, loss: 2.4845054149627686
step 4637, loss: 2.845198392868042
step 4638, loss: 2.8554458618164062
step 4639, loss: 3.0959830284118652
step 4640, loss: 3.659203290939331
step 4641, loss: 2.389451265335083
step 4642, loss: 3.759329319000244
step 4643, loss: 3.93821382522583
step 4644, loss: 3.3681862354278564
step 4645, loss: 3.754122257232666
step 4646, loss: 2.8534421920776367
step 4647, loss: 3.606801748275757
step 4648, loss: 3.1545073986053467
step 4649, loss: 3.8336026668548584
step 4650, loss: 3.438405752182007
step 4651, loss: 3.7855474948883057
step 4652, loss: 3.1680850982666016
step 4653, loss: 3.1358914375305176
step 4654, loss: 2.9828953742980957
step 4655, loss: 3.357020854949951
step 4656, loss: 3.711097478866577
step 4657, loss: 2.9299726486206055
step 4658, loss: 3.90283465385437
step 4659, loss: 3.017910957336426
step 4660, loss: 3.127993583679199
step 4661, loss: 2.377859592437744
step 4662, loss: 3.6331639289855957
step 4663, loss: 3.658867359161377
step 4664, loss: 3.1580162048339844
step 4665, loss: 2.7864091396331787
step 4666, loss: 3.1108314990997314
step 4667, loss: 2.6133882999420166
step 4668, loss: 2.656580686569214
step 4669, loss: 2.6165947914123535
step 4670, loss: 2.527027130126953
step 4671, loss: 3.1226742267608643
step 4672, loss: 3.099445104598999
step 4673, loss: 2.811798572540283
step 4674, loss: 2.6393582820892334
step 4675, loss: 3.008977174758911
step 4676, loss: 2.768134593963623
step 4677, loss: 1.8779587745666504
step 4678, loss: 2.2630198001861572
step 4679, loss: 2.336017370223999
step 4680, loss: 2.909294605255127
step 4681, loss: 3.1055753231048584
step 4682, loss: 2.2714760303497314
step 4683, loss: 2.251086473464966
step 4684, loss: 1.8786343336105347
step 4685, loss: 2.6596522331237793
step 4686, loss: 2.0644137859344482
step 4687, loss: 3.175194025039673
step 4688, loss: 3.065800428390503
step 4689, loss: 2.531524419784546
step 4690, loss: 2.8297059535980225
step 4691, loss: 1.933957815170288
step 4692, loss: 2.7425765991210938
step 4693, loss: 3.0623421669006348
step 4694, loss: 3.041755199432373
step 4695, loss: 2.4587478637695312
step 4696, loss: 2.5233001708984375
step 4697, loss: 2.7600255012512207
step 4698, loss: 2.4215550422668457
step 4699, loss: 3.2194905281066895
step 4700, loss: 2.5584704875946045
step 4701, loss: 3.4389216899871826
step 4702, loss: 2.9735007286071777
step 4703, loss: 3.240248441696167
step 4704, loss: 3.068035125732422
step 4705, loss: 4.192045211791992
step 4706, loss: 2.89190673828125
step 4707, loss: 2.901552677154541
step 4708, loss: 2.5949201583862305
step 4709, loss: 2.2351624965667725
step 4710, loss: 3.7539944648742676
step 4711, loss: 3.2967281341552734
step 4712, loss: 3.0162124633789062
step 4713, loss: 3.059603214263916
step 4714, loss: 2.168269634246826
step 4715, loss: 2.2430925369262695
step 4716, loss: 2.7356069087982178
step 4717, loss: 3.1018459796905518
step 4718, loss: 4.04389762878418
step 4719, loss: 3.2677695751190186
step 4720, loss: 2.9261670112609863
step 4721, loss: 2.664247989654541
step 4722, loss: 3.5139782428741455
step 4723, loss: 2.607642889022827
step 4724, loss: 2.5111889839172363
step 4725, loss: 2.915402412414551
step 4726, loss: 2.810760259628296
step 4727, loss: 3.460597276687622
step 4728, loss: 2.697197198867798
step 4729, loss: 2.7188868522644043
step 4730, loss: 3.316760778427124
step 4731, loss: 2.995582103729248
step 4732, loss: 2.2782177925109863
step 4733, loss: 3.3004395961761475
step 4734, loss: 3.6108880043029785
step 4735, loss: 3.5922834873199463
step 4736, loss: 3.180210590362549
step 4737, loss: 3.3775386810302734
step 4738, loss: 3.398315191268921
step 4739, loss: 3.556659460067749
step 4740, loss: 3.577267646789551
step 4741, loss: 2.489027738571167
step 4742, loss: 2.7847330570220947
step 4743, loss: 2.2885563373565674
step 4744, loss: 2.7601375579833984
step 4745, loss: 3.1855475902557373
step 4746, loss: 3.5561206340789795
step 4747, loss: 2.644366979598999
step 4748, loss: 2.372231960296631
step 4749, loss: 3.070392370223999
step 4750, loss: 3.576425790786743
step 4751, loss: 3.174147844314575
step 4752, loss: 2.5409955978393555
step 4753, loss: 2.384974479675293
step 4754, loss: 3.4659035205841064
step 4755, loss: 2.6966257095336914
step 4756, loss: 3.264853000640869
step 4757, loss: 3.3510141372680664
step 4758, loss: 3.5976555347442627
step 4759, loss: 3.5205867290496826
step 4760, loss: 3.659808874130249
step 4761, loss: 3.375938892364502
step 4762, loss: 3.18381667137146
step 4763, loss: 3.8897204399108887
step 4764, loss: 3.2142250537872314
step 4765, loss: 2.769653797149658
step 4766, loss: 3.3720359802246094
step 4767, loss: 2.9082140922546387
step 4768, loss: 3.322493076324463
step 4769, loss: 3.0274765491485596
step 4770, loss: 3.469644069671631
step 4771, loss: 2.925034284591675
step 4772, loss: 2.6137900352478027
step 4773, loss: 2.9207541942596436
step 4774, loss: 2.5660312175750732
step 4775, loss: 2.150665044784546
step 4776, loss: 2.759870767593384
step 4777, loss: 3.032991647720337
step 4778, loss: 3.052194595336914
step 4779, loss: 2.5716969966888428
step 4780, loss: 2.8490822315216064
step 4781, loss: 2.757866144180298
step 4782, loss: 3.2068254947662354
step 4783, loss: 2.6160926818847656
step 4784, loss: 2.5262291431427
step 4785, loss: 3.231858730316162
step 4786, loss: 3.4096715450286865
step 4787, loss: 2.867074489593506
step 4788, loss: 2.9695487022399902
step 4789, loss: 2.4200189113616943
step 4790, loss: 3.1288323402404785
step 4791, loss: 3.0434391498565674
step 4792, loss: 3.4494740962982178
step 4793, loss: 3.0451741218566895
step 4794, loss: 3.4965953826904297
step 4795, loss: 4.518931865692139
step 4796, loss: 3.570652484893799
step 4797, loss: 2.797349214553833
step 4798, loss: 3.031041145324707
step 4799, loss: 3.5377399921417236
step 4800, loss: 2.2790980339050293
step 4801, loss: 3.3170065879821777
step 4802, loss: 2.638054370880127
step 4803, loss: 2.815721273422241
step 4804, loss: 3.0336503982543945
step 4805, loss: 2.7349233627319336
step 4806, loss: 3.137367010116577
step 4807, loss: 2.9825081825256348
step 4808, loss: 2.8495168685913086
step 4809, loss: 2.9374735355377197
step 4810, loss: 2.9596004486083984
step 4811, loss: 2.1419103145599365
step 4812, loss: 3.2486419677734375
step 4813, loss: 2.687074899673462
step 4814, loss: 2.8273773193359375
step 4815, loss: 2.563877582550049
step 4816, loss: 3.236812114715576
step 4817, loss: 3.1618244647979736
step 4818, loss: 3.241581916809082
step 4819, loss: 2.8666064739227295
step 4820, loss: 3.015174627304077
step 4821, loss: 2.625065326690674
step 4822, loss: 2.897878885269165
step 4823, loss: 3.279653549194336
step 4824, loss: 3.3916635513305664
step 4825, loss: 4.103219032287598
step 4826, loss: 3.5375726222991943
step 4827, loss: 2.2123496532440186
step 4828, loss: 2.065694570541382
step 4829, loss: 2.441707134246826
step 4830, loss: 2.660130262374878
step 4831, loss: 2.245521068572998
step 4832, loss: 2.9643733501434326
step 4833, loss: 2.8787734508514404
step 4834, loss: 3.1733880043029785
step 4835, loss: 2.6130166053771973
step 4836, loss: 2.506404161453247
step 4837, loss: 2.0188448429107666
step 4838, loss: 3.4410252571105957
step 4839, loss: 2.8333096504211426
step 4840, loss: 2.948115348815918
step 4841, loss: 3.0383427143096924
step 4842, loss: 2.1859896183013916
step 4843, loss: 2.4426026344299316
step 4844, loss: 3.2968788146972656
step 4845, loss: 3.090973377227783
step 4846, loss: 3.736725330352783
step 4847, loss: 3.6608352661132812
step 4848, loss: 2.954139471054077
step 4849, loss: 2.7810628414154053
step 4850, loss: 2.869211196899414
step 4851, loss: 3.002661943435669
step 4852, loss: 3.5168073177337646
step 4853, loss: 3.07314133644104
step 4854, loss: 2.634509801864624
step 4855, loss: 2.3673524856567383
step 4856, loss: 3.165806770324707
step 4857, loss: 3.0431885719299316
step 4858, loss: 3.0598247051239014
step 4859, loss: 2.3583362102508545
step 4860, loss: 2.35347843170166
step 4861, loss: 2.8449597358703613
step 4862, loss: 3.0146470069885254
step 4863, loss: 3.186795949935913
step 4864, loss: 3.5342390537261963
step 4865, loss: 2.701138973236084
step 4866, loss: 2.9536356925964355
step 4867, loss: 2.789743661880493
step 4868, loss: 3.3838226795196533
step 4869, loss: 2.928439140319824
step 4870, loss: 1.609163761138916
step 4871, loss: 2.2804741859436035
step 4872, loss: 2.619314432144165
step 4873, loss: 2.504059076309204
step 4874, loss: 2.8013827800750732
step 4875, loss: 3.1196773052215576
step 4876, loss: 3.363280773162842
step 4877, loss: 3.44169020652771
step 4878, loss: 2.7073042392730713
step 4879, loss: 3.310926675796509
step 4880, loss: 2.2674832344055176
step 4881, loss: 2.433289051055908
step 4882, loss: 2.169443130493164
step 4883, loss: 2.9842846393585205
step 4884, loss: 3.0441462993621826
step 4885, loss: 3.3639402389526367
step 4886, loss: 2.422247886657715
step 4887, loss: 2.3407390117645264
step 4888, loss: 2.607532501220703
step 4889, loss: 2.861314058303833
step 4890, loss: 2.742161273956299
step 4891, loss: 2.8114070892333984
step 4892, loss: 2.5501794815063477
step 4893, loss: 3.4542815685272217
step 4894, loss: 3.277987003326416
step 4895, loss: 4.039438247680664
step 4896, loss: 2.9883551597595215
step 4897, loss: 2.1957767009735107
step 4898, loss: 3.0208852291107178
step 4899, loss: 2.838906764984131
step 4900, loss: 2.7704572677612305
step 4901, loss: 2.480642795562744
step 4902, loss: 2.8043525218963623
step 4903, loss: 2.418294906616211
step 4904, loss: 2.783198833465576
step 4905, loss: 2.567413568496704
step 4906, loss: 2.6745176315307617
step 4907, loss: 3.469064712524414
step 4908, loss: 3.455070734024048
step 4909, loss: 3.3796181678771973
step 4910, loss: 3.94089674949646
step 4911, loss: 3.1281239986419678
step 4912, loss: 3.2701594829559326
step 4913, loss: 3.8640758991241455
step 4914, loss: 3.4617185592651367
step 4915, loss: 3.3243520259857178
step 4916, loss: 2.8111073970794678
step 4917, loss: 3.3894472122192383
step 4918, loss: 3.363827705383301
step 4919, loss: 3.5121352672576904
step 4920, loss: 3.7469258308410645
step 4921, loss: 3.3531734943389893
step 4922, loss: 2.900909662246704
step 4923, loss: 3.3908634185791016
step 4924, loss: 3.550771474838257
step 4925, loss: 3.654099225997925
step 4926, loss: 3.716752767562866
step 4927, loss: 3.3299970626831055
step 4928, loss: 2.982257843017578
step 4929, loss: 2.9808146953582764
step 4930, loss: 3.0501909255981445
step 4931, loss: 3.292587995529175
step 4932, loss: 2.4778316020965576
step 4933, loss: 2.8457834720611572
step 4934, loss: 3.3085458278656006
step 4935, loss: 3.251178741455078
step 4936, loss: 3.0538461208343506
step 4937, loss: 4.17011022567749
step 4938, loss: 3.7327492237091064
step 4939, loss: 4.029112815856934
step 4940, loss: 3.210355520248413
step 4941, loss: 2.881523370742798
step 4942, loss: 3.380772590637207
step 4943, loss: 3.0845744609832764
step 4944, loss: 3.120372772216797
step 4945, loss: 2.863572835922241
step 4946, loss: 3.4081428050994873
step 4947, loss: 3.47731876373291
step 4948, loss: 2.8620808124542236
step 4949, loss: 3.306913375854492
step 4950, loss: 3.4603443145751953
step 4951, loss: 3.0566747188568115
step 4952, loss: 2.8934438228607178
step 4953, loss: 3.273070812225342
step 4954, loss: 2.9679670333862305
step 4955, loss: 3.0493111610412598
step 4956, loss: 2.4090046882629395
step 4957, loss: 3.306757926940918
step 4958, loss: 3.0661048889160156
step 4959, loss: 2.737060308456421
step 4960, loss: 3.1413187980651855
step 4961, loss: 2.5869619846343994
step 4962, loss: 2.784818649291992
step 4963, loss: 2.470060348510742
step 4964, loss: 2.535342216491699
step 4965, loss: 2.5766756534576416
step 4966, loss: 2.729473114013672
step 4967, loss: 3.455240488052368
step 4968, loss: 2.7328171730041504
step 4969, loss: 3.1301233768463135
step 4970, loss: 2.7746317386627197
step 4971, loss: 3.248142957687378
step 4972, loss: 2.9651753902435303
step 4973, loss: 2.9156439304351807
step 4974, loss: 2.419086456298828
step 4975, loss: 3.288095235824585
step 4976, loss: 3.097661018371582
step 4977, loss: 2.894050121307373
step 4978, loss: 2.682682514190674
step 4979, loss: 3.1421639919281006
step 4980, loss: 2.302834987640381
step 4981, loss: 3.0610876083374023
step 4982, loss: 2.938805341720581
step 4983, loss: 2.384164571762085
step 4984, loss: 2.6086113452911377
step 4985, loss: 3.1273083686828613
step 4986, loss: 2.6933176517486572
step 4987, loss: 2.1828155517578125
step 4988, loss: 2.1043999195098877
step 4989, loss: 1.9861233234405518
step 4990, loss: 3.212587594985962
step 4991, loss: 2.4806151390075684
step 4992, loss: 2.947298049926758
step 4993, loss: 2.7977683544158936
step 4994, loss: 2.8435211181640625
step 4995, loss: 2.907458782196045
step 4996, loss: 2.756821393966675
step 4997, loss: 2.9643304347991943
step 4998, loss: 2.602830648422241
step 4999, loss: 1.8635419607162476
step 5000, loss: 2.7201297283172607
step 5001, loss: 2.5040035247802734
step 5002, loss: 2.334498643875122
step 5003, loss: 3.1169917583465576
step 5004, loss: 2.546083927154541
step 5005, loss: 2.8196654319763184
step 5006, loss: 3.154160499572754
step 5007, loss: 2.6837782859802246
step 5008, loss: 2.7947185039520264
step 5009, loss: 2.7861225605010986
step 5010, loss: 2.0604217052459717
step 5011, loss: 3.283419370651245
step 5012, loss: 2.4192097187042236
step 5013, loss: 2.9285261631011963
step 5014, loss: 2.8545827865600586
step 5015, loss: 3.275705575942993
step 5016, loss: 2.0084547996520996
step 5017, loss: 2.5177834033966064
step 5018, loss: 2.7271454334259033
step 5019, loss: 2.1335368156433105
step 5020, loss: 2.2575151920318604
step 5021, loss: 1.773983120918274
step 5022, loss: 1.9330964088439941
step 5023, loss: 3.386305809020996
step 5024, loss: 2.7755699157714844
step 5025, loss: 2.400693893432617
step 5026, loss: 3.2244515419006348
step 5027, loss: 2.3644466400146484
step 5028, loss: 2.9781382083892822
step 5029, loss: 2.545633554458618
step 5030, loss: 3.471259593963623
step 5031, loss: 2.5874249935150146
step 5032, loss: 2.9318687915802
step 5033, loss: 2.8227434158325195
step 5034, loss: 2.9358999729156494
step 5035, loss: 3.7892539501190186
step 5036, loss: 3.2694320678710938
step 5037, loss: 2.979963779449463
step 5038, loss: 2.825399160385132
step 5039, loss: 2.7146599292755127
step 5040, loss: 3.1487207412719727
step 5041, loss: 3.297517776489258
step 5042, loss: 3.3005645275115967
step 5043, loss: 3.1526150703430176
step 5044, loss: 2.4792304039001465
step 5045, loss: 3.0294487476348877
step 5046, loss: 2.669084072113037
step 5047, loss: 2.984084367752075
step 5048, loss: 2.9191088676452637
step 5049, loss: 2.7162773609161377
step 5050, loss: 2.682333469390869
step 5051, loss: 3.2990663051605225
step 5052, loss: 3.2161757946014404
step 5053, loss: 2.776810884475708
step 5054, loss: 1.8171344995498657
step 5055, loss: 3.568948745727539
step 5056, loss: 4.516336917877197
step 5057, loss: 3.658360481262207
step 5058, loss: 2.422107696533203
step 5059, loss: 2.1880412101745605
step 5060, loss: 2.3666088581085205
step 5061, loss: 3.0601351261138916
step 5062, loss: 3.038949728012085
step 5063, loss: 2.900054454803467
step 5064, loss: 2.96683931350708
step 5065, loss: 3.0987000465393066
step 5066, loss: 2.3961801528930664
step 5067, loss: 2.5496883392333984
step 5068, loss: 3.2409725189208984
step 5069, loss: 3.4478838443756104
step 5070, loss: 3.0661957263946533
step 5071, loss: 2.692831039428711
step 5072, loss: 1.529909372329712
Saved best model with loss: 1.5299 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.5299.pt
step 5073, loss: 2.324331760406494
step 5074, loss: 2.416459321975708
step 5075, loss: 3.075603723526001
step 5076, loss: 3.405597448348999
step 5077, loss: 2.1536993980407715
step 5078, loss: 2.794445037841797
step 5079, loss: 3.5300824642181396
step 5080, loss: 2.887585163116455
step 5081, loss: 3.208815813064575
step 5082, loss: 3.035464286804199
step 5083, loss: 3.2660226821899414
step 5084, loss: 2.685098886489868
step 5085, loss: 2.8007140159606934
step 5086, loss: 2.5682830810546875
step 5087, loss: 3.1321229934692383
step 5088, loss: 2.87522554397583
step 5089, loss: 2.356873035430908
step 5090, loss: 2.9078688621520996
step 5091, loss: 2.54807448387146
step 5092, loss: 3.3934879302978516
step 5093, loss: 3.2297894954681396
step 5094, loss: 3.0661139488220215
step 5095, loss: 2.534501314163208
step 5096, loss: 3.1141061782836914
step 5097, loss: 3.213690996170044
step 5098, loss: 2.6531918048858643
step 5099, loss: 3.692469835281372
step 5100, loss: 3.7206783294677734
step 5101, loss: 2.559952735900879
step 5102, loss: 3.014374017715454
step 5103, loss: 2.8466641902923584
step 5104, loss: 2.871079683303833
step 5105, loss: 2.8199188709259033
step 5106, loss: 2.734241247177124
step 5107, loss: 2.3680264949798584
step 5108, loss: 2.648472547531128
step 5109, loss: 2.737748384475708
step 5110, loss: 2.4986796379089355
step 5111, loss: 2.896390199661255
step 5112, loss: 2.5222177505493164
step 5113, loss: 3.1183009147644043
step 5114, loss: 2.758044719696045
step 5115, loss: 3.4328689575195312
step 5116, loss: 2.5631229877471924
step 5117, loss: 2.28462290763855
step 5118, loss: 2.521756887435913
step 5119, loss: 2.713998556137085
step 5120, loss: 2.3092377185821533
step 5121, loss: 2.458202838897705
step 5122, loss: 3.0243289470672607
step 5123, loss: 2.790361166000366
step 5124, loss: 2.8755950927734375
step 5125, loss: 2.47865891456604
step 5126, loss: 3.3715362548828125
step 5127, loss: 2.401254653930664
step 5128, loss: 2.9545674324035645
step 5129, loss: 2.3889033794403076
step 5130, loss: 2.64900541305542
step 5131, loss: 2.0995278358459473
step 5132, loss: 2.4931485652923584
step 5133, loss: 3.530355930328369
step 5134, loss: 2.722470760345459
step 5135, loss: 2.610450267791748
step 5136, loss: 2.349848747253418
step 5137, loss: 2.749885082244873
step 5138, loss: 3.576296806335449
step 5139, loss: 3.2593443393707275
step 5140, loss: 3.167983055114746
step 5141, loss: 3.0456440448760986
step 5142, loss: 2.1035752296447754
step 5143, loss: 2.4502694606781006
step 5144, loss: 2.540235757827759
step 5145, loss: 2.8939762115478516
step 5146, loss: 2.8675644397735596
step 5147, loss: 2.137802839279175
step 5148, loss: 2.049886703491211
step 5149, loss: 2.709129571914673
step 5150, loss: 3.1405346393585205
step 5151, loss: 2.8578033447265625
step 5152, loss: 2.858474016189575
step 5153, loss: 2.7475361824035645
step 5154, loss: 2.9712982177734375
step 5155, loss: 2.559568166732788
step 5156, loss: 2.557931423187256
step 5157, loss: 2.605238914489746
step 5158, loss: 2.4222817420959473
step 5159, loss: 2.833221673965454
step 5160, loss: 2.3808867931365967
step 5161, loss: 2.2311713695526123
step 5162, loss: 2.986118793487549
step 5163, loss: 2.4942307472229004
step 5164, loss: 2.6685876846313477
step 5165, loss: 1.817795753479004
step 5166, loss: 2.2366700172424316
step 5167, loss: 2.46964693069458
step 5168, loss: 2.628382444381714
step 5169, loss: 2.1746065616607666
step 5170, loss: 2.083710193634033
step 5171, loss: 2.670727252960205
step 5172, loss: 2.7591142654418945
step 5173, loss: 2.167620897293091
step 5174, loss: 2.419691562652588
step 5175, loss: 2.3252618312835693
step 5176, loss: 2.369556427001953
step 5177, loss: 2.6322145462036133
step 5178, loss: 2.7275044918060303
step 5179, loss: 2.388735055923462
step 5180, loss: 2.325605869293213
step 5181, loss: 1.8601726293563843
step 5182, loss: 2.1369333267211914
step 5183, loss: 2.4608991146087646
step 5184, loss: 1.806759238243103
step 5185, loss: 2.016526699066162
step 5186, loss: 2.520951271057129
step 5187, loss: 3.307732105255127
step 5188, loss: 2.3911964893341064
step 5189, loss: 3.3365399837493896
step 5190, loss: 3.5270633697509766
step 5191, loss: 3.34285569190979
step 5192, loss: 3.5280730724334717
step 5193, loss: 2.429910182952881
step 5194, loss: 2.323782205581665
step 5195, loss: 3.5775558948516846
step 5196, loss: 3.394711971282959
step 5197, loss: 3.365041732788086
step 5198, loss: 3.757908344268799
step 5199, loss: 3.186213493347168
step 5200, loss: 2.862783432006836
step 5201, loss: 3.1722705364227295
step 5202, loss: 3.8299760818481445
step 5203, loss: 3.1761090755462646
step 5204, loss: 3.367682456970215
step 5205, loss: 3.459671974182129
step 5206, loss: 3.0669028759002686
step 5207, loss: 3.011219024658203
step 5208, loss: 2.8146634101867676
step 5209, loss: 3.389704942703247
step 5210, loss: 3.5872104167938232
step 5211, loss: 3.46108078956604
step 5212, loss: 4.254120349884033
step 5213, loss: 3.5598552227020264
step 5214, loss: 3.5285212993621826
step 5215, loss: 3.721060037612915
step 5216, loss: 3.269765615463257
step 5217, loss: 3.714353561401367
step 5218, loss: 3.447659492492676
step 5219, loss: 3.530034065246582
step 5220, loss: 3.12977933883667
step 5221, loss: 3.744166851043701
step 5222, loss: 3.2529428005218506
step 5223, loss: 3.9024367332458496
step 5224, loss: 3.4571938514709473
step 5225, loss: 3.1087899208068848
step 5226, loss: 3.290252685546875
step 5227, loss: 3.310106039047241
step 5228, loss: 2.6725172996520996
step 5229, loss: 2.9815943241119385
step 5230, loss: 2.4531662464141846
step 5231, loss: 3.181684970855713
step 5232, loss: 3.628390073776245
step 5233, loss: 3.566680908203125
step 5234, loss: 2.9571692943573
step 5235, loss: 2.959547758102417
step 5236, loss: 2.821756362915039
step 5237, loss: 3.4065961837768555
step 5238, loss: 3.35213303565979
step 5239, loss: 3.470975399017334
step 5240, loss: 3.052260398864746
step 5241, loss: 3.669146776199341
step 5242, loss: 3.4764208793640137
step 5243, loss: 3.6296164989471436
step 5244, loss: 3.2332255840301514
step 5245, loss: 3.703594923019409
step 5246, loss: 3.1062710285186768
step 5247, loss: 3.28556489944458
step 5248, loss: 2.6698880195617676
step 5249, loss: 2.618133783340454
step 5250, loss: 3.2646639347076416
step 5251, loss: 3.015439748764038
step 5252, loss: 3.220139741897583
step 5253, loss: 2.627511978149414
step 5254, loss: 3.039057731628418
step 5255, loss: 3.004441261291504
step 5256, loss: 3.074302911758423
step 5257, loss: 2.726261615753174
step 5258, loss: 2.647566318511963
step 5259, loss: 2.238471031188965
step 5260, loss: 2.3544633388519287
step 5261, loss: 2.719726324081421
step 5262, loss: 2.3788506984710693
step 5263, loss: 3.0639984607696533
step 5264, loss: 2.848837375640869
step 5265, loss: 2.627218723297119
step 5266, loss: 2.473895311355591
step 5267, loss: 2.4170169830322266
step 5268, loss: 3.701524257659912
step 5269, loss: 3.498683214187622
step 5270, loss: 3.389890670776367
step 5271, loss: 2.445512294769287
step 5272, loss: 2.4139058589935303
step 5273, loss: 3.1865789890289307
step 5274, loss: 3.202266216278076
step 5275, loss: 2.3049535751342773
step 5276, loss: 2.922531843185425
step 5277, loss: 2.3941810131073
step 5278, loss: 2.6615562438964844
step 5279, loss: 3.0847060680389404
step 5280, loss: 2.446200370788574
step 5281, loss: 3.149388551712036
step 5282, loss: 2.844265937805176
step 5283, loss: 3.1472740173339844
step 5284, loss: 2.7409298419952393
step 5285, loss: 2.7666175365448
step 5286, loss: 3.456082582473755
step 5287, loss: 3.150786876678467
step 5288, loss: 2.7631032466888428
step 5289, loss: 3.228677272796631
step 5290, loss: 3.039598226547241
step 5291, loss: 2.7312755584716797
step 5292, loss: 2.950113534927368
step 5293, loss: 2.9679479598999023
step 5294, loss: 3.1479604244232178
step 5295, loss: 2.8668336868286133
step 5296, loss: 2.954282522201538
step 5297, loss: 3.6449131965637207
step 5298, loss: 2.773624897003174
step 5299, loss: 2.971945285797119
step 5300, loss: 2.9473040103912354
step 5301, loss: 2.9657769203186035
step 5302, loss: 2.236870765686035
step 5303, loss: 2.8291170597076416
step 5304, loss: 2.36002516746521
step 5305, loss: 2.4151718616485596
step 5306, loss: 2.332432270050049
step 5307, loss: 2.9302799701690674
step 5308, loss: 2.755828857421875
step 5309, loss: 2.799787759780884
step 5310, loss: 2.9580812454223633
step 5311, loss: 2.778761625289917
step 5312, loss: 2.931203842163086
step 5313, loss: 2.1346654891967773
step 5314, loss: 3.3846278190612793
step 5315, loss: 2.9596617221832275
step 5316, loss: 2.4182541370391846
step 5317, loss: 3.0449135303497314
step 5318, loss: 2.1742849349975586
step 5319, loss: 2.3714754581451416
step 5320, loss: 3.075439929962158
step 5321, loss: 2.0372865200042725
step 5322, loss: 2.3096985816955566
step 5323, loss: 2.4924702644348145
step 5324, loss: 2.447721004486084
step 5325, loss: 2.3634350299835205
step 5326, loss: 1.9446033239364624
step 5327, loss: 2.4347305297851562
step 5328, loss: 2.6847023963928223
step 5329, loss: 2.7374267578125
step 5330, loss: 2.984802484512329
step 5331, loss: 3.044246196746826
step 5332, loss: 2.3530538082122803
step 5333, loss: 2.8167946338653564
step 5334, loss: 3.0207347869873047
step 5335, loss: 2.5877861976623535
step 5336, loss: 2.5834884643554688
step 5337, loss: 2.870739698410034
step 5338, loss: 2.2282729148864746
step 5339, loss: 2.511852264404297
step 5340, loss: 2.354440689086914
step 5341, loss: 2.227996826171875
step 5342, loss: 2.4232687950134277
step 5343, loss: 2.0171091556549072
step 5344, loss: 2.844937801361084
step 5345, loss: 2.9802703857421875
step 5346, loss: 2.690586566925049
step 5347, loss: 2.369107484817505
step 5348, loss: 2.360400676727295
step 5349, loss: 2.5022358894348145
step 5350, loss: 2.6294169425964355
step 5351, loss: 2.3658130168914795
step 5352, loss: 2.579305648803711
step 5353, loss: 2.7168703079223633
step 5354, loss: 3.1715891361236572
step 5355, loss: 2.6857197284698486
step 5356, loss: 2.3619494438171387
step 5357, loss: 2.1888997554779053
step 5358, loss: 2.3692173957824707
step 5359, loss: 2.109522581100464
step 5360, loss: 2.7242891788482666
step 5361, loss: 2.868563652038574
step 5362, loss: 2.6027395725250244
step 5363, loss: 1.857822299003601
step 5364, loss: 2.277764320373535
step 5365, loss: 2.3717141151428223
step 5366, loss: 2.3257648944854736
step 5367, loss: 2.536036729812622
step 5368, loss: 2.4910812377929688
step 5369, loss: 2.980638265609741
step 5370, loss: 2.5201950073242188
step 5371, loss: 2.3424789905548096
step 5372, loss: 2.641113042831421
step 5373, loss: 2.2980592250823975
step 5374, loss: 2.1313023567199707
step 5375, loss: 2.3329365253448486
step 5376, loss: 2.2139031887054443
step 5377, loss: 2.3874144554138184
step 5378, loss: 1.8905034065246582
step 5379, loss: 2.312997817993164
step 5380, loss: 2.8527848720550537
step 5381, loss: 1.9737294912338257
step 5382, loss: 2.6755454540252686
step 5383, loss: 2.833991289138794
step 5384, loss: 2.2647454738616943
step 5385, loss: 2.5732383728027344
step 5386, loss: 2.9750165939331055
step 5387, loss: 2.5823051929473877
step 5388, loss: 2.290499687194824
step 5389, loss: 2.4144115447998047
step 5390, loss: 2.5262129306793213
step 5391, loss: 2.321044921875
step 5392, loss: 2.1741185188293457
step 5393, loss: 2.633161783218384
step 5394, loss: 2.7994630336761475
step 5395, loss: 2.7628095149993896
step 5396, loss: 2.4457287788391113
step 5397, loss: 2.5259904861450195
step 5398, loss: 2.332009792327881
step 5399, loss: 2.5162599086761475
step 5400, loss: 2.48874831199646
step 5401, loss: 2.7716410160064697
step 5402, loss: 3.010136365890503
step 5403, loss: 3.100642204284668
step 5404, loss: 3.2022855281829834
step 5405, loss: 2.1905996799468994
step 5406, loss: 2.1140880584716797
step 5407, loss: 2.288360834121704
step 5408, loss: 2.279974937438965
step 5409, loss: 2.024472951889038
step 5410, loss: 2.5111143589019775
step 5411, loss: 2.583264112472534
step 5412, loss: 2.5958752632141113
step 5413, loss: 2.473888874053955
step 5414, loss: 2.295175313949585
step 5415, loss: 2.2040066719055176
step 5416, loss: 1.8965563774108887
step 5417, loss: 2.1873762607574463
step 5418, loss: 2.227965831756592
step 5419, loss: 2.154259204864502
step 5420, loss: 2.472191095352173
step 5421, loss: 2.5285396575927734
step 5422, loss: 2.92265248298645
step 5423, loss: 1.902602195739746
step 5424, loss: 1.9313527345657349
step 5425, loss: 1.8429368734359741
step 5426, loss: 2.091522455215454
step 5427, loss: 2.2404627799987793
step 5428, loss: 2.4917891025543213
step 5429, loss: 2.7459840774536133
step 5430, loss: 2.635230302810669
step 5431, loss: 2.3871827125549316
step 5432, loss: 2.5198872089385986
step 5433, loss: 2.478768825531006
step 5434, loss: 2.6340765953063965
step 5435, loss: 2.5194811820983887
step 5436, loss: 2.702918767929077
step 5437, loss: 2.3126823902130127
step 5438, loss: 2.08280348777771
step 5439, loss: 2.1461448669433594
step 5440, loss: 1.771477222442627
step 5441, loss: 2.0748658180236816
step 5442, loss: 2.5276989936828613
step 5443, loss: 1.5105082988739014
Saved best model with loss: 1.5105 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.5105.pt
step 5444, loss: 2.229478359222412
step 5445, loss: 2.4071242809295654
step 5446, loss: 2.575059175491333
step 5447, loss: 2.0792267322540283
step 5448, loss: 2.1838340759277344
step 5449, loss: 2.7830915451049805
step 5450, loss: 2.762772560119629
step 5451, loss: 2.1302716732025146
step 5452, loss: 2.40419864654541
step 5453, loss: 2.741511344909668
step 5454, loss: 2.4054129123687744
step 5455, loss: 3.2121500968933105
step 5456, loss: 2.29372501373291
step 5457, loss: 2.1723592281341553
step 5458, loss: 2.0164804458618164
step 5459, loss: 1.8208500146865845
step 5460, loss: 2.1042063236236572
step 5461, loss: 1.6735728979110718
step 5462, loss: 1.969879388809204
step 5463, loss: 2.084805727005005
step 5464, loss: 1.959987759590149
step 5465, loss: 1.9156334400177002
step 5466, loss: 1.9168164730072021
step 5467, loss: 2.1471364498138428
step 5468, loss: 2.3176064491271973
step 5469, loss: 2.037609577178955
step 5470, loss: 1.7576051950454712
step 5471, loss: 1.609113335609436
step 5472, loss: 2.0775816440582275
step 5473, loss: 2.1986470222473145
step 5474, loss: 2.0598528385162354
step 5475, loss: 2.414719343185425
step 5476, loss: 2.0061287879943848
step 5477, loss: 1.9443167448043823
step 5478, loss: 2.4877257347106934
step 5479, loss: 2.3272387981414795
step 5480, loss: 1.8966683149337769
step 5481, loss: 1.8645520210266113
step 5482, loss: 2.470526695251465
step 5483, loss: 2.069469928741455
step 5484, loss: 2.8530938625335693
step 5485, loss: 2.6482417583465576
step 5486, loss: 2.944495916366577
step 5487, loss: 2.463129758834839
step 5488, loss: 1.8260571956634521
step 5489, loss: 2.3485684394836426
step 5490, loss: 2.9596877098083496
step 5491, loss: 2.893822431564331
step 5492, loss: 2.569549322128296
step 5493, loss: 2.164566993713379
step 5494, loss: 2.462222099304199
step 5495, loss: 2.1429076194763184
step 5496, loss: 2.270981788635254
step 5497, loss: 2.15466570854187
step 5498, loss: 2.4635114669799805
step 5499, loss: 1.9190624952316284
step 5500, loss: 2.5363240242004395
step 5501, loss: 1.9949634075164795
step 5502, loss: 2.339467763900757
step 5503, loss: 2.178523302078247
step 5504, loss: 1.8861111402511597
step 5505, loss: 2.284534454345703
step 5506, loss: 1.9938292503356934
step 5507, loss: 2.2989609241485596
step 5508, loss: 2.3381166458129883
step 5509, loss: 2.3728764057159424
step 5510, loss: 2.446416139602661
step 5511, loss: 2.2960731983184814
step 5512, loss: 2.470109224319458
step 5513, loss: 2.7789275646209717
step 5514, loss: 2.553609848022461
step 5515, loss: 2.4995501041412354
step 5516, loss: 2.219881296157837
step 5517, loss: 1.8154186010360718
step 5518, loss: 1.964698076248169
step 5519, loss: 2.3766212463378906
step 5520, loss: 1.7071707248687744
step 5521, loss: 2.176746368408203
step 5522, loss: 2.079979181289673
step 5523, loss: 2.162736415863037
step 5524, loss: 2.2696847915649414
step 5525, loss: 2.618821620941162
step 5526, loss: 2.30271577835083
step 5527, loss: 2.626330852508545
step 5528, loss: 2.431087017059326
step 5529, loss: 2.5204403400421143
step 5530, loss: 2.2297756671905518
step 5531, loss: 2.882946252822876
step 5532, loss: 1.917604923248291
step 5533, loss: 2.1514503955841064
step 5534, loss: 1.8694947957992554
step 5535, loss: 1.8188732862472534
step 5536, loss: 1.6200271844863892
step 5537, loss: 1.9787688255310059
step 5538, loss: 1.981624722480774
step 5539, loss: 2.0994906425476074
step 5540, loss: 2.5599799156188965
step 5541, loss: 2.5338480472564697
step 5542, loss: 2.4497790336608887
step 5543, loss: 2.7233057022094727
step 5544, loss: 2.8552041053771973
step 5545, loss: 2.353165864944458
step 5546, loss: 2.0312719345092773
step 5547, loss: 2.6776089668273926
step 5548, loss: 2.335639715194702
step 5549, loss: 1.9881305694580078
step 5550, loss: 1.7922569513320923
step 5551, loss: 1.9717836380004883
step 5552, loss: 2.411592960357666
step 5553, loss: 2.275657892227173
step 5554, loss: 2.1922380924224854
step 5555, loss: 2.4213640689849854
step 5556, loss: 2.2085189819335938
step 5557, loss: 2.438185214996338
step 5558, loss: 1.833922266960144
step 5559, loss: 1.5801869630813599
step 5560, loss: 1.7483288049697876
step 5561, loss: 2.051915407180786
step 5562, loss: 1.8982844352722168
step 5563, loss: 2.241133451461792
step 5564, loss: 2.0956387519836426
step 5565, loss: 1.9340897798538208
step 5566, loss: 1.7123185396194458
step 5567, loss: 1.874820351600647
step 5568, loss: 1.9898637533187866
step 5569, loss: 2.2313928604125977
step 5570, loss: 2.312551259994507
step 5571, loss: 2.417422294616699
step 5572, loss: 2.1659789085388184
step 5573, loss: 1.919979214668274
step 5574, loss: 2.1781275272369385
step 5575, loss: 1.5854687690734863
step 5576, loss: 2.2528235912323
step 5577, loss: 2.524200677871704
step 5578, loss: 2.5133087635040283
step 5579, loss: 2.281787157058716
step 5580, loss: 2.663646936416626
step 5581, loss: 2.4367220401763916
step 5582, loss: 2.2623450756073
step 5583, loss: 2.2622272968292236
step 5584, loss: 2.256572723388672
step 5585, loss: 2.2539472579956055
step 5586, loss: 2.0726428031921387
step 5587, loss: 2.634178400039673
step 5588, loss: 1.9495311975479126
step 5589, loss: 2.7050960063934326
step 5590, loss: 2.0240683555603027
step 5591, loss: 2.169490098953247
step 5592, loss: 2.63384747505188
step 5593, loss: 2.4381139278411865
step 5594, loss: 2.4316163063049316
step 5595, loss: 2.3409945964813232
step 5596, loss: 2.5108399391174316
step 5597, loss: 2.847536325454712
step 5598, loss: 2.2196669578552246
step 5599, loss: 2.728827714920044
step 5600, loss: 1.8764870166778564
step 5601, loss: 2.6659770011901855
step 5602, loss: 2.221268892288208
step 5603, loss: 2.6238269805908203
step 5604, loss: 3.239436388015747
step 5605, loss: 2.4616339206695557
step 5606, loss: 2.892665386199951
step 5607, loss: 2.51607084274292
step 5608, loss: 2.329921245574951
step 5609, loss: 2.4188501834869385
step 5610, loss: 2.440255880355835
step 5611, loss: 2.102496862411499
step 5612, loss: 3.2756667137145996
step 5613, loss: 2.5650572776794434
step 5614, loss: 2.4449944496154785
step 5615, loss: 2.5135879516601562
step 5616, loss: 2.380770206451416
step 5617, loss: 2.5508415699005127
step 5618, loss: 2.570919990539551
step 5619, loss: 2.8263468742370605
step 5620, loss: 2.4000585079193115
step 5621, loss: 2.231527328491211
step 5622, loss: 2.325251579284668
step 5623, loss: 1.9609336853027344
step 5624, loss: 1.687378168106079
step 5625, loss: 2.4674735069274902
step 5626, loss: 2.105332851409912
step 5627, loss: 2.1173582077026367
step 5628, loss: 2.2280335426330566
step 5629, loss: 2.35475754737854
step 5630, loss: 2.0848891735076904
step 5631, loss: 2.2579030990600586
step 5632, loss: 1.9113363027572632
step 5633, loss: 2.3124277591705322
step 5634, loss: 2.183960199356079
step 5635, loss: 2.398977518081665
step 5636, loss: 2.2474942207336426
step 5637, loss: 2.1579151153564453
step 5638, loss: 1.9727449417114258
step 5639, loss: 1.890188217163086
step 5640, loss: 1.8585470914840698
step 5641, loss: 2.1991662979125977
step 5642, loss: 2.283827781677246
step 5643, loss: 2.0207340717315674
step 5644, loss: 2.336909770965576
step 5645, loss: 1.68289315700531
step 5646, loss: 2.3314905166625977
step 5647, loss: 2.2916042804718018
step 5648, loss: 2.502143144607544
step 5649, loss: 2.4265379905700684
step 5650, loss: 2.495061159133911
step 5651, loss: 2.407654047012329
step 5652, loss: 2.1383168697357178
step 5653, loss: 2.633169174194336
step 5654, loss: 2.488724708557129
step 5655, loss: 2.535355806350708
step 5656, loss: 2.4130289554595947
step 5657, loss: 2.2672572135925293
step 5658, loss: 1.9264299869537354
step 5659, loss: 2.11543607711792
step 5660, loss: 1.8058592081069946
step 5661, loss: 1.994551420211792
step 5662, loss: 1.7973381280899048
step 5663, loss: 2.3884975910186768
step 5664, loss: 2.357239007949829
step 5665, loss: 2.0442585945129395
step 5666, loss: 2.4113991260528564
step 5667, loss: 1.8965753316879272
step 5668, loss: 1.7479169368743896
step 5669, loss: 1.9992111921310425
step 5670, loss: 2.172771692276001
step 5671, loss: 1.943771243095398
step 5672, loss: 1.8571854829788208
step 5673, loss: 1.7366915941238403
step 5674, loss: 1.7106295824050903
step 5675, loss: 1.9637665748596191
step 5676, loss: 1.6175307035446167
step 5677, loss: 1.6631953716278076
step 5678, loss: 1.9037895202636719
step 5679, loss: 1.6689256429672241
step 5680, loss: 1.0778076648712158
Saved best model with loss: 1.0778 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.0778.pt
step 5681, loss: 1.1216596364974976
step 5682, loss: 1.726413369178772
step 5683, loss: 2.0152833461761475
step 5684, loss: 2.119163751602173
step 5685, loss: 1.7638967037200928
step 5686, loss: 1.5560240745544434
step 5687, loss: 1.220334768295288
step 5688, loss: 1.6940877437591553
step 5689, loss: 2.128891706466675
step 5690, loss: 1.5918912887573242
step 5691, loss: 2.606044292449951
step 5692, loss: 2.6082770824432373
step 5693, loss: 2.8772172927856445
step 5694, loss: 2.2034122943878174
step 5695, loss: 1.8561497926712036
step 5696, loss: 2.3509514331817627
step 5697, loss: 1.9794405698776245
step 5698, loss: 2.11287522315979
step 5699, loss: 2.5640149116516113
step 5700, loss: 2.471162796020508
step 5701, loss: 2.2209911346435547
step 5702, loss: 2.3686583042144775
step 5703, loss: 2.1114518642425537
step 5704, loss: 2.203998327255249
step 5705, loss: 2.226076602935791
step 5706, loss: 2.5228097438812256
step 5707, loss: 2.102548837661743
step 5708, loss: 2.257920980453491
step 5709, loss: 2.1712605953216553
step 5710, loss: 2.0574491024017334
step 5711, loss: 2.5903573036193848
step 5712, loss: 2.0452237129211426
step 5713, loss: 2.504209518432617
step 5714, loss: 1.7908459901809692
step 5715, loss: 2.5211918354034424
step 5716, loss: 2.186201810836792
step 5717, loss: 2.279101848602295
step 5718, loss: 2.461360454559326
step 5719, loss: 1.7862008810043335
step 5720, loss: 1.844718098640442
step 5721, loss: 2.247871160507202
step 5722, loss: 2.054377555847168
step 5723, loss: 1.9004173278808594
step 5724, loss: 2.1858420372009277
step 5725, loss: 2.3601491451263428
step 5726, loss: 2.230262041091919
step 5727, loss: 1.7657946348190308
step 5728, loss: 1.8194782733917236
step 5729, loss: 2.14251708984375
step 5730, loss: 1.922964096069336
step 5731, loss: 2.5613350868225098
step 5732, loss: 2.301884174346924
step 5733, loss: 2.0997798442840576
step 5734, loss: 1.939387559890747
step 5735, loss: 2.214503049850464
step 5736, loss: 1.995497703552246
step 5737, loss: 1.9765080213546753
step 5738, loss: 2.56632399559021
step 5739, loss: 2.0697808265686035
step 5740, loss: 2.2334063053131104
step 5741, loss: 2.165912628173828
step 5742, loss: 1.8132537603378296
step 5743, loss: 1.9295992851257324
step 5744, loss: 1.8198325634002686
step 5745, loss: 1.6870924234390259
step 5746, loss: 1.7278285026550293
step 5747, loss: 2.078352212905884
step 5748, loss: 2.147000789642334
step 5749, loss: 1.9096605777740479
step 5750, loss: 1.756944179534912
step 5751, loss: 1.9739474058151245
step 5752, loss: 2.432121992111206
step 5753, loss: 2.3821372985839844
step 5754, loss: 2.1322600841522217
step 5755, loss: 2.508521795272827
step 5756, loss: 2.1310641765594482
step 5757, loss: 1.948101282119751
step 5758, loss: 2.2368898391723633
step 5759, loss: 2.105576515197754
step 5760, loss: 1.938447117805481
step 5761, loss: 2.481593132019043
step 5762, loss: 1.8520420789718628
step 5763, loss: 2.5526657104492188
step 5764, loss: 2.245272159576416
step 5765, loss: 2.188723564147949
step 5766, loss: 2.214733839035034
step 5767, loss: 2.3663644790649414
step 5768, loss: 2.0463099479675293
step 5769, loss: 2.45400333404541
step 5770, loss: 2.2653660774230957
step 5771, loss: 1.98014497756958
step 5772, loss: 2.2954981327056885
step 5773, loss: 1.9846209287643433
step 5774, loss: 2.1958653926849365
step 5775, loss: 2.2856154441833496
step 5776, loss: 2.4679057598114014
step 5777, loss: 2.2174370288848877
step 5778, loss: 2.458956718444824
step 5779, loss: 2.2299749851226807
step 5780, loss: 2.3096868991851807
step 5781, loss: 1.7808222770690918
step 5782, loss: 2.3757565021514893
step 5783, loss: 2.553943157196045
step 5784, loss: 2.026970386505127
step 5785, loss: 1.5009987354278564
step 5786, loss: 1.6787095069885254
step 5787, loss: 2.1789000034332275
step 5788, loss: 1.9920607805252075
step 5789, loss: 1.8863826990127563
step 5790, loss: 2.3223814964294434
step 5791, loss: 2.1707425117492676
step 5792, loss: 1.8163630962371826
step 5793, loss: 2.3787224292755127
step 5794, loss: 1.87166166305542
step 5795, loss: 2.1802620887756348
step 5796, loss: 2.3298401832580566
step 5797, loss: 2.0779058933258057
step 5798, loss: 2.3256261348724365
step 5799, loss: 1.8172624111175537
step 5800, loss: 1.817376971244812
step 5801, loss: 1.6909422874450684
step 5802, loss: 1.8992429971694946
step 5803, loss: 1.4654916524887085
step 5804, loss: 1.3179726600646973
step 5805, loss: 1.981647253036499
step 5806, loss: 2.2145893573760986
step 5807, loss: 1.7037612199783325
step 5808, loss: 2.0567514896392822
step 5809, loss: 2.4292235374450684
step 5810, loss: 1.7738076448440552
step 5811, loss: 2.1847522258758545
step 5812, loss: 2.0916333198547363
step 5813, loss: 2.081873655319214
step 5814, loss: 2.0364370346069336
step 5815, loss: 1.9283865690231323
step 5816, loss: 1.895104169845581
step 5817, loss: 2.0162603855133057
step 5818, loss: 1.6860102415084839
step 5819, loss: 2.0149948596954346
step 5820, loss: 1.7114931344985962
step 5821, loss: 1.6843020915985107
step 5822, loss: 2.062967300415039
step 5823, loss: 1.6658686399459839
step 5824, loss: 1.8537627458572388
step 5825, loss: 1.847222924232483
step 5826, loss: 2.048280954360962
step 5827, loss: 2.2165465354919434
step 5828, loss: 2.158384084701538
step 5829, loss: 1.6879843473434448
step 5830, loss: 1.6086021661758423
step 5831, loss: 1.8634397983551025
step 5832, loss: 2.2841637134552
step 5833, loss: 1.8069573640823364
step 5834, loss: 2.037973403930664
step 5835, loss: 2.1915080547332764
step 5836, loss: 2.071166753768921
step 5837, loss: 2.0452542304992676
step 5838, loss: 2.1202809810638428
step 5839, loss: 2.250056743621826
step 5840, loss: 1.9991602897644043
step 5841, loss: 2.090662956237793
step 5842, loss: 1.8988029956817627
step 5843, loss: 1.6096296310424805
step 5844, loss: 2.0584700107574463
step 5845, loss: 2.4726643562316895
step 5846, loss: 1.9337903261184692
step 5847, loss: 1.934380054473877
step 5848, loss: 1.9544318914413452
step 5849, loss: 1.9885456562042236
step 5850, loss: 1.4816386699676514
step 5851, loss: 1.959218978881836
step 5852, loss: 2.216486930847168
step 5853, loss: 1.9943029880523682
step 5854, loss: 2.069016218185425
step 5855, loss: 2.2676620483398438
step 5856, loss: 2.3672428131103516
step 5857, loss: 1.9461389780044556
step 5858, loss: 2.3748533725738525
step 5859, loss: 1.850169062614441
step 5860, loss: 1.5912615060806274
step 5861, loss: 2.21191668510437
step 5862, loss: 2.329603910446167
step 5863, loss: 2.112136125564575
step 5864, loss: 2.4498074054718018
step 5865, loss: 1.9699064493179321
step 5866, loss: 3.021003246307373
step 5867, loss: 2.271193504333496
step 5868, loss: 1.8149901628494263
step 5869, loss: 2.2474050521850586
step 5870, loss: 1.9249608516693115
step 5871, loss: 2.056135892868042
step 5872, loss: 2.299445390701294
step 5873, loss: 2.371830701828003
step 5874, loss: 1.7930339574813843
step 5875, loss: 2.109642505645752
step 5876, loss: 2.2058446407318115
step 5877, loss: 1.9025051593780518
step 5878, loss: 2.090306282043457
step 5879, loss: 2.1516664028167725
step 5880, loss: 1.9038524627685547
step 5881, loss: 1.9835587739944458
step 5882, loss: 1.9881739616394043
step 5883, loss: 2.4682319164276123
step 5884, loss: 2.2302358150482178
step 5885, loss: 2.422029495239258
step 5886, loss: 2.4121456146240234
step 5887, loss: 2.1946604251861572
step 5888, loss: 2.4810726642608643
step 5889, loss: 1.9922373294830322
step 5890, loss: 1.8613290786743164
step 5891, loss: 2.151912212371826
step 5892, loss: 2.4954118728637695
step 5893, loss: 1.9210890531539917
step 5894, loss: 1.518962025642395
step 5895, loss: 1.976574420928955
step 5896, loss: 1.9609448909759521
step 5897, loss: 1.838098406791687
step 5898, loss: 1.9259040355682373
step 5899, loss: 2.2283194065093994
step 5900, loss: 2.038433074951172
step 5901, loss: 1.9730490446090698
step 5902, loss: 2.533811092376709
step 5903, loss: 2.614023447036743
step 5904, loss: 2.118044376373291
step 5905, loss: 2.2148776054382324
step 5906, loss: 2.002934217453003
step 5907, loss: 2.0211522579193115
step 5908, loss: 2.237915277481079
step 5909, loss: 2.184237003326416
step 5910, loss: 2.0652966499328613
step 5911, loss: 1.5837750434875488
step 5912, loss: 2.4741971492767334
step 5913, loss: 1.8496041297912598
step 5914, loss: 2.0525052547454834
step 5915, loss: 1.5859060287475586
step 5916, loss: 2.041813611984253
step 5917, loss: 1.923105001449585
step 5918, loss: 1.4586302042007446
step 5919, loss: 1.5649701356887817
step 5920, loss: 2.6898341178894043
step 5921, loss: 2.099421739578247
step 5922, loss: 1.9533567428588867
step 5923, loss: 1.8036446571350098
step 5924, loss: 1.9088430404663086
step 5925, loss: 2.191572427749634
step 5926, loss: 1.8547314405441284
step 5927, loss: 2.19458270072937
step 5928, loss: 1.683082938194275
step 5929, loss: 2.111065149307251
step 5930, loss: 1.7158886194229126
step 5931, loss: 2.0204946994781494
step 5932, loss: 2.277770757675171
step 5933, loss: 2.3594577312469482
step 5934, loss: 1.9563473463058472
step 5935, loss: 2.3044626712799072
step 5936, loss: 2.1725261211395264
step 5937, loss: 2.1725263595581055
step 5938, loss: 2.0688869953155518
step 5939, loss: 1.8928269147872925
step 5940, loss: 2.469482898712158
step 5941, loss: 2.0724730491638184
step 5942, loss: 2.148834466934204
step 5943, loss: 1.747873067855835
step 5944, loss: 1.7939943075180054
step 5945, loss: 2.234039545059204
step 5946, loss: 1.673405647277832
step 5947, loss: 2.0342886447906494
step 5948, loss: 1.8531861305236816
step 5949, loss: 1.912888526916504
step 5950, loss: 1.9149383306503296
step 5951, loss: 2.4804282188415527
step 5952, loss: 2.1055946350097656
step 5953, loss: 1.5822659730911255
step 5954, loss: 1.7581381797790527
step 5955, loss: 1.542210578918457
step 5956, loss: 1.395116925239563
step 5957, loss: 2.312962770462036
step 5958, loss: 1.9306544065475464
step 5959, loss: 2.103010654449463
step 5960, loss: 2.506345748901367
step 5961, loss: 2.455134153366089
step 5962, loss: 2.2206709384918213
step 5963, loss: 2.061760187149048
step 5964, loss: 1.571808099746704
step 5965, loss: 1.332201600074768
step 5966, loss: 1.4675215482711792
step 5967, loss: 1.5757687091827393
step 5968, loss: 1.9246591329574585
step 5969, loss: 1.4112333059310913
step 5970, loss: 1.8778831958770752
step 5971, loss: 2.0959692001342773
step 5972, loss: 2.7072110176086426
step 5973, loss: 1.8432183265686035
step 5974, loss: 1.638245940208435
step 5975, loss: 2.120771884918213
step 5976, loss: 2.1625919342041016
step 5977, loss: 1.8710378408432007
step 5978, loss: 1.9876459836959839
step 5979, loss: 1.883522391319275
step 5980, loss: 1.7984049320220947
step 5981, loss: 1.72747004032135
step 5982, loss: 1.891609787940979
step 5983, loss: 2.4754018783569336
step 5984, loss: 1.8999024629592896
step 5985, loss: 1.7906566858291626
step 5986, loss: 2.0749311447143555
step 5987, loss: 2.145711898803711
step 5988, loss: 1.8752070665359497
step 5989, loss: 2.0705208778381348
step 5990, loss: 2.221418619155884
step 5991, loss: 2.1231319904327393
step 5992, loss: 2.0551249980926514
step 5993, loss: 1.980711817741394
step 5994, loss: 2.1528851985931396
step 5995, loss: 1.5597490072250366
step 5996, loss: 2.0344507694244385
step 5997, loss: 1.8335232734680176
step 5998, loss: 1.9291582107543945
step 5999, loss: 1.695432186126709
step 6000, loss: 1.5984399318695068
step 6001, loss: 1.4271403551101685
step 6002, loss: 1.6335359811782837
step 6003, loss: 1.9209636449813843
step 6004, loss: 2.319470167160034
step 6005, loss: 2.2793524265289307
step 6006, loss: 2.411639451980591
step 6007, loss: 1.7485603094100952
step 6008, loss: 2.636953592300415
step 6009, loss: 1.9706881046295166
step 6010, loss: 2.081486463546753
step 6011, loss: 1.5351440906524658
step 6012, loss: 1.8711752891540527
step 6013, loss: 2.0814898014068604
step 6014, loss: 2.1238930225372314
step 6015, loss: 2.490316390991211
step 6016, loss: 2.241825819015503
step 6017, loss: 1.7857383489608765
step 6018, loss: 1.6912555694580078
step 6019, loss: 2.007354497909546
step 6020, loss: 2.2483415603637695
step 6021, loss: 1.7141450643539429
step 6022, loss: 1.6251275539398193
step 6023, loss: 2.1556992530822754
step 6024, loss: 2.051419973373413
step 6025, loss: 2.1434972286224365
step 6026, loss: 2.4204933643341064
step 6027, loss: 2.363236665725708
step 6028, loss: 1.9087215662002563
step 6029, loss: 2.119424819946289
step 6030, loss: 1.9817218780517578
step 6031, loss: 2.4074785709381104
step 6032, loss: 2.6799919605255127
step 6033, loss: 2.2569408416748047
step 6034, loss: 2.497580051422119
step 6035, loss: 2.0950071811676025
step 6036, loss: 2.4900264739990234
step 6037, loss: 2.4932308197021484
step 6038, loss: 2.176772117614746
step 6039, loss: 2.6888937950134277
step 6040, loss: 2.393507242202759
step 6041, loss: 2.3656787872314453
step 6042, loss: 2.521131753921509
step 6043, loss: 2.0769920349121094
step 6044, loss: 2.538374662399292
step 6045, loss: 2.3171582221984863
step 6046, loss: 2.3759236335754395
step 6047, loss: 2.3357620239257812
step 6048, loss: 2.6356451511383057
step 6049, loss: 2.289430856704712
step 6050, loss: 2.5271973609924316
step 6051, loss: 2.4542293548583984
step 6052, loss: 2.1795716285705566
step 6053, loss: 2.28547739982605
step 6054, loss: 2.6760082244873047
step 6055, loss: 2.3268351554870605
step 6056, loss: 2.0810940265655518
step 6057, loss: 2.237793207168579
step 6058, loss: 1.7349705696105957
step 6059, loss: 2.3328659534454346
step 6060, loss: 1.8533656597137451
step 6061, loss: 2.1214585304260254
step 6062, loss: 2.2278800010681152
step 6063, loss: 2.0808186531066895
step 6064, loss: 2.2855947017669678
step 6065, loss: 2.0384535789489746
step 6066, loss: 1.601916790008545
step 6067, loss: 2.317222833633423
step 6068, loss: 2.2332613468170166
step 6069, loss: 2.3017425537109375
step 6070, loss: 1.8790720701217651
step 6071, loss: 2.098841667175293
step 6072, loss: 1.7730430364608765
step 6073, loss: 2.209785223007202
step 6074, loss: 1.8019757270812988
step 6075, loss: 1.9633111953735352
step 6076, loss: 1.9155569076538086
step 6077, loss: 2.2158360481262207
step 6078, loss: 2.1641149520874023
step 6079, loss: 2.5513195991516113
step 6080, loss: 1.9983487129211426
step 6081, loss: 1.597815752029419
step 6082, loss: 1.7047510147094727
step 6083, loss: 2.25091552734375
step 6084, loss: 2.10127854347229
step 6085, loss: 2.164060592651367
step 6086, loss: 2.0966074466705322
step 6087, loss: 1.8265661001205444
step 6088, loss: 2.1488640308380127
step 6089, loss: 2.3697824478149414
step 6090, loss: 2.652764081954956
step 6091, loss: 2.4891140460968018
step 6092, loss: 1.9412437677383423
step 6093, loss: 2.2026238441467285
step 6094, loss: 2.3031210899353027
step 6095, loss: 2.577476978302002
step 6096, loss: 2.2589550018310547
step 6097, loss: 2.0810630321502686
step 6098, loss: 2.287755250930786
step 6099, loss: 2.338996410369873
step 6100, loss: 2.132378101348877
step 6101, loss: 2.118622064590454
step 6102, loss: 1.8179035186767578
step 6103, loss: 1.9965474605560303
step 6104, loss: 2.148235559463501
step 6105, loss: 2.56182861328125
step 6106, loss: 2.581228017807007
step 6107, loss: 1.865494728088379
step 6108, loss: 1.8347376585006714
step 6109, loss: 2.09309458732605
step 6110, loss: 2.0387651920318604
step 6111, loss: 2.0574722290039062
step 6112, loss: 2.039599895477295
step 6113, loss: 2.398277521133423
step 6114, loss: 2.186154365539551
step 6115, loss: 2.1092793941497803
step 6116, loss: 1.9423165321350098
step 6117, loss: 2.4206440448760986
step 6118, loss: 2.0773534774780273
step 6119, loss: 1.9751031398773193
step 6120, loss: 2.0687663555145264
step 6121, loss: 2.108077049255371
step 6122, loss: 1.8319236040115356
step 6123, loss: 2.0593643188476562
step 6124, loss: 2.370954751968384
step 6125, loss: 2.317765951156616
step 6126, loss: 2.4919023513793945
step 6127, loss: 2.398646354675293
step 6128, loss: 2.178865671157837
step 6129, loss: 1.7048999071121216
step 6130, loss: 1.64702308177948
step 6131, loss: 2.189249038696289
step 6132, loss: 2.238074779510498
step 6133, loss: 2.2068469524383545
step 6134, loss: 2.4062633514404297
step 6135, loss: 2.30646014213562
step 6136, loss: 2.154125213623047
step 6137, loss: 1.8958170413970947
step 6138, loss: 2.176548957824707
step 6139, loss: 1.6494190692901611
step 6140, loss: 2.544489860534668
step 6141, loss: 1.837315559387207
step 6142, loss: 1.7004777193069458
step 6143, loss: 1.7180827856063843
step 6144, loss: 2.14062237739563
step 6145, loss: 1.738433599472046
step 6146, loss: 2.054734468460083
step 6147, loss: 1.9800059795379639
step 6148, loss: 2.02409029006958
step 6149, loss: 2.300471305847168
step 6150, loss: 2.3245246410369873
step 6151, loss: 1.775689721107483
step 6152, loss: 2.595540761947632
step 6153, loss: 2.131830930709839
step 6154, loss: 2.137998342514038
step 6155, loss: 2.3664634227752686
step 6156, loss: 2.1019463539123535
step 6157, loss: 2.1150827407836914
step 6158, loss: 2.144098997116089
step 6159, loss: 2.6003713607788086
step 6160, loss: 2.43526291847229
step 6161, loss: 2.460542678833008
step 6162, loss: 2.0968809127807617
step 6163, loss: 1.8477879762649536
step 6164, loss: 2.0318663120269775
step 6165, loss: 2.8727378845214844
step 6166, loss: 2.271360397338867
step 6167, loss: 2.1117331981658936
step 6168, loss: 2.351616144180298
step 6169, loss: 2.263840913772583
step 6170, loss: 2.352827787399292
step 6171, loss: 1.91266930103302
step 6172, loss: 2.214125871658325
step 6173, loss: 2.0412166118621826
step 6174, loss: 2.255324602127075
step 6175, loss: 2.0719714164733887
step 6176, loss: 2.2945046424865723
step 6177, loss: 2.1707828044891357
step 6178, loss: 1.5967870950698853
step 6179, loss: 2.240043878555298
step 6180, loss: 2.201266288757324
step 6181, loss: 2.87507963180542
step 6182, loss: 2.023695707321167
step 6183, loss: 2.005572557449341
step 6184, loss: 2.3555564880371094
step 6185, loss: 2.368889093399048
step 6186, loss: 1.8058419227600098
step 6187, loss: 1.8782920837402344
step 6188, loss: 1.8987610340118408
step 6189, loss: 1.735297441482544
step 6190, loss: 2.7027571201324463
step 6191, loss: 2.344372034072876
step 6192, loss: 2.2607555389404297
step 6193, loss: 1.9683672189712524
step 6194, loss: 2.196408987045288
step 6195, loss: 2.1359331607818604
step 6196, loss: 2.227299451828003
step 6197, loss: 2.514878749847412
step 6198, loss: 1.9326883554458618
step 6199, loss: 2.32401442527771
step 6200, loss: 1.79691743850708
step 6201, loss: 2.534792900085449
step 6202, loss: 2.428402900695801
step 6203, loss: 1.9187743663787842
step 6204, loss: 1.8295432329177856
step 6205, loss: 1.9061325788497925
step 6206, loss: 2.2567737102508545
step 6207, loss: 1.7279506921768188
step 6208, loss: 2.1289784908294678
step 6209, loss: 2.130141258239746
step 6210, loss: 2.320545196533203
step 6211, loss: 2.0167486667633057
step 6212, loss: 2.145618438720703
step 6213, loss: 2.303386926651001
step 6214, loss: 2.03426456451416
step 6215, loss: 2.1139490604400635
step 6216, loss: 2.091458320617676
step 6217, loss: 2.4167046546936035
step 6218, loss: 2.025644063949585
step 6219, loss: 1.9382500648498535
step 6220, loss: 2.2920258045196533
step 6221, loss: 2.097635507583618
step 6222, loss: 2.1485695838928223
step 6223, loss: 1.6597115993499756
step 6224, loss: 2.384265661239624
step 6225, loss: 1.7539271116256714
step 6226, loss: 2.1840105056762695
step 6227, loss: 2.220071315765381
step 6228, loss: 1.9236400127410889
step 6229, loss: 1.9616323709487915
step 6230, loss: 2.22516131401062
step 6231, loss: 2.53096079826355
step 6232, loss: 2.5559487342834473
step 6233, loss: 2.3453054428100586
step 6234, loss: 2.093886613845825
step 6235, loss: 1.9453386068344116
step 6236, loss: 2.672675609588623
step 6237, loss: 2.498603343963623
step 6238, loss: 2.120302677154541
step 6239, loss: 1.7138845920562744
step 6240, loss: 2.4478960037231445
step 6241, loss: 2.631681442260742
step 6242, loss: 2.3723690509796143
step 6243, loss: 2.2575247287750244
step 6244, loss: 1.8408502340316772
step 6245, loss: 2.6292271614074707
step 6246, loss: 1.5505956411361694
step 6247, loss: 2.2006983757019043
step 6248, loss: 1.8929920196533203
step 6249, loss: 2.4577813148498535
step 6250, loss: 1.7735209465026855
step 6251, loss: 1.4496289491653442
step 6252, loss: 1.7243555784225464
step 6253, loss: 2.199026584625244
step 6254, loss: 2.5511348247528076
step 6255, loss: 2.6133980751037598
step 6256, loss: 2.35968279838562
step 6257, loss: 2.6027870178222656
step 6258, loss: 2.0973899364471436
step 6259, loss: 2.4374001026153564
step 6260, loss: 1.9278441667556763
step 6261, loss: 2.363300085067749
step 6262, loss: 2.0628929138183594
step 6263, loss: 2.5449795722961426
step 6264, loss: 2.0422110557556152
step 6265, loss: 2.302210807800293
step 6266, loss: 2.033560037612915
step 6267, loss: 2.3775737285614014
step 6268, loss: 2.0314290523529053
step 6269, loss: 1.8371480703353882
step 6270, loss: 1.6436328887939453
step 6271, loss: 1.4929826259613037
step 6272, loss: 1.6319036483764648
step 6273, loss: 1.546180009841919
step 6274, loss: 1.6166657209396362
step 6275, loss: 2.3481075763702393
step 6276, loss: 1.9494470357894897
step 6277, loss: 2.0340371131896973
step 6278, loss: 1.909188985824585
step 6279, loss: 2.098905563354492
step 6280, loss: 1.7583073377609253
step 6281, loss: 1.3347171545028687
step 6282, loss: 1.5570176839828491
step 6283, loss: 1.6657906770706177
step 6284, loss: 2.3602161407470703
step 6285, loss: 2.129896640777588
step 6286, loss: 1.8307063579559326
step 6287, loss: 2.571457862854004
step 6288, loss: 1.8166543245315552
step 6289, loss: 2.1682169437408447
step 6290, loss: 1.8708635568618774
step 6291, loss: 2.0924131870269775
step 6292, loss: 2.1336610317230225
step 6293, loss: 1.6660449504852295
step 6294, loss: 2.2954635620117188
step 6295, loss: 2.1462159156799316
step 6296, loss: 2.2186875343322754
step 6297, loss: 2.1949946880340576
step 6298, loss: 2.51495099067688
step 6299, loss: 2.40297269821167
step 6300, loss: 2.60309100151062
step 6301, loss: 2.556974411010742
step 6302, loss: 2.161520004272461
step 6303, loss: 2.2279129028320312
step 6304, loss: 2.0384340286254883
step 6305, loss: 2.020375967025757
step 6306, loss: 2.0649828910827637
step 6307, loss: 2.4242701530456543
step 6308, loss: 1.9749383926391602
step 6309, loss: 1.5693466663360596
step 6310, loss: 1.7486988306045532
step 6311, loss: 2.053130626678467
step 6312, loss: 2.1439449787139893
step 6313, loss: 2.3316233158111572
step 6314, loss: 2.0599610805511475
step 6315, loss: 2.0101640224456787
step 6316, loss: 2.0068113803863525
step 6317, loss: 1.9274790287017822
step 6318, loss: 1.7529737949371338
step 6319, loss: 1.6369653940200806
step 6320, loss: 1.9923763275146484
step 6321, loss: 2.0366463661193848
step 6322, loss: 1.9686192274093628
step 6323, loss: 2.3089215755462646
step 6324, loss: 2.1554300785064697
step 6325, loss: 2.4437317848205566
step 6326, loss: 1.9698678255081177
step 6327, loss: 2.466783046722412
step 6328, loss: 1.849361538887024
step 6329, loss: 2.1728732585906982
step 6330, loss: 1.924022912979126
step 6331, loss: 1.9823925495147705
step 6332, loss: 2.286100149154663
step 6333, loss: 2.0350053310394287
step 6334, loss: 2.084665298461914
step 6335, loss: 2.0626251697540283
step 6336, loss: 2.2966084480285645
step 6337, loss: 2.178239345550537
step 6338, loss: 2.3997724056243896
step 6339, loss: 2.1369526386260986
step 6340, loss: 2.4812307357788086
step 6341, loss: 2.6277332305908203
step 6342, loss: 2.769674301147461
step 6343, loss: 2.2983016967773438
step 6344, loss: 1.8746726512908936
step 6345, loss: 2.015880584716797
step 6346, loss: 1.8614519834518433
step 6347, loss: 2.0044949054718018
step 6348, loss: 2.5632455348968506
step 6349, loss: 2.2859156131744385
step 6350, loss: 2.0177974700927734
step 6351, loss: 1.8155386447906494
step 6352, loss: 2.430027961730957
step 6353, loss: 2.2349371910095215
step 6354, loss: 2.341109275817871
step 6355, loss: 1.991760015487671
step 6356, loss: 1.9678481817245483
step 6357, loss: 1.9710116386413574
step 6358, loss: 1.9134547710418701
step 6359, loss: 2.3432066440582275
step 6360, loss: 2.1185736656188965
step 6361, loss: 1.8570502996444702
step 6362, loss: 2.233123779296875
step 6363, loss: 2.105682134628296
step 6364, loss: 1.937087059020996
step 6365, loss: 2.5445497035980225
step 6366, loss: 2.161975622177124
step 6367, loss: 1.6445823907852173
step 6368, loss: 2.0201539993286133
step 6369, loss: 2.346874713897705
step 6370, loss: 1.966070532798767
step 6371, loss: 2.2535791397094727
step 6372, loss: 2.0134084224700928
step 6373, loss: 2.118727922439575
step 6374, loss: 2.293022394180298
step 6375, loss: 2.3971877098083496
step 6376, loss: 2.220061779022217
step 6377, loss: 1.9506664276123047
step 6378, loss: 2.065950870513916
step 6379, loss: 2.5181562900543213
step 6380, loss: 2.1572265625
step 6381, loss: 2.4102954864501953
step 6382, loss: 2.3122565746307373
step 6383, loss: 2.671380043029785
step 6384, loss: 2.0766983032226562
step 6385, loss: 1.9645293951034546
step 6386, loss: 2.3515288829803467
step 6387, loss: 2.202333688735962
step 6388, loss: 1.8768068552017212
step 6389, loss: 2.1368634700775146
step 6390, loss: 2.745054244995117
step 6391, loss: 2.070159435272217
step 6392, loss: 2.4745771884918213
step 6393, loss: 2.3854472637176514
step 6394, loss: 2.3392515182495117
step 6395, loss: 2.2648606300354004
step 6396, loss: 2.5463831424713135
step 6397, loss: 2.6456024646759033
step 6398, loss: 2.1652913093566895
step 6399, loss: 2.0734026432037354
step 6400, loss: 2.3204545974731445
step 6401, loss: 1.9303048849105835
step 6402, loss: 2.4188616275787354
step 6403, loss: 2.2567715644836426
step 6404, loss: 2.057955265045166
step 6405, loss: 2.359679698944092
step 6406, loss: 2.436016798019409
step 6407, loss: 1.95972740650177
step 6408, loss: 2.225285768508911
step 6409, loss: 2.0113368034362793
step 6410, loss: 2.275421142578125
step 6411, loss: 2.1354763507843018
step 6412, loss: 2.0113089084625244
step 6413, loss: 1.6923418045043945
step 6414, loss: 2.060476303100586
step 6415, loss: 2.1149728298187256
step 6416, loss: 2.2550110816955566
step 6417, loss: 2.567824363708496
step 6418, loss: 2.705172300338745
step 6419, loss: 2.517399787902832
step 6420, loss: 2.0517561435699463
step 6421, loss: 2.365943193435669
step 6422, loss: 2.2727999687194824
step 6423, loss: 2.6701931953430176
step 6424, loss: 2.0419723987579346
step 6425, loss: 2.5286335945129395
step 6426, loss: 1.7198898792266846
step 6427, loss: 1.9458928108215332
step 6428, loss: 2.3000707626342773
step 6429, loss: 2.250835657119751
step 6430, loss: 2.4045984745025635
step 6431, loss: 2.1930339336395264
step 6432, loss: 1.703735113143921
step 6433, loss: 1.9655481576919556
step 6434, loss: 1.9061262607574463
step 6435, loss: 1.8155654668807983
step 6436, loss: 1.6435102224349976
step 6437, loss: 2.0145440101623535
step 6438, loss: 1.9152604341506958
step 6439, loss: 2.177032232284546
step 6440, loss: 1.6858060359954834
step 6441, loss: 1.8684701919555664
step 6442, loss: 2.4659740924835205
step 6443, loss: 2.4088170528411865
step 6444, loss: 2.233020305633545
step 6445, loss: 1.9398829936981201
step 6446, loss: 2.0140607357025146
step 6447, loss: 2.20920991897583
step 6448, loss: 2.232757806777954
step 6449, loss: 1.8986963033676147
step 6450, loss: 2.519246816635132
step 6451, loss: 2.2761521339416504
step 6452, loss: 2.0316426753997803
step 6453, loss: 1.9661864042282104
step 6454, loss: 2.327052593231201
step 6455, loss: 1.7679293155670166
step 6456, loss: 2.3211734294891357
step 6457, loss: 1.8787935972213745
step 6458, loss: 2.1650428771972656
step 6459, loss: 2.559751033782959
step 6460, loss: 2.2302937507629395
step 6461, loss: 2.5167737007141113
step 6462, loss: 2.106743335723877
step 6463, loss: 2.0143423080444336
step 6464, loss: 1.9545482397079468
step 6465, loss: 1.7869467735290527
step 6466, loss: 1.8406468629837036
step 6467, loss: 2.047239303588867
step 6468, loss: 2.1467463970184326
step 6469, loss: 1.8683319091796875
step 6470, loss: 2.081062078475952
step 6471, loss: 2.05047345161438
step 6472, loss: 1.894659161567688
step 6473, loss: 1.9836221933364868
step 6474, loss: 2.191041946411133
step 6475, loss: 2.072321653366089
step 6476, loss: 1.9271970987319946
step 6477, loss: 1.8925873041152954
step 6478, loss: 1.7099462747573853
step 6479, loss: 2.1574933528900146
step 6480, loss: 2.5117266178131104
step 6481, loss: 2.2393856048583984
step 6482, loss: 2.1679880619049072
step 6483, loss: 2.100889205932617
step 6484, loss: 2.7843616008758545
step 6485, loss: 2.670487642288208
step 6486, loss: 2.3993639945983887
step 6487, loss: 1.9903843402862549
step 6488, loss: 2.0822911262512207
step 6489, loss: 2.527737617492676
step 6490, loss: 2.0853915214538574
step 6491, loss: 1.7532349824905396
step 6492, loss: 2.1323771476745605
step 6493, loss: 2.092747449874878
step 6494, loss: 2.4172937870025635
step 6495, loss: 2.229369878768921
step 6496, loss: 2.1219847202301025
step 6497, loss: 2.086042881011963
step 6498, loss: 2.1677160263061523
step 6499, loss: 1.999218463897705
step 6500, loss: 1.7313429117202759
step 6501, loss: 1.6636857986450195
step 6502, loss: 1.9355838298797607
step 6503, loss: 2.7174270153045654
step 6504, loss: 1.9973231554031372
step 6505, loss: 1.738783597946167
step 6506, loss: 1.8813832998275757
step 6507, loss: 2.287274122238159
step 6508, loss: 1.6899957656860352
step 6509, loss: 1.8036922216415405
step 6510, loss: 1.9780255556106567
step 6511, loss: 2.12372088432312
step 6512, loss: 2.2295432090759277
step 6513, loss: 2.0346293449401855
step 6514, loss: 2.3528974056243896
step 6515, loss: 2.1163933277130127
step 6516, loss: 2.08380126953125
step 6517, loss: 2.494990348815918
step 6518, loss: 2.0546014308929443
step 6519, loss: 2.099599599838257
step 6520, loss: 1.9808224439620972
step 6521, loss: 2.0369043350219727
step 6522, loss: 2.04428768157959
step 6523, loss: 2.0940279960632324
step 6524, loss: 2.114302635192871
step 6525, loss: 2.2117714881896973
step 6526, loss: 1.8679864406585693
step 6527, loss: 1.8407419919967651
step 6528, loss: 2.1467957496643066
step 6529, loss: 1.644174575805664
step 6530, loss: 1.9537159204483032
step 6531, loss: 1.8976175785064697
step 6532, loss: 2.5421042442321777
step 6533, loss: 1.953545331954956
step 6534, loss: 1.877595067024231
step 6535, loss: 1.9975367784500122
step 6536, loss: 2.3404691219329834
step 6537, loss: 2.1903793811798096
step 6538, loss: 2.0737857818603516
step 6539, loss: 1.899793028831482
step 6540, loss: 2.631730079650879
step 6541, loss: 2.133007764816284
step 6542, loss: 2.4570395946502686
step 6543, loss: 2.5657732486724854
step 6544, loss: 1.8220319747924805
step 6545, loss: 1.97001314163208
step 6546, loss: 2.2195160388946533
step 6547, loss: 1.9726167917251587
step 6548, loss: 2.0806732177734375
step 6549, loss: 1.915834665298462
step 6550, loss: 2.1608433723449707
step 6551, loss: 1.4748715162277222
step 6552, loss: 1.853369951248169
step 6553, loss: 1.9374399185180664
step 6554, loss: 1.9563344717025757
step 6555, loss: 2.702739715576172
step 6556, loss: 2.4777169227600098
step 6557, loss: 2.115853786468506
step 6558, loss: 1.9581630229949951
step 6559, loss: 2.4563369750976562
step 6560, loss: 2.3223774433135986
step 6561, loss: 2.3233985900878906
step 6562, loss: 1.9892464876174927
step 6563, loss: 1.7529196739196777
step 6564, loss: 1.9072513580322266
step 6565, loss: 1.890323281288147
step 6566, loss: 1.749089241027832
step 6567, loss: 2.246209144592285
step 6568, loss: 1.8876824378967285
step 6569, loss: 1.9366577863693237
step 6570, loss: 2.4612114429473877
step 6571, loss: 2.4584603309631348
step 6572, loss: 2.3381664752960205
step 6573, loss: 1.9393037557601929
step 6574, loss: 1.7746379375457764
step 6575, loss: 1.9896718263626099
step 6576, loss: 2.0779612064361572
step 6577, loss: 2.452427387237549
step 6578, loss: 1.966984748840332
step 6579, loss: 1.5313405990600586
step 6580, loss: 1.6831612586975098
step 6581, loss: 2.221083641052246
step 6582, loss: 1.9415974617004395
step 6583, loss: 1.8232091665267944
step 6584, loss: 2.1570427417755127
step 6585, loss: 2.221085786819458
step 6586, loss: 1.9028677940368652
step 6587, loss: 2.140577554702759
step 6588, loss: 1.6951160430908203
step 6589, loss: 1.9785428047180176
step 6590, loss: 2.169403314590454
step 6591, loss: 1.6651374101638794
step 6592, loss: 2.2821297645568848
step 6593, loss: 2.4596669673919678
step 6594, loss: 2.115907669067383
step 6595, loss: 1.7295669317245483
step 6596, loss: 2.2651748657226562
step 6597, loss: 2.252204418182373
step 6598, loss: 1.848471760749817
step 6599, loss: 2.1051368713378906
step 6600, loss: 1.8020222187042236
step 6601, loss: 2.1941726207733154
step 6602, loss: 2.054615020751953
step 6603, loss: 2.059565544128418
step 6604, loss: 1.9294151067733765
step 6605, loss: 2.8245890140533447
step 6606, loss: 2.1096572875976562
step 6607, loss: 2.2555792331695557
step 6608, loss: 2.043951988220215
step 6609, loss: 2.6209468841552734
step 6610, loss: 2.2705771923065186
step 6611, loss: 2.2668001651763916
step 6612, loss: 2.3383677005767822
step 6613, loss: 2.551961660385132
step 6614, loss: 2.479016065597534
step 6615, loss: 2.5315120220184326
step 6616, loss: 2.139618396759033
step 6617, loss: 1.6113851070404053
step 6618, loss: 1.9740761518478394
step 6619, loss: 2.2680323123931885
step 6620, loss: 2.485992193222046
step 6621, loss: 2.3766555786132812
step 6622, loss: 2.1539266109466553
step 6623, loss: 2.1827847957611084
step 6624, loss: 1.8099496364593506
step 6625, loss: 2.2759318351745605
step 6626, loss: 2.1689071655273438
step 6627, loss: 2.3586723804473877
step 6628, loss: 1.8057526350021362
step 6629, loss: 2.4623143672943115
step 6630, loss: 2.3544254302978516
step 6631, loss: 2.2114288806915283
step 6632, loss: 1.7405927181243896
step 6633, loss: 1.9662387371063232
step 6634, loss: 2.1142120361328125
step 6635, loss: 2.2183475494384766
step 6636, loss: 2.185577630996704
step 6637, loss: 2.1304612159729004
step 6638, loss: 2.115096092224121
step 6639, loss: 2.4734816551208496
step 6640, loss: 2.235790729522705
step 6641, loss: 2.5327000617980957
step 6642, loss: 2.0728769302368164
step 6643, loss: 2.0811073780059814
step 6644, loss: 2.069324254989624
step 6645, loss: 1.7842077016830444
step 6646, loss: 1.9962997436523438
step 6647, loss: 2.114283323287964
step 6648, loss: 1.6728734970092773
step 6649, loss: 1.8511258363723755
step 6650, loss: 2.308580160140991
step 6651, loss: 1.5958019495010376
step 6652, loss: 1.6758242845535278
step 6653, loss: 1.6368064880371094
step 6654, loss: 1.8774676322937012
step 6655, loss: 1.762598991394043
step 6656, loss: 1.61258864402771
step 6657, loss: 1.7779123783111572
step 6658, loss: 1.8277180194854736
step 6659, loss: 1.6020184755325317
step 6660, loss: 1.5296891927719116
step 6661, loss: 2.2740461826324463
step 6662, loss: 1.6502920389175415
step 6663, loss: 2.5879342555999756
step 6664, loss: 2.0841245651245117
step 6665, loss: 1.569267988204956
step 6666, loss: 1.960318922996521
step 6667, loss: 1.7262204885482788
step 6668, loss: 1.8965961933135986
step 6669, loss: 1.6164952516555786
step 6670, loss: 2.370800733566284
step 6671, loss: 2.462955951690674
step 6672, loss: 2.4162254333496094
step 6673, loss: 2.2154343128204346
step 6674, loss: 1.8795437812805176
step 6675, loss: 1.9314206838607788
step 6676, loss: 1.9488776922225952
step 6677, loss: 2.19159197807312
step 6678, loss: 2.38094162940979
step 6679, loss: 2.1052451133728027
step 6680, loss: 1.9949312210083008
step 6681, loss: 2.004516124725342
step 6682, loss: 2.3372955322265625
step 6683, loss: 2.1553895473480225
step 6684, loss: 2.3989691734313965
step 6685, loss: 2.2777087688446045
step 6686, loss: 2.2355704307556152
step 6687, loss: 1.888146162033081
step 6688, loss: 2.6060025691986084
step 6689, loss: 2.3193790912628174
step 6690, loss: 2.2700061798095703
step 6691, loss: 2.437746286392212
step 6692, loss: 2.1560213565826416
step 6693, loss: 2.7423880100250244
step 6694, loss: 2.1199543476104736
step 6695, loss: 2.367757558822632
step 6696, loss: 2.40824031829834
step 6697, loss: 2.8478026390075684
step 6698, loss: 1.8786747455596924
step 6699, loss: 1.8318672180175781
step 6700, loss: 1.9424549341201782
step 6701, loss: 2.4977359771728516
step 6702, loss: 2.2260468006134033
step 6703, loss: 2.2294185161590576
step 6704, loss: 2.0198891162872314
step 6705, loss: 2.336336612701416
step 6706, loss: 2.2808480262756348
step 6707, loss: 2.326690435409546
step 6708, loss: 2.2233009338378906
step 6709, loss: 2.110987663269043
step 6710, loss: 2.3467090129852295
step 6711, loss: 2.0643818378448486
step 6712, loss: 2.440315008163452
step 6713, loss: 2.135777711868286
step 6714, loss: 1.9519519805908203
step 6715, loss: 2.1787357330322266
step 6716, loss: 2.320107936859131
step 6717, loss: 2.0305888652801514
step 6718, loss: 2.1766412258148193
step 6719, loss: 1.5806654691696167
step 6720, loss: 1.8036364316940308
step 6721, loss: 2.5085928440093994
step 6722, loss: 2.6694118976593018
step 6723, loss: 2.653211832046509
step 6724, loss: 2.5800321102142334
step 6725, loss: 2.0302653312683105
step 6726, loss: 1.977632999420166
step 6727, loss: 1.8200849294662476
step 6728, loss: 1.8355145454406738
step 6729, loss: 2.172520637512207
step 6730, loss: 1.4776983261108398
step 6731, loss: 1.6988035440444946
step 6732, loss: 1.6175038814544678
step 6733, loss: 2.2103724479675293
step 6734, loss: 1.6620709896087646
step 6735, loss: 1.8593546152114868
step 6736, loss: 2.3240246772766113
step 6737, loss: 2.2639999389648438
step 6738, loss: 2.145322799682617
step 6739, loss: 2.03791880607605
step 6740, loss: 2.0314407348632812
step 6741, loss: 2.4538981914520264
step 6742, loss: 1.8980727195739746
step 6743, loss: 2.2682719230651855
step 6744, loss: 2.0434892177581787
step 6745, loss: 1.8174349069595337
step 6746, loss: 1.9851096868515015
step 6747, loss: 2.36514949798584
step 6748, loss: 1.936964750289917
step 6749, loss: 2.1878244876861572
step 6750, loss: 2.361820936203003
step 6751, loss: 2.5294413566589355
step 6752, loss: 2.1389119625091553
step 6753, loss: 2.254526376724243
step 6754, loss: 2.057206630706787
step 6755, loss: 2.1951963901519775
step 6756, loss: 1.8252449035644531
step 6757, loss: 2.1255574226379395
step 6758, loss: 1.9204630851745605
step 6759, loss: 1.8815234899520874
step 6760, loss: 2.3202269077301025
step 6761, loss: 1.9450740814208984
step 6762, loss: 2.0858237743377686
step 6763, loss: 2.102315664291382
step 6764, loss: 2.5122928619384766
step 6765, loss: 2.111130475997925
step 6766, loss: 2.103971242904663
step 6767, loss: 2.1174633502960205
step 6768, loss: 2.418390989303589
step 6769, loss: 2.0773584842681885
step 6770, loss: 2.027768850326538
step 6771, loss: 2.358372926712036
step 6772, loss: 2.4142370223999023
step 6773, loss: 2.1368207931518555
step 6774, loss: 2.2055282592773438
step 6775, loss: 2.428359270095825
step 6776, loss: 2.222115993499756
step 6777, loss: 1.5615720748901367
step 6778, loss: 2.1515843868255615
step 6779, loss: 1.6956522464752197
step 6780, loss: 2.331186056137085
step 6781, loss: 1.8710875511169434
step 6782, loss: 2.0755739212036133
step 6783, loss: 1.6194521188735962
step 6784, loss: 1.487051248550415
step 6785, loss: 1.493572473526001
step 6786, loss: 1.4245129823684692
step 6787, loss: 1.5322095155715942
step 6788, loss: 1.65036141872406
step 6789, loss: 1.49376380443573
step 6790, loss: 1.6402639150619507
step 6791, loss: 1.3138742446899414
step 6792, loss: 1.5872324705123901
step 6793, loss: 1.71396005153656
step 6794, loss: 1.2995272874832153
step 6795, loss: 1.9459068775177002
step 6796, loss: 2.3973145484924316
step 6797, loss: 2.1745030879974365
step 6798, loss: 2.250417709350586
step 6799, loss: 1.9873563051223755
step 6800, loss: 2.0490658283233643
step 6801, loss: 1.751840353012085
step 6802, loss: 1.939156413078308
step 6803, loss: 2.11594557762146
step 6804, loss: 2.152071475982666
step 6805, loss: 2.3499040603637695
step 6806, loss: 1.854729175567627
step 6807, loss: 1.673628330230713
step 6808, loss: 1.8610073328018188
step 6809, loss: 2.6583261489868164
step 6810, loss: 1.9457728862762451
step 6811, loss: 2.4315974712371826
step 6812, loss: 2.356773853302002
step 6813, loss: 1.762740969657898
step 6814, loss: 2.0252246856689453
step 6815, loss: 2.299689292907715
step 6816, loss: 1.9831851720809937
step 6817, loss: 2.393810987472534
step 6818, loss: 2.299283027648926
step 6819, loss: 1.7465825080871582
step 6820, loss: 2.2139744758605957
step 6821, loss: 2.5375919342041016
step 6822, loss: 2.088805913925171
step 6823, loss: 2.054443120956421
step 6824, loss: 1.804328203201294
step 6825, loss: 1.6441059112548828
step 6826, loss: 1.9600324630737305
step 6827, loss: 2.1310346126556396
step 6828, loss: 2.232024669647217
step 6829, loss: 1.9938908815383911
step 6830, loss: 1.787109613418579
step 6831, loss: 1.7465596199035645
step 6832, loss: 1.845167875289917
step 6833, loss: 2.0420989990234375
step 6834, loss: 2.174900531768799
step 6835, loss: 2.153670072555542
step 6836, loss: 2.2783572673797607
step 6837, loss: 2.371434450149536
step 6838, loss: 1.8003711700439453
step 6839, loss: 1.3950861692428589
step 6840, loss: 1.7307153940200806
step 6841, loss: 2.30804181098938
step 6842, loss: 2.1291472911834717
step 6843, loss: 2.2803542613983154
step 6844, loss: 1.732620358467102
step 6845, loss: 2.0391318798065186
step 6846, loss: 2.232846975326538
step 6847, loss: 2.1286208629608154
step 6848, loss: 1.9773904085159302
step 6849, loss: 2.1481897830963135
step 6850, loss: 1.9994432926177979
step 6851, loss: 1.9176750183105469
step 6852, loss: 2.3142759799957275
step 6853, loss: 1.8795013427734375
step 6854, loss: 1.5641095638275146
step 6855, loss: 2.1365838050842285
step 6856, loss: 2.295525074005127
step 6857, loss: 1.9715297222137451
step 6858, loss: 2.0088860988616943
step 6859, loss: 1.711008071899414
step 6860, loss: 1.5675833225250244
step 6861, loss: 1.7575013637542725
step 6862, loss: 2.2580604553222656
step 6863, loss: 2.3349428176879883
step 6864, loss: 2.2405378818511963
step 6865, loss: 1.8013609647750854
step 6866, loss: 2.10917329788208
step 6867, loss: 1.9251521825790405
step 6868, loss: 2.1355721950531006
step 6869, loss: 1.7015690803527832
step 6870, loss: 2.2141153812408447
step 6871, loss: 1.9232062101364136
step 6872, loss: 1.9781244993209839
step 6873, loss: 1.9044890403747559
step 6874, loss: 2.0339455604553223
step 6875, loss: 1.8817939758300781
step 6876, loss: 1.8521591424942017
step 6877, loss: 1.7819466590881348
step 6878, loss: 2.0757837295532227
step 6879, loss: 1.8552659749984741
step 6880, loss: 2.114938497543335
step 6881, loss: 1.5942615270614624
step 6882, loss: 2.1696925163269043
step 6883, loss: 1.7102102041244507
step 6884, loss: 1.5227062702178955
step 6885, loss: 1.9315037727355957
step 6886, loss: 2.138930320739746
step 6887, loss: 1.8480501174926758
step 6888, loss: 1.7424981594085693
step 6889, loss: 1.745261788368225
step 6890, loss: 2.028278112411499
step 6891, loss: 1.968697428703308
step 6892, loss: 1.988134741783142
step 6893, loss: 1.9216375350952148
step 6894, loss: 2.243213176727295
step 6895, loss: 1.8075239658355713
step 6896, loss: 1.6011191606521606
step 6897, loss: 2.4204020500183105
step 6898, loss: 2.3041465282440186
step 6899, loss: 2.1014506816864014
step 6900, loss: 1.6941884756088257
step 6901, loss: 2.104423999786377
step 6902, loss: 2.133357286453247
step 6903, loss: 2.3696017265319824
step 6904, loss: 2.3805315494537354
step 6905, loss: 2.216071128845215
step 6906, loss: 1.9117701053619385
step 6907, loss: 1.8871779441833496
step 6908, loss: 1.697047472000122
step 6909, loss: 2.2889976501464844
step 6910, loss: 2.6742372512817383
step 6911, loss: 2.503619432449341
step 6912, loss: 2.416691303253174
step 6913, loss: 2.211796522140503
step 6914, loss: 1.850942611694336
step 6915, loss: 1.823725700378418
step 6916, loss: 2.1042463779449463
step 6917, loss: 1.7998782396316528
step 6918, loss: 1.8396440744400024
step 6919, loss: 1.982147455215454
step 6920, loss: 1.6277601718902588
step 6921, loss: 2.0314395427703857
step 6922, loss: 1.5313788652420044
step 6923, loss: 1.7473691701889038
step 6924, loss: 2.1694116592407227
step 6925, loss: 1.8377749919891357
step 6926, loss: 2.0005953311920166
step 6927, loss: 1.824919581413269
step 6928, loss: 2.0013091564178467
step 6929, loss: 1.6544420719146729
step 6930, loss: 2.1019020080566406
step 6931, loss: 1.8795156478881836
step 6932, loss: 1.8407983779907227
step 6933, loss: 2.1722891330718994
step 6934, loss: 1.9253418445587158
step 6935, loss: 2.2929065227508545
step 6936, loss: 2.28753662109375
step 6937, loss: 2.5789332389831543
step 6938, loss: 2.0512747764587402
step 6939, loss: 2.173680543899536
step 6940, loss: 1.6796839237213135
step 6941, loss: 1.8221763372421265
step 6942, loss: 2.2238504886627197
step 6943, loss: 2.406562328338623
step 6944, loss: 2.2355947494506836
step 6945, loss: 2.1222739219665527
step 6946, loss: 2.291280746459961
step 6947, loss: 2.501957416534424
step 6948, loss: 2.0373878479003906
step 6949, loss: 2.5586326122283936
step 6950, loss: 2.418067455291748
step 6951, loss: 2.6305084228515625
step 6952, loss: 2.583442211151123
step 6953, loss: 2.368891477584839
step 6954, loss: 2.3547306060791016
step 6955, loss: 2.4134647846221924
step 6956, loss: 2.694884777069092
step 6957, loss: 2.0827040672302246
step 6958, loss: 2.5976152420043945
step 6959, loss: 2.2051572799682617
step 6960, loss: 2.3204662799835205
step 6961, loss: 2.8859431743621826
step 6962, loss: 2.411094903945923
step 6963, loss: 2.431424856185913
step 6964, loss: 2.3387222290039062
step 6965, loss: 2.6579253673553467
step 6966, loss: 2.5826237201690674
step 6967, loss: 2.8677735328674316
step 6968, loss: 2.904764413833618
step 6969, loss: 2.4043335914611816
step 6970, loss: 2.2332184314727783
step 6971, loss: 2.578855037689209
step 6972, loss: 1.9953328371047974
step 6973, loss: 2.693180799484253
step 6974, loss: 2.446319341659546
step 6975, loss: 2.6366937160491943
step 6976, loss: 2.3333723545074463
step 6977, loss: 2.5473151206970215
step 6978, loss: 2.4388742446899414
step 6979, loss: 1.986721396446228
step 6980, loss: 2.397324800491333
step 6981, loss: 2.1423377990722656
step 6982, loss: 2.5485761165618896
step 6983, loss: 2.2412421703338623
step 6984, loss: 1.9982545375823975
step 6985, loss: 2.4164135456085205
step 6986, loss: 1.9224889278411865
step 6987, loss: 2.187760829925537
step 6988, loss: 2.1553478240966797
step 6989, loss: 2.136627197265625
step 6990, loss: 2.1263747215270996
step 6991, loss: 1.6899075508117676
step 6992, loss: 2.4048104286193848
step 6993, loss: 1.933611273765564
step 6994, loss: 2.4548802375793457
step 6995, loss: 2.7310545444488525
step 6996, loss: 2.0179810523986816
step 6997, loss: 1.8903765678405762
step 6998, loss: 1.727889895439148
step 6999, loss: 2.329988479614258
step 7000, loss: 2.0091521739959717
step 7001, loss: 2.2360687255859375
step 7002, loss: 2.683345317840576
step 7003, loss: 2.1747348308563232
step 7004, loss: 2.1860597133636475
step 7005, loss: 2.8594000339508057
step 7006, loss: 2.235877513885498
step 7007, loss: 2.791560649871826
step 7008, loss: 2.406423568725586
step 7009, loss: 2.6427459716796875
step 7010, loss: 2.6486423015594482
step 7011, loss: 2.6028871536254883
step 7012, loss: 2.233142852783203
step 7013, loss: 2.7286536693573
step 7014, loss: 2.191331386566162
step 7015, loss: 2.602895736694336
step 7016, loss: 2.219109296798706
step 7017, loss: 2.486081600189209
step 7018, loss: 2.5805368423461914
step 7019, loss: 2.3969762325286865
step 7020, loss: 2.3359861373901367
step 7021, loss: 2.484476327896118
step 7022, loss: 2.383301019668579
step 7023, loss: 2.5741939544677734
step 7024, loss: 2.435573101043701
step 7025, loss: 2.2374393939971924
step 7026, loss: 2.380025625228882
step 7027, loss: 2.361788511276245
step 7028, loss: 2.6217963695526123
step 7029, loss: 2.29577898979187
step 7030, loss: 1.534450888633728
step 7031, loss: 2.0822243690490723
step 7032, loss: 2.209303379058838
step 7033, loss: 2.335905075073242
step 7034, loss: 2.141801118850708
step 7035, loss: 2.2607314586639404
step 7036, loss: 2.3327832221984863
step 7037, loss: 2.6030845642089844
step 7038, loss: 2.2463791370391846
step 7039, loss: 2.1977360248565674
step 7040, loss: 2.3500165939331055
step 7041, loss: 2.5699594020843506
step 7042, loss: 2.174788236618042
step 7043, loss: 2.498647928237915
step 7044, loss: 2.210057497024536
step 7045, loss: 2.5815987586975098
step 7046, loss: 2.3856961727142334
step 7047, loss: 2.2213008403778076
step 7048, loss: 2.1411986351013184
step 7049, loss: 2.318068504333496
step 7050, loss: 2.66990065574646
step 7051, loss: 2.1218807697296143
step 7052, loss: 2.3668646812438965
step 7053, loss: 2.546792984008789
step 7054, loss: 2.378610610961914
step 7055, loss: 2.212440013885498
step 7056, loss: 2.8444888591766357
step 7057, loss: 2.143226146697998
step 7058, loss: 2.369121551513672
step 7059, loss: 2.4554240703582764
step 7060, loss: 2.660836935043335
step 7061, loss: 2.138780355453491
step 7062, loss: 1.7625746726989746
step 7063, loss: 1.7404799461364746
step 7064, loss: 2.1917355060577393
step 7065, loss: 2.6259548664093018
step 7066, loss: 2.6069822311401367
step 7067, loss: 2.6095163822174072
step 7068, loss: 2.766829013824463
step 7069, loss: 2.3233389854431152
step 7070, loss: 2.347006320953369
step 7071, loss: 2.2052712440490723
step 7072, loss: 2.550541400909424
step 7073, loss: 2.315169334411621
step 7074, loss: 2.492194175720215
step 7075, loss: 2.3086109161376953
step 7076, loss: 2.6044085025787354
step 7077, loss: 2.4812076091766357
step 7078, loss: 2.870610475540161
step 7079, loss: 2.556065559387207
step 7080, loss: 2.5025177001953125
step 7081, loss: 2.43753719329834
step 7082, loss: 2.1456849575042725
step 7083, loss: 2.3177385330200195
step 7084, loss: 2.2522213459014893
step 7085, loss: 2.310776710510254
step 7086, loss: 2.2991995811462402
step 7087, loss: 2.4371602535247803
step 7088, loss: 2.6080307960510254
step 7089, loss: 2.6474997997283936
step 7090, loss: 2.59277606010437
step 7091, loss: 2.2744619846343994
step 7092, loss: 2.4240972995758057
step 7093, loss: 2.5819637775421143
step 7094, loss: 2.185107469558716
step 7095, loss: 2.0294463634490967
step 7096, loss: 1.9950783252716064
step 7097, loss: 2.258209466934204
step 7098, loss: 2.542121648788452
step 7099, loss: 2.0281662940979004
step 7100, loss: 2.155797004699707
step 7101, loss: 2.000720977783203
step 7102, loss: 2.0845346450805664
step 7103, loss: 1.8079971075057983
step 7104, loss: 1.7299424409866333
step 7105, loss: 2.1157374382019043
step 7106, loss: 2.215299129486084
step 7107, loss: 2.006108522415161
step 7108, loss: 1.8265784978866577
step 7109, loss: 1.8419029712677002
step 7110, loss: 2.3244900703430176
step 7111, loss: 2.459186553955078
step 7112, loss: 2.6810996532440186
step 7113, loss: 2.3262059688568115
step 7114, loss: 2.486401081085205
step 7115, loss: 2.433666467666626
step 7116, loss: 2.4097588062286377
step 7117, loss: 2.141373872756958
step 7118, loss: 2.4856081008911133
step 7119, loss: 2.1662256717681885
step 7120, loss: 2.540228843688965
step 7121, loss: 2.7072103023529053
step 7122, loss: 2.4938247203826904
step 7123, loss: 2.044490337371826
step 7124, loss: 2.5964598655700684
step 7125, loss: 2.4710161685943604
step 7126, loss: 1.9625704288482666
step 7127, loss: 1.9633347988128662
step 7128, loss: 2.5803842544555664
step 7129, loss: 2.335902452468872
step 7130, loss: 2.5759224891662598
step 7131, loss: 1.9493389129638672
step 7132, loss: 2.094434976577759
step 7133, loss: 1.8887385129928589
step 7134, loss: 1.942613124847412
step 7135, loss: 1.8132673501968384
step 7136, loss: 1.883034348487854
step 7137, loss: 2.126570224761963
step 7138, loss: 2.066269874572754
step 7139, loss: 2.0895111560821533
step 7140, loss: 1.7932661771774292
step 7141, loss: 1.6965194940567017
step 7142, loss: 1.5047247409820557
step 7143, loss: 2.232738494873047
step 7144, loss: 2.082273244857788
step 7145, loss: 2.06933856010437
step 7146, loss: 2.3735053539276123
step 7147, loss: 2.4135403633117676
step 7148, loss: 2.536221981048584
step 7149, loss: 2.409666061401367
step 7150, loss: 2.2160964012145996
step 7151, loss: 2.004136562347412
step 7152, loss: 2.128694534301758
step 7153, loss: 2.618481397628784
step 7154, loss: 1.617884874343872
step 7155, loss: 2.416574239730835
step 7156, loss: 2.7468254566192627
step 7157, loss: 2.634783983230591
step 7158, loss: 2.3752224445343018
step 7159, loss: 2.5734143257141113
step 7160, loss: 2.3759329319000244
step 7161, loss: 2.123323678970337
step 7162, loss: 2.579831123352051
step 7163, loss: 2.710465431213379
step 7164, loss: 2.3116025924682617
step 7165, loss: 2.0487117767333984
step 7166, loss: 2.410403251647949
step 7167, loss: 2.122342824935913
step 7168, loss: 2.3530468940734863
step 7169, loss: 2.183912515640259
step 7170, loss: 2.315215587615967
step 7171, loss: 2.1456151008605957
step 7172, loss: 2.026095151901245
step 7173, loss: 1.9833893775939941
step 7174, loss: 2.3599693775177
step 7175, loss: 2.522481918334961
step 7176, loss: 1.9202888011932373
step 7177, loss: 1.8306701183319092
step 7178, loss: 2.274500608444214
step 7179, loss: 1.9065916538238525
step 7180, loss: 1.9958407878875732
step 7181, loss: 1.5332733392715454
step 7182, loss: 2.475966215133667
step 7183, loss: 2.242184638977051
step 7184, loss: 2.1643824577331543
step 7185, loss: 2.01774263381958
step 7186, loss: 1.848979115486145
step 7187, loss: 2.026947021484375
step 7188, loss: 2.3219258785247803
step 7189, loss: 2.0396673679351807
step 7190, loss: 1.881333827972412
step 7191, loss: 1.9539059400558472
step 7192, loss: 2.3272111415863037
step 7193, loss: 2.309398889541626
step 7194, loss: 2.333843231201172
step 7195, loss: 2.3482918739318848
step 7196, loss: 1.909557819366455
step 7197, loss: 1.925681710243225
step 7198, loss: 2.4061334133148193
step 7199, loss: 2.347616195678711
step 7200, loss: 2.7266221046447754
step 7201, loss: 2.684701442718506
step 7202, loss: 2.632786989212036
step 7203, loss: 2.624053478240967
step 7204, loss: 2.4456839561462402
step 7205, loss: 2.6010239124298096
step 7206, loss: 2.427947759628296
step 7207, loss: 1.972892165184021
step 7208, loss: 2.636296510696411
step 7209, loss: 2.2960710525512695
step 7210, loss: 2.552548885345459
step 7211, loss: 2.3116233348846436
step 7212, loss: 2.376763105392456
step 7213, loss: 2.869924545288086
step 7214, loss: 2.9622058868408203
step 7215, loss: 2.664201021194458
step 7216, loss: 2.5908203125
step 7217, loss: 2.598313331604004
step 7218, loss: 2.7049975395202637
step 7219, loss: 2.7738966941833496
step 7220, loss: 2.1087942123413086
step 7221, loss: 1.765228033065796
step 7222, loss: 2.7340850830078125
step 7223, loss: 2.091125726699829
step 7224, loss: 2.3798611164093018
step 7225, loss: 2.4548802375793457
step 7226, loss: 2.5166025161743164
step 7227, loss: 2.408203125
step 7228, loss: 2.4370360374450684
step 7229, loss: 2.530778408050537
step 7230, loss: 2.4435136318206787
step 7231, loss: 2.230607748031616
step 7232, loss: 2.494232416152954
step 7233, loss: 2.437413215637207
step 7234, loss: 2.591052532196045
step 7235, loss: 2.3187549114227295
step 7236, loss: 2.363682270050049
step 7237, loss: 2.5623135566711426
step 7238, loss: 2.0481131076812744
step 7239, loss: 1.7902284860610962
step 7240, loss: 1.6657780408859253
step 7241, loss: 2.1862823963165283
step 7242, loss: 2.249117612838745
step 7243, loss: 2.4857547283172607
step 7244, loss: 2.726667881011963
step 7245, loss: 2.2776405811309814
step 7246, loss: 2.507664203643799
step 7247, loss: 2.2444028854370117
step 7248, loss: 2.140430450439453
step 7249, loss: 2.395592451095581
step 7250, loss: 2.227780342102051
step 7251, loss: 2.172684669494629
step 7252, loss: 2.3220365047454834
step 7253, loss: 2.8545584678649902
step 7254, loss: 1.690789818763733
step 7255, loss: 2.287168025970459
step 7256, loss: 2.58013916015625
step 7257, loss: 2.3383312225341797
step 7258, loss: 2.299771785736084
step 7259, loss: 2.4041521549224854
step 7260, loss: 2.7619102001190186
step 7261, loss: 2.181112051010132
step 7262, loss: 2.536616086959839
step 7263, loss: 2.3282060623168945
step 7264, loss: 2.1144652366638184
step 7265, loss: 2.815593957901001
step 7266, loss: 2.2606053352355957
step 7267, loss: 2.016373872756958
step 7268, loss: 2.5075504779815674
step 7269, loss: 2.022991418838501
step 7270, loss: 2.357166051864624
step 7271, loss: 1.8977891206741333
step 7272, loss: 1.9963207244873047
step 7273, loss: 1.8230254650115967
step 7274, loss: 1.9322340488433838
step 7275, loss: 1.9495997428894043
step 7276, loss: 1.7347644567489624
step 7277, loss: 2.0580525398254395
step 7278, loss: 2.1393849849700928
step 7279, loss: 2.226111650466919
step 7280, loss: 2.627120018005371
step 7281, loss: 1.7981997728347778
step 7282, loss: 2.6656320095062256
step 7283, loss: 2.4595751762390137
step 7284, loss: 2.244704246520996
step 7285, loss: 2.923948049545288
step 7286, loss: 1.9745458364486694
step 7287, loss: 2.3032777309417725
step 7288, loss: 2.0444953441619873
step 7289, loss: 2.4887301921844482
step 7290, loss: 2.515643835067749
step 7291, loss: 2.6316604614257812
step 7292, loss: 2.233255624771118
step 7293, loss: 2.367720365524292
step 7294, loss: 2.244396924972534
step 7295, loss: 2.480512857437134
step 7296, loss: 2.4501309394836426
step 7297, loss: 1.8999272584915161
step 7298, loss: 2.497438430786133
step 7299, loss: 2.0185108184814453
step 7300, loss: 2.2237699031829834
step 7301, loss: 1.6738290786743164
step 7302, loss: 2.491373062133789
step 7303, loss: 2.42676043510437
step 7304, loss: 2.434776782989502
step 7305, loss: 2.0278964042663574
step 7306, loss: 2.1295900344848633
step 7307, loss: 1.877655267715454
step 7308, loss: 1.9852588176727295
step 7309, loss: 1.9126437902450562
step 7310, loss: 1.7344088554382324
step 7311, loss: 2.169219732284546
step 7312, loss: 2.1820168495178223
step 7313, loss: 1.9505187273025513
step 7314, loss: 1.8758362531661987
step 7315, loss: 1.9818047285079956
step 7316, loss: 1.8910616636276245
step 7317, loss: 1.4389854669570923
step 7318, loss: 1.5809414386749268
step 7319, loss: 1.728097915649414
step 7320, loss: 1.9367761611938477
step 7321, loss: 2.047133684158325
step 7322, loss: 1.7877562046051025
step 7323, loss: 1.584838628768921
step 7324, loss: 1.3807802200317383
step 7325, loss: 1.8810869455337524
step 7326, loss: 1.3926206827163696
step 7327, loss: 1.8474628925323486
step 7328, loss: 2.1852519512176514
step 7329, loss: 1.649990200996399
step 7330, loss: 1.7682559490203857
step 7331, loss: 1.5037802457809448
step 7332, loss: 2.179182767868042
step 7333, loss: 2.1510303020477295
step 7334, loss: 2.257981061935425
step 7335, loss: 1.7129994630813599
step 7336, loss: 1.7121118307113647
step 7337, loss: 1.7759326696395874
step 7338, loss: 1.6884818077087402
step 7339, loss: 2.2001450061798096
step 7340, loss: 1.8177037239074707
step 7341, loss: 2.5055203437805176
step 7342, loss: 2.033172369003296
step 7343, loss: 2.0669898986816406
step 7344, loss: 2.1509344577789307
step 7345, loss: 2.601013660430908
step 7346, loss: 1.8873698711395264
step 7347, loss: 2.1223952770233154
step 7348, loss: 1.701996922492981
step 7349, loss: 1.391585111618042
step 7350, loss: 2.6514601707458496
step 7351, loss: 2.2042481899261475
step 7352, loss: 2.2248318195343018
step 7353, loss: 2.205439805984497
step 7354, loss: 1.630168080329895
step 7355, loss: 1.5541605949401855
step 7356, loss: 2.055562973022461
step 7357, loss: 2.1992838382720947
step 7358, loss: 2.834582567214966
step 7359, loss: 2.1780965328216553
step 7360, loss: 2.018584966659546
step 7361, loss: 1.8582806587219238
step 7362, loss: 2.4246273040771484
step 7363, loss: 1.896222710609436
step 7364, loss: 1.9084967374801636
step 7365, loss: 1.884653091430664
step 7366, loss: 1.9304897785186768
step 7367, loss: 2.1890032291412354
step 7368, loss: 1.8521007299423218
step 7369, loss: 1.866533875465393
step 7370, loss: 2.038513422012329
step 7371, loss: 1.9842190742492676
step 7372, loss: 1.5719538927078247
step 7373, loss: 2.367532968521118
step 7374, loss: 2.3798325061798096
step 7375, loss: 2.4811925888061523
step 7376, loss: 2.376938581466675
step 7377, loss: 2.3784077167510986
step 7378, loss: 2.2482361793518066
step 7379, loss: 2.5141429901123047
step 7380, loss: 2.2987730503082275
step 7381, loss: 1.9973299503326416
step 7382, loss: 1.9773188829421997
step 7383, loss: 1.6812617778778076
step 7384, loss: 1.8565171957015991
step 7385, loss: 2.1568026542663574
step 7386, loss: 2.366269588470459
step 7387, loss: 1.9204821586608887
step 7388, loss: 1.7391637563705444
step 7389, loss: 2.1388962268829346
step 7390, loss: 2.233919382095337
step 7391, loss: 2.2519402503967285
step 7392, loss: 1.8329943418502808
step 7393, loss: 1.7686482667922974
step 7394, loss: 2.191471576690674
step 7395, loss: 1.9784585237503052
step 7396, loss: 2.2372124195098877
step 7397, loss: 2.490912914276123
step 7398, loss: 2.487928867340088
step 7399, loss: 2.5052998065948486
step 7400, loss: 2.3323864936828613
step 7401, loss: 2.4092049598693848
step 7402, loss: 2.178670883178711
step 7403, loss: 2.6460680961608887
step 7404, loss: 2.2639870643615723
step 7405, loss: 2.045598030090332
step 7406, loss: 2.203674077987671
step 7407, loss: 2.2696871757507324
step 7408, loss: 2.554933547973633
step 7409, loss: 2.3155462741851807
step 7410, loss: 2.295090675354004
step 7411, loss: 2.0290262699127197
step 7412, loss: 1.8728410005569458
step 7413, loss: 2.2476389408111572
step 7414, loss: 1.81974196434021
step 7415, loss: 1.6098859310150146
step 7416, loss: 1.995172142982483
step 7417, loss: 1.8291802406311035
step 7418, loss: 2.2242865562438965
step 7419, loss: 1.8003242015838623
step 7420, loss: 2.092891216278076
step 7421, loss: 2.0379626750946045
step 7422, loss: 2.005671262741089
step 7423, loss: 2.088224172592163
step 7424, loss: 1.8214284181594849
step 7425, loss: 2.070298910140991
step 7426, loss: 2.437056303024292
step 7427, loss: 2.2250237464904785
step 7428, loss: 2.0082309246063232
step 7429, loss: 1.8873835802078247
step 7430, loss: 2.2226619720458984
step 7431, loss: 1.9437423944473267
step 7432, loss: 2.5151562690734863
step 7433, loss: 2.190049409866333
step 7434, loss: 2.6673169136047363
step 7435, loss: 3.0902774333953857
step 7436, loss: 2.7211508750915527
step 7437, loss: 2.195051670074463
step 7438, loss: 2.010263204574585
step 7439, loss: 2.4329140186309814
step 7440, loss: 1.8262804746627808
step 7441, loss: 2.328301191329956
step 7442, loss: 1.9946280717849731
step 7443, loss: 2.0350632667541504
step 7444, loss: 2.2463252544403076
step 7445, loss: 1.9029830694198608
step 7446, loss: 2.2985122203826904
step 7447, loss: 2.025880813598633
step 7448, loss: 1.8970394134521484
step 7449, loss: 2.284682035446167
step 7450, loss: 2.199237108230591
step 7451, loss: 1.4543719291687012
step 7452, loss: 2.3875534534454346
step 7453, loss: 2.014253616333008
step 7454, loss: 2.233689785003662
step 7455, loss: 1.9074701070785522
step 7456, loss: 2.3100695610046387
step 7457, loss: 2.237114429473877
step 7458, loss: 2.2096455097198486
step 7459, loss: 1.8990365266799927
step 7460, loss: 2.2598884105682373
step 7461, loss: 1.8575860261917114
step 7462, loss: 2.0834858417510986
step 7463, loss: 2.248164415359497
step 7464, loss: 2.23174786567688
step 7465, loss: 2.302457094192505
step 7466, loss: 2.1899306774139404
step 7467, loss: 1.7248271703720093
step 7468, loss: 1.5236759185791016
step 7469, loss: 1.7399826049804688
step 7470, loss: 1.9368118047714233
step 7471, loss: 1.6450177431106567
step 7472, loss: 2.0963199138641357
step 7473, loss: 2.0322659015655518
step 7474, loss: 2.0477957725524902
step 7475, loss: 1.839911937713623
step 7476, loss: 1.7171390056610107
step 7477, loss: 1.5049411058425903
step 7478, loss: 2.430189847946167
step 7479, loss: 1.9910435676574707
step 7480, loss: 2.0826542377471924
step 7481, loss: 1.9682108163833618
step 7482, loss: 1.486139178276062
step 7483, loss: 1.7466583251953125
step 7484, loss: 2.109769582748413
step 7485, loss: 1.8876187801361084
step 7486, loss: 2.5844178199768066
step 7487, loss: 2.4869651794433594
step 7488, loss: 2.160210609436035
step 7489, loss: 2.2145578861236572
step 7490, loss: 1.9466400146484375
step 7491, loss: 2.0160470008850098
step 7492, loss: 2.586886405944824
step 7493, loss: 2.3551928997039795
step 7494, loss: 1.9475785493850708
step 7495, loss: 1.671847939491272
step 7496, loss: 2.333935499191284
step 7497, loss: 2.1265532970428467
step 7498, loss: 2.248338460922241
step 7499, loss: 1.9157812595367432
step 7500, loss: 1.8989670276641846
step 7501, loss: 1.9035383462905884
step 7502, loss: 2.2062270641326904
step 7503, loss: 2.3160645961761475
step 7504, loss: 2.6445746421813965
step 7505, loss: 1.8843995332717896
step 7506, loss: 2.1828813552856445
step 7507, loss: 2.12625789642334
step 7508, loss: 2.6503968238830566
step 7509, loss: 2.1730518341064453
step 7510, loss: 1.2551021575927734
step 7511, loss: 1.7242273092269897
step 7512, loss: 2.0607385635375977
step 7513, loss: 1.9457814693450928
step 7514, loss: 2.019334554672241
step 7515, loss: 2.3010330200195312
step 7516, loss: 2.538919448852539
step 7517, loss: 2.2734591960906982
step 7518, loss: 2.043895721435547
step 7519, loss: 2.329411506652832
step 7520, loss: 1.8041768074035645
step 7521, loss: 1.833281397819519
step 7522, loss: 1.7393546104431152
step 7523, loss: 2.338874101638794
step 7524, loss: 2.188272476196289
step 7525, loss: 2.2746567726135254
step 7526, loss: 1.8247569799423218
step 7527, loss: 1.784921646118164
step 7528, loss: 1.8663408756256104
step 7529, loss: 1.7492084503173828
step 7530, loss: 1.990396499633789
step 7531, loss: 2.0585596561431885
step 7532, loss: 1.8883154392242432
step 7533, loss: 2.427863359451294
step 7534, loss: 2.433835744857788
step 7535, loss: 2.656399726867676
step 7536, loss: 2.399570941925049
step 7537, loss: 1.7299214601516724
step 7538, loss: 2.250417709350586
step 7539, loss: 2.178297758102417
step 7540, loss: 1.905795931816101
step 7541, loss: 1.7792677879333496
step 7542, loss: 2.0121877193450928
step 7543, loss: 1.809031367301941
step 7544, loss: 2.037665605545044
step 7545, loss: 1.9004096984863281
step 7546, loss: 1.8111947774887085
step 7547, loss: 2.341038465499878
step 7548, loss: 2.2338101863861084
step 7549, loss: 2.572922706604004
step 7550, loss: 2.5167887210845947
step 7551, loss: 1.986379861831665
step 7552, loss: 2.2591497898101807
step 7553, loss: 2.504741907119751
step 7554, loss: 2.1113483905792236
step 7555, loss: 2.412229061126709
step 7556, loss: 2.00158429145813
step 7557, loss: 2.2475438117980957
step 7558, loss: 2.3691394329071045
step 7559, loss: 2.241595983505249
step 7560, loss: 2.347062826156616
step 7561, loss: 2.153991937637329
step 7562, loss: 1.5807883739471436
step 7563, loss: 2.116046905517578
step 7564, loss: 1.9674190282821655
step 7565, loss: 2.393526315689087
step 7566, loss: 2.2778332233428955
step 7567, loss: 1.704163908958435
step 7568, loss: 1.9964691400527954
step 7569, loss: 2.08610200881958
step 7570, loss: 2.0854640007019043
step 7571, loss: 1.8391482830047607
step 7572, loss: 1.8199340105056763
step 7573, loss: 2.019195795059204
step 7574, loss: 2.3396942615509033
step 7575, loss: 1.9679205417633057
step 7576, loss: 1.8608187437057495
step 7577, loss: 2.5265376567840576
step 7578, loss: 2.1637277603149414
step 7579, loss: 2.3487298488616943
step 7580, loss: 2.064316511154175
step 7581, loss: 2.0230283737182617
step 7582, loss: 2.2842469215393066
step 7583, loss: 2.1588714122772217
step 7584, loss: 2.1627418994903564
step 7585, loss: 2.094250202178955
step 7586, loss: 2.593855381011963
step 7587, loss: 2.1588573455810547
step 7588, loss: 2.1540510654449463
step 7589, loss: 2.3324339389801025
step 7590, loss: 2.259763240814209
step 7591, loss: 2.051678419113159
step 7592, loss: 2.018671989440918
step 7593, loss: 2.4665656089782715
step 7594, loss: 1.9436782598495483
step 7595, loss: 2.3741419315338135
step 7596, loss: 1.6668154001235962
step 7597, loss: 2.063945770263672
step 7598, loss: 2.1981284618377686
step 7599, loss: 1.8497211933135986
step 7600, loss: 2.2145121097564697
step 7601, loss: 1.8631528615951538
step 7602, loss: 1.8170440196990967
step 7603, loss: 1.9025683403015137
step 7604, loss: 1.7407331466674805
step 7605, loss: 1.8826342821121216
step 7606, loss: 1.5924546718597412
step 7607, loss: 2.0727126598358154
step 7608, loss: 1.8376317024230957
step 7609, loss: 1.9145225286483765
step 7610, loss: 1.8983923196792603
step 7611, loss: 1.9942220449447632
step 7612, loss: 1.7926663160324097
step 7613, loss: 1.77937912940979
step 7614, loss: 1.4804726839065552
step 7615, loss: 2.1087324619293213
step 7616, loss: 2.1533236503601074
step 7617, loss: 1.7294429540634155
step 7618, loss: 1.8889247179031372
step 7619, loss: 2.175502061843872
step 7620, loss: 1.5606553554534912
step 7621, loss: 1.9259659051895142
step 7622, loss: 1.9486905336380005
step 7623, loss: 1.7777254581451416
step 7624, loss: 1.849859595298767
step 7625, loss: 1.6927088499069214
step 7626, loss: 1.740864634513855
step 7627, loss: 1.8311796188354492
step 7628, loss: 1.6477489471435547
step 7629, loss: 1.5371614694595337
step 7630, loss: 2.360743761062622
step 7631, loss: 2.0536398887634277
step 7632, loss: 1.8711166381835938
step 7633, loss: 1.9731262922286987
step 7634, loss: 1.974669098854065
step 7635, loss: 1.9126447439193726
step 7636, loss: 2.0173521041870117
step 7637, loss: 1.9606822729110718
step 7638, loss: 1.9847747087478638
step 7639, loss: 1.394246220588684
step 7640, loss: 1.9021282196044922
step 7641, loss: 2.003084897994995
step 7642, loss: 1.6807883977890015
step 7643, loss: 1.897877812385559
step 7644, loss: 1.9407950639724731
step 7645, loss: 1.946897268295288
step 7646, loss: 2.1999216079711914
step 7647, loss: 1.7001655101776123
step 7648, loss: 2.0073893070220947
step 7649, loss: 1.8206682205200195
step 7650, loss: 1.489974856376648
step 7651, loss: 2.087005853652954
step 7652, loss: 1.6721876859664917
step 7653, loss: 1.914157748222351
step 7654, loss: 2.1027157306671143
step 7655, loss: 2.0518293380737305
step 7656, loss: 1.392937183380127
step 7657, loss: 1.7034695148468018
step 7658, loss: 1.7651221752166748
step 7659, loss: 1.619430661201477
step 7660, loss: 1.4247515201568604
step 7661, loss: 1.3165080547332764
step 7662, loss: 1.5528454780578613
step 7663, loss: 2.096144914627075
step 7664, loss: 1.822520136833191
step 7665, loss: 1.6702488660812378
step 7666, loss: 2.095391035079956
step 7667, loss: 1.5341291427612305
step 7668, loss: 1.8981239795684814
step 7669, loss: 1.7558225393295288
step 7670, loss: 2.351975440979004
step 7671, loss: 1.8112270832061768
step 7672, loss: 1.896422028541565
step 7673, loss: 1.997301697731018
step 7674, loss: 1.8678169250488281
step 7675, loss: 1.8585753440856934
step 7676, loss: 2.027266025543213
step 7677, loss: 1.9159687757492065
step 7678, loss: 1.9022700786590576
step 7679, loss: 2.0381760597229004
step 7680, loss: 2.0668439865112305
step 7681, loss: 2.121896505355835
step 7682, loss: 2.0035312175750732
step 7683, loss: 2.1467583179473877
step 7684, loss: 1.5843243598937988
step 7685, loss: 2.1599528789520264
step 7686, loss: 1.5831907987594604
step 7687, loss: 2.133018970489502
step 7688, loss: 2.1573398113250732
step 7689, loss: 1.921802043914795
step 7690, loss: 1.9484306573867798
step 7691, loss: 2.328929901123047
step 7692, loss: 2.2737624645233154
step 7693, loss: 2.070441484451294
step 7694, loss: 1.4882925748825073
step 7695, loss: 1.9190905094146729
step 7696, loss: 2.483266592025757
step 7697, loss: 2.3914570808410645
step 7698, loss: 1.6345051527023315
step 7699, loss: 1.7944819927215576
step 7700, loss: 1.7239738702774048
step 7701, loss: 2.075731039047241
step 7702, loss: 2.0661990642547607
step 7703, loss: 2.2354702949523926
step 7704, loss: 2.1896495819091797
step 7705, loss: 2.2106359004974365
step 7706, loss: 1.768824815750122
step 7707, loss: 1.9104490280151367
step 7708, loss: 2.3437559604644775
step 7709, loss: 2.1552488803863525
step 7710, loss: 2.1553800106048584
step 7711, loss: 2.0825257301330566
step 7712, loss: 1.2906842231750488
step 7713, loss: 1.806329607963562
step 7714, loss: 2.007030487060547
step 7715, loss: 2.0966341495513916
step 7716, loss: 2.393162250518799
step 7717, loss: 1.463750958442688
step 7718, loss: 1.822824478149414
step 7719, loss: 2.195298910140991
step 7720, loss: 1.8601022958755493
step 7721, loss: 2.142180919647217
step 7722, loss: 2.0158379077911377
step 7723, loss: 1.7596362829208374
step 7724, loss: 1.6355879306793213
step 7725, loss: 1.8692318201065063
step 7726, loss: 1.7090920209884644
step 7727, loss: 1.9583436250686646
step 7728, loss: 1.7404812574386597
step 7729, loss: 1.5786991119384766
step 7730, loss: 1.8569753170013428
step 7731, loss: 1.677841067314148
step 7732, loss: 1.9044437408447266
step 7733, loss: 2.438629627227783
step 7734, loss: 2.273780107498169
step 7735, loss: 1.8309199810028076
step 7736, loss: 2.026172161102295
step 7737, loss: 2.212310314178467
step 7738, loss: 1.6826611757278442
step 7739, loss: 2.2754759788513184
step 7740, loss: 2.38498592376709
step 7741, loss: 1.971609354019165
step 7742, loss: 2.3483939170837402
step 7743, loss: 2.2804629802703857
step 7744, loss: 2.0906224250793457
step 7745, loss: 2.087916374206543
step 7746, loss: 2.0830845832824707
step 7747, loss: 1.8467954397201538
step 7748, loss: 2.1571710109710693
step 7749, loss: 2.1680266857147217
step 7750, loss: 1.9015614986419678
step 7751, loss: 2.0776712894439697
step 7752, loss: 1.7406411170959473
step 7753, loss: 2.3239541053771973
step 7754, loss: 2.0556037425994873
step 7755, loss: 2.3055126667022705
step 7756, loss: 1.7687199115753174
step 7757, loss: 1.6460750102996826
step 7758, loss: 1.9038764238357544
step 7759, loss: 2.0787365436553955
step 7760, loss: 1.646836280822754
step 7761, loss: 1.7567315101623535
step 7762, loss: 2.0713586807250977
step 7763, loss: 1.9877434968948364
step 7764, loss: 2.0253138542175293
step 7765, loss: 1.8710681200027466
step 7766, loss: 1.9401473999023438
step 7767, loss: 1.7717031240463257
step 7768, loss: 1.8663181066513062
step 7769, loss: 1.550992727279663
step 7770, loss: 1.8100018501281738
step 7771, loss: 1.3866944313049316
step 7772, loss: 1.580928087234497
step 7773, loss: 2.281493902206421
step 7774, loss: 1.832337498664856
step 7775, loss: 1.8925713300704956
step 7776, loss: 1.6896485090255737
step 7777, loss: 2.0736680030822754
step 7778, loss: 2.5430469512939453
step 7779, loss: 2.479780673980713
step 7780, loss: 2.106490135192871
step 7781, loss: 2.0859642028808594
step 7782, loss: 1.6687270402908325
step 7783, loss: 1.6937693357467651
step 7784, loss: 1.702457070350647
step 7785, loss: 1.980846881866455
step 7786, loss: 2.2136342525482178
step 7787, loss: 1.4625014066696167
step 7788, loss: 1.4609489440917969
step 7789, loss: 2.0214312076568604
step 7790, loss: 2.1622724533081055
step 7791, loss: 1.9938342571258545
step 7792, loss: 2.0000197887420654
step 7793, loss: 1.761638879776001
step 7794, loss: 2.045898199081421
step 7795, loss: 1.9491685628890991
step 7796, loss: 1.8444205522537231
step 7797, loss: 1.9550784826278687
step 7798, loss: 1.7971385717391968
step 7799, loss: 1.8613770008087158
step 7800, loss: 1.7156745195388794
step 7801, loss: 1.407781958580017
step 7802, loss: 1.842040777206421
step 7803, loss: 1.95574152469635
step 7804, loss: 2.1116690635681152
step 7805, loss: 1.3685232400894165
step 7806, loss: 1.5450949668884277
step 7807, loss: 1.8021565675735474
step 7808, loss: 1.6443372964859009
step 7809, loss: 1.506805658340454
step 7810, loss: 1.5837355852127075
step 7811, loss: 2.074345588684082
step 7812, loss: 1.847362756729126
step 7813, loss: 1.5624083280563354
step 7814, loss: 1.7272839546203613
step 7815, loss: 1.8439042568206787
step 7816, loss: 1.6723469495773315
step 7817, loss: 1.8183737993240356
step 7818, loss: 1.7505487203598022
step 7819, loss: 1.7245198488235474
step 7820, loss: 1.5138896703720093
step 7821, loss: 1.4883968830108643
step 7822, loss: 1.5346945524215698
step 7823, loss: 1.8997094631195068
step 7824, loss: 1.3990662097930908
step 7825, loss: 1.4842000007629395
step 7826, loss: 1.6860665082931519
step 7827, loss: 2.154655933380127
step 7828, loss: 1.5972353219985962
step 7829, loss: 2.1316258907318115
step 7830, loss: 2.1415092945098877
step 7831, loss: 1.996423602104187
step 7832, loss: 2.1036911010742188
step 7833, loss: 1.7615989446640015
step 7834, loss: 1.7698835134506226
step 7835, loss: 2.462655782699585
step 7836, loss: 2.2864224910736084
step 7837, loss: 2.4733998775482178
step 7838, loss: 2.3509843349456787
step 7839, loss: 1.9812430143356323
step 7840, loss: 2.142054319381714
step 7841, loss: 2.0085270404815674
step 7842, loss: 2.6246728897094727
step 7843, loss: 2.202664375305176
step 7844, loss: 2.56901216506958
step 7845, loss: 2.430473804473877
step 7846, loss: 2.2542741298675537
step 7847, loss: 1.9668480157852173
step 7848, loss: 1.7801634073257446
step 7849, loss: 2.431516408920288
step 7850, loss: 2.4329068660736084
step 7851, loss: 2.444734811782837
step 7852, loss: 2.9066572189331055
step 7853, loss: 2.416396141052246
step 7854, loss: 2.2215332984924316
step 7855, loss: 2.572033166885376
step 7856, loss: 2.242940902709961
step 7857, loss: 2.4787437915802
step 7858, loss: 2.3778505325317383
step 7859, loss: 2.4777307510375977
step 7860, loss: 2.3085827827453613
step 7861, loss: 2.542633295059204
step 7862, loss: 2.272617816925049
step 7863, loss: 2.404264211654663
step 7864, loss: 2.376594066619873
step 7865, loss: 2.0362746715545654
step 7866, loss: 2.113487720489502
step 7867, loss: 2.127737045288086
step 7868, loss: 1.9021646976470947
step 7869, loss: 2.065361738204956
step 7870, loss: 1.7163825035095215
step 7871, loss: 2.1392598152160645
step 7872, loss: 2.106078624725342
step 7873, loss: 2.2348601818084717
step 7874, loss: 1.8905609846115112
step 7875, loss: 2.0057871341705322
step 7876, loss: 2.0493555068969727
step 7877, loss: 2.3989524841308594
step 7878, loss: 2.2212886810302734
step 7879, loss: 2.2660422325134277
step 7880, loss: 2.1745808124542236
step 7881, loss: 2.622079610824585
step 7882, loss: 2.2494900226593018
step 7883, loss: 2.3964459896087646
step 7884, loss: 2.3521170616149902
step 7885, loss: 2.2590789794921875
step 7886, loss: 2.1370954513549805
step 7887, loss: 2.2602181434631348
step 7888, loss: 1.9399287700653076
step 7889, loss: 1.922971248626709
step 7890, loss: 2.3059182167053223
step 7891, loss: 2.1018340587615967
step 7892, loss: 2.3946847915649414
step 7893, loss: 1.9606633186340332
step 7894, loss: 2.2954177856445312
step 7895, loss: 2.2829558849334717
step 7896, loss: 2.137207269668579
step 7897, loss: 1.890373945236206
step 7898, loss: 1.8875731229782104
step 7899, loss: 1.6178046464920044
step 7900, loss: 1.7176486253738403
step 7901, loss: 1.8749347925186157
step 7902, loss: 1.6188288927078247
step 7903, loss: 1.7865087985992432
step 7904, loss: 1.9543710947036743
step 7905, loss: 1.804365634918213
step 7906, loss: 1.8092750310897827
step 7907, loss: 1.8581985235214233
step 7908, loss: 2.349597930908203
step 7909, loss: 2.360461711883545
step 7910, loss: 2.3560047149658203
step 7911, loss: 1.6826913356781006
step 7912, loss: 1.7195461988449097
step 7913, loss: 2.0301389694213867
step 7914, loss: 1.8581138849258423
step 7915, loss: 1.6291714906692505
step 7916, loss: 1.8400930166244507
step 7917, loss: 1.785725474357605
step 7918, loss: 1.9473296403884888
step 7919, loss: 2.2683844566345215
step 7920, loss: 1.7198092937469482
step 7921, loss: 2.179189920425415
step 7922, loss: 1.8710635900497437
step 7923, loss: 2.325542688369751
step 7924, loss: 1.964376449584961
step 7925, loss: 1.8050615787506104
step 7926, loss: 2.288468837738037
step 7927, loss: 1.7047488689422607
step 7928, loss: 1.8591783046722412
step 7929, loss: 2.1033828258514404
step 7930, loss: 2.182030439376831
step 7931, loss: 1.8778400421142578
step 7932, loss: 1.8818409442901611
step 7933, loss: 2.080458641052246
step 7934, loss: 1.9339059591293335
step 7935, loss: 1.849403738975525
step 7936, loss: 1.802243709564209
step 7937, loss: 2.1625235080718994
step 7938, loss: 2.010526418685913
step 7939, loss: 1.8210548162460327
step 7940, loss: 1.7415368556976318
step 7941, loss: 1.6983089447021484
step 7942, loss: 1.517206072807312
step 7943, loss: 1.794965147972107
step 7944, loss: 1.5663069486618042
step 7945, loss: 1.5975593328475952
step 7946, loss: 1.4764597415924072
step 7947, loss: 1.7608027458190918
step 7948, loss: 1.806499719619751
step 7949, loss: 1.7614188194274902
step 7950, loss: 1.8543031215667725
step 7951, loss: 1.6000645160675049
step 7952, loss: 1.838588833808899
step 7953, loss: 1.1761780977249146
step 7954, loss: 2.0499460697174072
step 7955, loss: 1.7627854347229004
step 7956, loss: 1.4688987731933594
step 7957, loss: 1.9558379650115967
step 7958, loss: 1.5238112211227417
step 7959, loss: 1.5789461135864258
step 7960, loss: 1.7630764245986938
step 7961, loss: 1.4130183458328247
step 7962, loss: 1.698892593383789
step 7963, loss: 1.689807653427124
step 7964, loss: 1.5961179733276367
step 7965, loss: 1.7921273708343506
step 7966, loss: 1.4753319025039673
step 7967, loss: 1.4930059909820557
step 7968, loss: 1.747655987739563
step 7969, loss: 1.8211604356765747
step 7970, loss: 1.6653432846069336
step 7971, loss: 1.7865618467330933
step 7972, loss: 1.4972294569015503
step 7973, loss: 1.6185111999511719
step 7974, loss: 1.7830677032470703
step 7975, loss: 1.815887212753296
step 7976, loss: 1.4475338459014893
step 7977, loss: 1.6520761251449585
step 7978, loss: 1.3101966381072998
step 7979, loss: 1.4783196449279785
step 7980, loss: 1.4757035970687866
step 7981, loss: 1.2690320014953613
step 7982, loss: 1.590852975845337
step 7983, loss: 1.2137879133224487
step 7984, loss: 1.3468888998031616
step 7985, loss: 1.732498288154602
step 7986, loss: 1.4935181140899658
step 7987, loss: 1.5337176322937012
step 7988, loss: 1.4398517608642578
step 7989, loss: 1.4174562692642212
step 7990, loss: 1.501721978187561
step 7991, loss: 1.43866765499115
step 7992, loss: 1.5529149770736694
step 7993, loss: 1.4769854545593262
step 7994, loss: 1.714686393737793
step 7995, loss: 1.9414498805999756
step 7996, loss: 1.2753950357437134
step 7997, loss: 1.314546823501587
step 7998, loss: 1.3591039180755615
step 7999, loss: 1.3178863525390625
step 8000, loss: 1.5415024757385254
step 8001, loss: 1.3497518301010132
step 8002, loss: 1.4618070125579834
step 8003, loss: 1.087113380432129
step 8004, loss: 1.3380717039108276
step 8005, loss: 1.5166548490524292
step 8006, loss: 1.4111450910568237
step 8007, loss: 1.4013267755508423
step 8008, loss: 1.4187206029891968
step 8009, loss: 1.823282241821289
step 8010, loss: 1.2599852085113525
step 8011, loss: 1.2781167030334473
step 8012, loss: 1.359319806098938
step 8013, loss: 1.3841828107833862
step 8014, loss: 1.4010188579559326
step 8015, loss: 1.314624309539795
step 8016, loss: 1.3662029504776
step 8017, loss: 1.3040460348129272
step 8018, loss: 1.1999800205230713
step 8019, loss: 1.2993417978286743
step 8020, loss: 1.6591917276382446
step 8021, loss: 1.3486469984054565
step 8022, loss: 1.7524898052215576
step 8023, loss: 1.790541648864746
step 8024, loss: 1.3148515224456787
step 8025, loss: 1.3131544589996338
step 8026, loss: 1.507236361503601
step 8027, loss: 1.5958126783370972
step 8028, loss: 1.448560357093811
step 8029, loss: 1.670454740524292
step 8030, loss: 1.4828159809112549
step 8031, loss: 1.4483747482299805
step 8032, loss: 1.5526514053344727
step 8033, loss: 1.822461724281311
step 8034, loss: 1.649070382118225
step 8035, loss: 1.606263279914856
step 8036, loss: 1.413621187210083
step 8037, loss: 1.4501700401306152
step 8038, loss: 1.4200800657272339
step 8039, loss: 1.7287895679473877
step 8040, loss: 1.5696889162063599
step 8041, loss: 1.5964033603668213
step 8042, loss: 1.5993584394454956
step 8043, loss: 1.759964108467102
step 8044, loss: 1.8734146356582642
step 8045, loss: 1.3939149379730225
step 8046, loss: 1.554650068283081
step 8047, loss: 1.5894091129302979
step 8048, loss: 1.5092703104019165
step 8049, loss: 1.565423846244812
step 8050, loss: 1.5337518453598022
step 8051, loss: 1.45561683177948
step 8052, loss: 1.49346125125885
step 8053, loss: 1.7257435321807861
step 8054, loss: 1.5241482257843018
step 8055, loss: 1.7908731698989868
step 8056, loss: 1.4901114702224731
step 8057, loss: 1.4663397073745728
step 8058, loss: 1.573279857635498
step 8059, loss: 1.4567564725875854
step 8060, loss: 1.6749653816223145
step 8061, loss: 1.561982274055481
step 8062, loss: 1.9017082452774048
step 8063, loss: 1.3161888122558594
step 8064, loss: 1.1446484327316284
step 8065, loss: 1.4207113981246948
step 8066, loss: 1.3471481800079346
step 8067, loss: 1.5547229051589966
step 8068, loss: 1.8765976428985596
step 8069, loss: 1.6324430704116821
step 8070, loss: 1.6360223293304443
step 8071, loss: 1.636003851890564
step 8072, loss: 1.5166324377059937
step 8073, loss: 1.401813268661499
step 8074, loss: 1.5278407335281372
step 8075, loss: 1.6117825508117676
step 8076, loss: 1.7357515096664429
step 8077, loss: 1.4752600193023682
step 8078, loss: 1.263485074043274
step 8079, loss: 1.5143771171569824
step 8080, loss: 1.1571357250213623
step 8081, loss: 1.4265761375427246
step 8082, loss: 1.4993016719818115
step 8083, loss: 1.160707712173462
step 8084, loss: 1.476327896118164
step 8085, loss: 1.786884069442749
step 8086, loss: 1.6376243829727173
step 8087, loss: 1.4025449752807617
step 8088, loss: 1.371692180633545
step 8089, loss: 1.4872499704360962
step 8090, loss: 1.689387321472168
step 8091, loss: 1.3226956129074097
step 8092, loss: 1.5128110647201538
step 8093, loss: 1.4068807363510132
step 8094, loss: 1.5000228881835938
step 8095, loss: 1.7512069940567017
step 8096, loss: 1.3253765106201172
step 8097, loss: 1.49978506565094
step 8098, loss: 1.2942171096801758
step 8099, loss: 1.2557473182678223
step 8100, loss: 1.5737401247024536
step 8101, loss: 1.1517415046691895
step 8102, loss: 1.2534377574920654
step 8103, loss: 1.3727059364318848
step 8104, loss: 1.3892567157745361
step 8105, loss: 1.2829252481460571
step 8106, loss: 1.1357009410858154
step 8107, loss: 1.3567360639572144
step 8108, loss: 1.5838960409164429
step 8109, loss: 1.23297917842865
step 8110, loss: 1.355005145072937
step 8111, loss: 1.048184871673584
Saved best model with loss: 1.0482 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.0482.pt
step 8112, loss: 1.1859678030014038
step 8113, loss: 1.4764143228530884
step 8114, loss: 1.1772420406341553
step 8115, loss: 1.4999704360961914
step 8116, loss: 1.1943373680114746
step 8117, loss: 1.3070354461669922
step 8118, loss: 1.3328564167022705
step 8119, loss: 1.4708393812179565
step 8120, loss: 1.2087846994400024
step 8121, loss: 1.1041139364242554
step 8122, loss: 1.4962849617004395
step 8123, loss: 1.2135151624679565
step 8124, loss: 1.732375979423523
step 8125, loss: 1.857188105583191
step 8126, loss: 1.4318175315856934
step 8127, loss: 1.677132487297058
step 8128, loss: 1.240893840789795
step 8129, loss: 1.2745331525802612
step 8130, loss: 2.034602165222168
step 8131, loss: 1.7717233896255493
step 8132, loss: 1.5992302894592285
step 8133, loss: 1.249715805053711
step 8134, loss: 1.5243651866912842
step 8135, loss: 1.5077491998672485
step 8136, loss: 1.4610501527786255
step 8137, loss: 1.357302188873291
step 8138, loss: 1.3594589233398438
step 8139, loss: 1.510298728942871
step 8140, loss: 1.4846895933151245
step 8141, loss: 1.30903160572052
step 8142, loss: 1.38925302028656
step 8143, loss: 1.4915540218353271
step 8144, loss: 1.3169976472854614
step 8145, loss: 1.4258909225463867
step 8146, loss: 1.4404175281524658
step 8147, loss: 1.614105463027954
step 8148, loss: 1.261842966079712
step 8149, loss: 1.6444307565689087
step 8150, loss: 1.5229519605636597
step 8151, loss: 1.2461354732513428
step 8152, loss: 1.667514443397522
step 8153, loss: 1.301259160041809
step 8154, loss: 1.6390838623046875
step 8155, loss: 1.6149911880493164
step 8156, loss: 1.6512863636016846
step 8157, loss: 1.1925535202026367
step 8158, loss: 1.5047247409820557
step 8159, loss: 1.5443949699401855
step 8160, loss: 1.2844569683074951
step 8161, loss: 1.403781771659851
step 8162, loss: 1.4281437397003174
step 8163, loss: 1.3558913469314575
step 8164, loss: 1.4876383543014526
step 8165, loss: 1.383352518081665
step 8166, loss: 1.4861561059951782
step 8167, loss: 1.4508708715438843
step 8168, loss: 1.5047392845153809
step 8169, loss: 1.5748741626739502
step 8170, loss: 1.3355860710144043
step 8171, loss: 1.5443577766418457
step 8172, loss: 1.3451895713806152
step 8173, loss: 1.3973405361175537
step 8174, loss: 1.3288733959197998
step 8175, loss: 1.3684827089309692
step 8176, loss: 1.3107144832611084
step 8177, loss: 1.3186683654785156
step 8178, loss: 1.3736965656280518
step 8179, loss: 1.1314587593078613
step 8180, loss: 1.593340516090393
step 8181, loss: 1.4495618343353271
step 8182, loss: 1.5744141340255737
step 8183, loss: 1.5704271793365479
step 8184, loss: 1.4616659879684448
step 8185, loss: 1.1254957914352417
step 8186, loss: 1.18203866481781
step 8187, loss: 1.8202918767929077
step 8188, loss: 1.484473705291748
step 8189, loss: 1.4534934759140015
step 8190, loss: 1.267736554145813
step 8191, loss: 1.2396864891052246
step 8192, loss: 1.4724197387695312
step 8193, loss: 1.4517122507095337
step 8194, loss: 1.3378366231918335
step 8195, loss: 1.1829115152359009
step 8196, loss: 1.389617919921875
step 8197, loss: 1.5204051733016968
step 8198, loss: 1.2120401859283447
step 8199, loss: 1.0764838457107544
step 8200, loss: 1.4083781242370605
step 8201, loss: 1.1327259540557861
step 8202, loss: 1.0902432203292847
step 8203, loss: 1.3469206094741821
step 8204, loss: 1.281822919845581
step 8205, loss: 0.9988200068473816
Saved best model with loss: 0.9988 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.9988.pt
step 8206, loss: 1.1507844924926758
step 8207, loss: 1.3379267454147339
step 8208, loss: 1.5213165283203125
step 8209, loss: 1.3786752223968506
step 8210, loss: 1.4960741996765137
step 8211, loss: 1.47915518283844
step 8212, loss: 1.0548644065856934
step 8213, loss: 1.214439034461975
step 8214, loss: 1.4488729238510132
step 8215, loss: 1.1548590660095215
step 8216, loss: 1.2753757238388062
step 8217, loss: 1.522493839263916
step 8218, loss: 1.2235791683197021
step 8219, loss: 1.2060052156448364
step 8220, loss: 1.6419179439544678
step 8221, loss: 1.4903322458267212
step 8222, loss: 1.3607865571975708
step 8223, loss: 1.610230565071106
step 8224, loss: 1.2034028768539429
step 8225, loss: 1.278134822845459
step 8226, loss: 1.4134199619293213
step 8227, loss: 1.6172728538513184
step 8228, loss: 1.2908337116241455
step 8229, loss: 1.684902310371399
step 8230, loss: 1.4701873064041138
step 8231, loss: 1.5027953386306763
step 8232, loss: 1.5621975660324097
step 8233, loss: 1.523340106010437
step 8234, loss: 1.4911690950393677
step 8235, loss: 1.441697359085083
step 8236, loss: 1.708734154701233
step 8237, loss: 1.7085086107254028
step 8238, loss: 1.226077914237976
step 8239, loss: 1.702954649925232
step 8240, loss: 1.3111072778701782
step 8241, loss: 1.7966803312301636
step 8242, loss: 1.354629397392273
step 8243, loss: 1.4281666278839111
step 8244, loss: 1.970752239227295
step 8245, loss: 1.4680854082107544
step 8246, loss: 2.023346185684204
step 8247, loss: 1.527262568473816
step 8248, loss: 1.3786324262619019
step 8249, loss: 1.5313938856124878
step 8250, loss: 1.4367966651916504
step 8251, loss: 1.4535775184631348
step 8252, loss: 2.0750813484191895
step 8253, loss: 1.6802641153335571
step 8254, loss: 1.5645595788955688
step 8255, loss: 1.461227297782898
step 8256, loss: 1.227796196937561
step 8257, loss: 1.3172765970230103
step 8258, loss: 1.4800610542297363
step 8259, loss: 1.7933154106140137
step 8260, loss: 1.5513063669204712
step 8261, loss: 1.3773857355117798
step 8262, loss: 1.787577509880066
step 8263, loss: 1.1439707279205322
step 8264, loss: 1.382165551185608
step 8265, loss: 1.4605746269226074
step 8266, loss: 1.4820127487182617
step 8267, loss: 1.2877414226531982
step 8268, loss: 1.3035558462142944
step 8269, loss: 1.5596991777420044
step 8270, loss: 1.4441936016082764
step 8271, loss: 1.50031578540802
step 8272, loss: 1.3323817253112793
step 8273, loss: 1.5806623697280884
step 8274, loss: 1.2346627712249756
step 8275, loss: 1.5667724609375
step 8276, loss: 1.2748737335205078
step 8277, loss: 1.4050016403198242
step 8278, loss: 1.305849552154541
step 8279, loss: 1.2526555061340332
step 8280, loss: 1.3653069734573364
step 8281, loss: 1.1987338066101074
step 8282, loss: 1.462892770767212
step 8283, loss: 1.5281651020050049
step 8284, loss: 1.4648730754852295
step 8285, loss: 1.2140308618545532
step 8286, loss: 1.4492125511169434
step 8287, loss: 1.6138534545898438
step 8288, loss: 1.343680739402771
step 8289, loss: 1.2275166511535645
step 8290, loss: 1.531407117843628
step 8291, loss: 1.3763378858566284
step 8292, loss: 1.3204256296157837
step 8293, loss: 1.7723459005355835
step 8294, loss: 1.5129519701004028
step 8295, loss: 1.5769051313400269
step 8296, loss: 1.627586841583252
step 8297, loss: 1.614000678062439
step 8298, loss: 1.5500237941741943
step 8299, loss: 1.534515619277954
step 8300, loss: 1.3250854015350342
step 8301, loss: 1.4914549589157104
step 8302, loss: 1.2856802940368652
step 8303, loss: 1.6329442262649536
step 8304, loss: 1.3648641109466553
step 8305, loss: 1.1478177309036255
step 8306, loss: 1.4226688146591187
step 8307, loss: 1.1422715187072754
step 8308, loss: 1.274242877960205
step 8309, loss: 1.2603667974472046
step 8310, loss: 1.3151334524154663
step 8311, loss: 1.1269618272781372
step 8312, loss: 1.1577473878860474
step 8313, loss: 0.9726876020431519
Saved best model with loss: 0.9727 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.9727.pt
step 8314, loss: 1.1302051544189453
step 8315, loss: 1.1343235969543457
step 8316, loss: 1.1169941425323486
step 8317, loss: 1.069867730140686
step 8318, loss: 1.3487322330474854
step 8319, loss: 0.9807682037353516
step 8320, loss: 0.7430791854858398
Saved best model with loss: 0.7431 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7431.pt
step 8321, loss: 0.8099549412727356
step 8322, loss: 1.0884554386138916
step 8323, loss: 1.5269811153411865
step 8324, loss: 1.4033539295196533
step 8325, loss: 1.206337809562683
step 8326, loss: 1.001189112663269
step 8327, loss: 0.8527968525886536
step 8328, loss: 1.0256961584091187
step 8329, loss: 1.0141065120697021
step 8330, loss: 1.0027304887771606
step 8331, loss: 1.463356614112854
step 8332, loss: 1.6655116081237793
step 8333, loss: 1.467610478401184
step 8334, loss: 1.4157179594039917
step 8335, loss: 0.9627601504325867
step 8336, loss: 1.3546682596206665
step 8337, loss: 1.0452775955200195
step 8338, loss: 1.3292200565338135
step 8339, loss: 1.4520983695983887
step 8340, loss: 1.3462070226669312
step 8341, loss: 1.529128074645996
step 8342, loss: 1.4982047080993652
step 8343, loss: 1.2241737842559814
step 8344, loss: 1.5265761613845825
step 8345, loss: 1.1766157150268555
step 8346, loss: 1.6913751363754272
step 8347, loss: 1.0946943759918213
step 8348, loss: 1.3983769416809082
step 8349, loss: 1.4144686460494995
step 8350, loss: 1.3980273008346558
step 8351, loss: 1.5969150066375732
step 8352, loss: 1.370084285736084
step 8353, loss: 1.3531183004379272
step 8354, loss: 1.317214012145996
step 8355, loss: 1.4984232187271118
step 8356, loss: 1.4503698348999023
step 8357, loss: 1.3873159885406494
step 8358, loss: 1.1640303134918213
step 8359, loss: 1.0730741024017334
step 8360, loss: 1.301334023475647
step 8361, loss: 1.400705337524414
step 8362, loss: 1.2812491655349731
step 8363, loss: 1.152815341949463
step 8364, loss: 1.5992010831832886
step 8365, loss: 1.32291579246521
step 8366, loss: 1.3040690422058105
step 8367, loss: 1.1200872659683228
step 8368, loss: 1.1334720849990845
step 8369, loss: 1.3927853107452393
step 8370, loss: 1.4739210605621338
step 8371, loss: 1.6306865215301514
step 8372, loss: 1.267652153968811
step 8373, loss: 1.2543847560882568
step 8374, loss: 1.1969205141067505
step 8375, loss: 1.1305800676345825
step 8376, loss: 1.2079259157180786
step 8377, loss: 1.1671637296676636
step 8378, loss: 1.3620802164077759
step 8379, loss: 1.2392823696136475
step 8380, loss: 1.490573525428772
step 8381, loss: 1.4861054420471191
step 8382, loss: 1.30099618434906
step 8383, loss: 1.2531369924545288
step 8384, loss: 1.003367304801941
step 8385, loss: 1.0958937406539917
step 8386, loss: 1.1738126277923584
step 8387, loss: 1.2563291788101196
step 8388, loss: 1.3358683586120605
step 8389, loss: 1.283172607421875
step 8390, loss: 1.1785025596618652
step 8391, loss: 1.36491060256958
step 8392, loss: 1.2876754999160767
step 8393, loss: 1.4808070659637451
step 8394, loss: 1.338816523551941
step 8395, loss: 1.717078447341919
step 8396, loss: 1.5863440036773682
step 8397, loss: 1.5910447835922241
step 8398, loss: 1.440932035446167
step 8399, loss: 1.4453318119049072
step 8400, loss: 1.2750465869903564
step 8401, loss: 1.759932518005371
step 8402, loss: 1.5942829847335815
step 8403, loss: 1.5934885740280151
step 8404, loss: 1.6126084327697754
step 8405, loss: 1.4395129680633545
step 8406, loss: 1.57169508934021
step 8407, loss: 1.3825318813323975
step 8408, loss: 1.5094654560089111
step 8409, loss: 1.5848040580749512
step 8410, loss: 1.404045820236206
step 8411, loss: 1.4369723796844482
step 8412, loss: 1.4108513593673706
step 8413, loss: 1.4160305261611938
step 8414, loss: 1.4840154647827148
step 8415, loss: 1.4600818157196045
step 8416, loss: 1.6936033964157104
step 8417, loss: 1.3450031280517578
step 8418, loss: 1.4664603471755981
step 8419, loss: 1.5119179487228394
step 8420, loss: 1.5181089639663696
step 8421, loss: 1.5992839336395264
step 8422, loss: 1.6011581420898438
step 8423, loss: 1.5272812843322754
step 8424, loss: 1.2869608402252197
step 8425, loss: 1.0950390100479126
step 8426, loss: 0.9336673617362976
step 8427, loss: 1.4788998365402222
step 8428, loss: 1.3911820650100708
step 8429, loss: 1.2886253595352173
step 8430, loss: 1.5067633390426636
step 8431, loss: 1.4319185018539429
step 8432, loss: 0.9894706606864929
step 8433, loss: 1.5502761602401733
step 8434, loss: 1.3510444164276123
step 8435, loss: 1.1771875619888306
step 8436, loss: 1.4440051317214966
step 8437, loss: 1.3555793762207031
step 8438, loss: 1.431059718132019
step 8439, loss: 1.248077154159546
step 8440, loss: 1.2791763544082642
step 8441, loss: 1.1065739393234253
step 8442, loss: 1.146784782409668
step 8443, loss: 1.259330153465271
step 8444, loss: 0.9299672245979309
step 8445, loss: 1.2018874883651733
step 8446, loss: 1.240045428276062
step 8447, loss: 1.0860936641693115
step 8448, loss: 1.1179884672164917
step 8449, loss: 1.4554619789123535
step 8450, loss: 1.1501390933990479
step 8451, loss: 1.3828493356704712
step 8452, loss: 1.30950927734375
step 8453, loss: 1.19198739528656
step 8454, loss: 1.3352104425430298
step 8455, loss: 1.2205827236175537
step 8456, loss: 1.1673574447631836
step 8457, loss: 1.2621054649353027
step 8458, loss: 1.1740696430206299
step 8459, loss: 1.4591995477676392
step 8460, loss: 1.182482123374939
step 8461, loss: 1.1909973621368408
step 8462, loss: 1.3853768110275269
step 8463, loss: 1.2561960220336914
step 8464, loss: 1.4817955493927002
step 8465, loss: 1.2540487051010132
step 8466, loss: 1.23806631565094
step 8467, loss: 1.463356614112854
step 8468, loss: 1.2996633052825928
step 8469, loss: 1.1324232816696167
step 8470, loss: 1.0263246297836304
step 8471, loss: 1.0820001363754272
step 8472, loss: 1.281665325164795
step 8473, loss: 1.2828391790390015
step 8474, loss: 1.0796422958374023
step 8475, loss: 1.241623044013977
step 8476, loss: 1.3676316738128662
step 8477, loss: 1.3015247583389282
step 8478, loss: 1.4730000495910645
step 8479, loss: 1.428888201713562
step 8480, loss: 1.4415271282196045
step 8481, loss: 1.3925399780273438
step 8482, loss: 1.4395285844802856
step 8483, loss: 0.9018484354019165
step 8484, loss: 1.2558519840240479
step 8485, loss: 1.4769701957702637
step 8486, loss: 1.2966188192367554
step 8487, loss: 1.3127851486206055
step 8488, loss: 1.4251877069473267
step 8489, loss: 1.4316284656524658
step 8490, loss: 1.0144662857055664
step 8491, loss: 1.244198203086853
step 8492, loss: 1.3203234672546387
step 8493, loss: 1.3190687894821167
step 8494, loss: 1.2951905727386475
step 8495, loss: 1.6996300220489502
step 8496, loss: 1.431501865386963
step 8497, loss: 1.23518705368042
step 8498, loss: 1.2216812372207642
step 8499, loss: 1.1395584344863892
step 8500, loss: 1.0990135669708252
step 8501, loss: 1.3637564182281494
step 8502, loss: 1.5020183324813843
step 8503, loss: 1.1955599784851074
step 8504, loss: 1.5028735399246216
step 8505, loss: 1.2298940420150757
step 8506, loss: 1.4685451984405518
step 8507, loss: 1.1979475021362305
step 8508, loss: 0.9551482200622559
step 8509, loss: 1.2570680379867554
step 8510, loss: 1.3166414499282837
step 8511, loss: 1.1958891153335571
step 8512, loss: 1.3946493864059448
step 8513, loss: 1.3364430665969849
step 8514, loss: 0.9782326221466064
step 8515, loss: 1.1945570707321167
step 8516, loss: 1.4443567991256714
step 8517, loss: 1.3314045667648315
step 8518, loss: 1.2965223789215088
step 8519, loss: 1.416670799255371
step 8520, loss: 1.0833204984664917
step 8521, loss: 0.9632945656776428
step 8522, loss: 1.1694446802139282
step 8523, loss: 1.3486095666885376
step 8524, loss: 1.4385414123535156
step 8525, loss: 1.3287651538848877
step 8526, loss: 1.630562424659729
step 8527, loss: 1.4788367748260498
step 8528, loss: 1.1675151586532593
step 8529, loss: 1.2595335245132446
step 8530, loss: 1.2359402179718018
step 8531, loss: 1.3506922721862793
step 8532, loss: 1.192368984222412
step 8533, loss: 1.1168807744979858
step 8534, loss: 1.2133233547210693
step 8535, loss: 1.2248144149780273
step 8536, loss: 1.366959810256958
step 8537, loss: 1.0860730409622192
step 8538, loss: 1.3003113269805908
step 8539, loss: 1.3514388799667358
step 8540, loss: 1.4001095294952393
step 8541, loss: 1.1241451501846313
step 8542, loss: 1.4422520399093628
step 8543, loss: 1.4274810552597046
step 8544, loss: 1.2369951009750366
step 8545, loss: 1.3905287981033325
step 8546, loss: 1.370569109916687
step 8547, loss: 1.4328988790512085
step 8548, loss: 1.605224609375
step 8549, loss: 1.5760927200317383
step 8550, loss: 1.2127540111541748
step 8551, loss: 1.3501750230789185
step 8552, loss: 1.6037018299102783
step 8553, loss: 1.3094712495803833
step 8554, loss: 1.380311131477356
step 8555, loss: 1.2458393573760986
step 8556, loss: 1.4359674453735352
step 8557, loss: 1.296263337135315
step 8558, loss: 0.9041301012039185
step 8559, loss: 1.3089836835861206
step 8560, loss: 1.5042219161987305
step 8561, loss: 1.056717038154602
step 8562, loss: 1.433623194694519
step 8563, loss: 1.2098824977874756
step 8564, loss: 1.1653255224227905
step 8565, loss: 1.3444817066192627
step 8566, loss: 1.2505608797073364
step 8567, loss: 1.3071824312210083
step 8568, loss: 0.9613974094390869
step 8569, loss: 1.2370212078094482
step 8570, loss: 1.1526923179626465
step 8571, loss: 1.0551762580871582
step 8572, loss: 1.249229907989502
step 8573, loss: 1.177307367324829
step 8574, loss: 1.1007694005966187
step 8575, loss: 1.064283847808838
step 8576, loss: 1.2327544689178467
step 8577, loss: 1.1373704671859741
step 8578, loss: 1.3725738525390625
step 8579, loss: 1.1086416244506836
step 8580, loss: 1.73326575756073
step 8581, loss: 1.4428843259811401
step 8582, loss: 1.0475903749465942
step 8583, loss: 1.2653177976608276
step 8584, loss: 1.2573862075805664
step 8585, loss: 1.4799352884292603
step 8586, loss: 1.0746750831604004
step 8587, loss: 0.969402551651001
step 8588, loss: 1.2988322973251343
step 8589, loss: 1.122743844985962
step 8590, loss: 1.177532434463501
step 8591, loss: 1.4842792749404907
step 8592, loss: 1.2713388204574585
step 8593, loss: 0.9828387498855591
step 8594, loss: 1.2851295471191406
step 8595, loss: 1.0123066902160645
step 8596, loss: 1.1299303770065308
step 8597, loss: 1.3035129308700562
step 8598, loss: 1.1544859409332275
step 8599, loss: 1.3713387250900269
step 8600, loss: 1.575735330581665
step 8601, loss: 1.5527089834213257
step 8602, loss: 1.2973542213439941
step 8603, loss: 1.3306012153625488
step 8604, loss: 1.1570165157318115
step 8605, loss: 0.7537023425102234
step 8606, loss: 0.9532997608184814
step 8607, loss: 0.9409530758857727
step 8608, loss: 1.2227245569229126
step 8609, loss: 1.3251148462295532
step 8610, loss: 1.2475188970565796
step 8611, loss: 1.1999099254608154
step 8612, loss: 1.5990568399429321
step 8613, loss: 1.375854730606079
step 8614, loss: 0.9113276600837708
step 8615, loss: 1.3702127933502197
step 8616, loss: 1.2617688179016113
step 8617, loss: 1.249222993850708
step 8618, loss: 1.1253927946090698
step 8619, loss: 1.1127756834030151
step 8620, loss: 1.3194586038589478
step 8621, loss: 1.1170345544815063
step 8622, loss: 1.1984732151031494
step 8623, loss: 1.5153075456619263
step 8624, loss: 1.2110515832901
step 8625, loss: 1.1998494863510132
step 8626, loss: 1.2801560163497925
step 8627, loss: 1.2108449935913086
step 8628, loss: 1.1873098611831665
step 8629, loss: 1.2444828748703003
step 8630, loss: 1.6135915517807007
step 8631, loss: 1.3412072658538818
step 8632, loss: 1.2139079570770264
step 8633, loss: 1.2752089500427246
step 8634, loss: 1.445302128791809
step 8635, loss: 1.176216721534729
step 8636, loss: 1.2149932384490967
step 8637, loss: 1.2323710918426514
step 8638, loss: 1.2024617195129395
step 8639, loss: 1.2271636724472046
step 8640, loss: 1.1568629741668701
step 8641, loss: 0.8338077068328857
step 8642, loss: 1.0490455627441406
step 8643, loss: 1.1761035919189453
step 8644, loss: 1.3774118423461914
step 8645, loss: 1.3923298120498657
step 8646, loss: 1.3817017078399658
step 8647, loss: 1.3131465911865234
step 8648, loss: 1.4120960235595703
step 8649, loss: 1.4213643074035645
step 8650, loss: 1.4201463460922241
step 8651, loss: 1.14371919631958
step 8652, loss: 1.1398762464523315
step 8653, loss: 1.5324549674987793
step 8654, loss: 1.440105676651001
step 8655, loss: 1.428023338317871
step 8656, loss: 1.4174758195877075
step 8657, loss: 1.1509628295898438
step 8658, loss: 1.1393635272979736
step 8659, loss: 1.3888462781906128
step 8660, loss: 1.2670503854751587
step 8661, loss: 1.0691765546798706
step 8662, loss: 1.099428653717041
step 8663, loss: 1.3938677310943604
step 8664, loss: 1.3220473527908325
step 8665, loss: 1.2093697786331177
step 8666, loss: 1.3836393356323242
step 8667, loss: 1.443406105041504
step 8668, loss: 1.1871442794799805
step 8669, loss: 1.0904611349105835
step 8670, loss: 1.3391032218933105
step 8671, loss: 1.4696377515792847
step 8672, loss: 1.6345789432525635
step 8673, loss: 1.4449430704116821
step 8674, loss: 1.4355719089508057
step 8675, loss: 1.2309324741363525
step 8676, loss: 1.5513583421707153
step 8677, loss: 1.264073371887207
step 8678, loss: 1.2925382852554321
step 8679, loss: 1.4925622940063477
step 8680, loss: 1.4894793033599854
step 8681, loss: 1.5897560119628906
step 8682, loss: 1.3640326261520386
step 8683, loss: 1.444148063659668
step 8684, loss: 1.5131738185882568
step 8685, loss: 1.6078481674194336
step 8686, loss: 1.352587103843689
step 8687, loss: 1.3647871017456055
step 8688, loss: 1.6338698863983154
step 8689, loss: 1.5227992534637451
step 8690, loss: 1.5683369636535645
step 8691, loss: 1.640814185142517
step 8692, loss: 1.1557132005691528
step 8693, loss: 1.4436721801757812
step 8694, loss: 1.7442004680633545
step 8695, loss: 1.5227116346359253
step 8696, loss: 1.1799143552780151
step 8697, loss: 1.4129087924957275
step 8698, loss: 1.039239525794983
step 8699, loss: 1.538128137588501
step 8700, loss: 1.310004472732544
step 8701, loss: 1.488040566444397
step 8702, loss: 1.4239928722381592
step 8703, loss: 1.1826426982879639
step 8704, loss: 1.2363276481628418
step 8705, loss: 1.2040038108825684
step 8706, loss: 0.9083596467971802
step 8707, loss: 1.3280481100082397
step 8708, loss: 1.174688696861267
step 8709, loss: 1.334001064300537
step 8710, loss: 1.1163654327392578
step 8711, loss: 1.252711534500122
step 8712, loss: 1.0209678411483765
step 8713, loss: 1.3261184692382812
step 8714, loss: 1.2063355445861816
step 8715, loss: 1.1685917377471924
step 8716, loss: 1.1736615896224976
step 8717, loss: 1.3555262088775635
step 8718, loss: 1.1560661792755127
step 8719, loss: 1.5460437536239624
step 8720, loss: 1.3880985975265503
step 8721, loss: 0.8994818329811096
step 8722, loss: 0.9324499368667603
step 8723, loss: 1.2821661233901978
step 8724, loss: 1.2435798645019531
step 8725, loss: 1.09061861038208
step 8726, loss: 1.6652653217315674
step 8727, loss: 1.1330105066299438
step 8728, loss: 1.1729493141174316
step 8729, loss: 1.139922022819519
step 8730, loss: 1.6511505842208862
step 8731, loss: 1.3280905485153198
step 8732, loss: 1.2397189140319824
step 8733, loss: 1.4137938022613525
step 8734, loss: 1.373639464378357
step 8735, loss: 1.2596337795257568
step 8736, loss: 1.211310863494873
step 8737, loss: 1.3333159685134888
step 8738, loss: 1.1389153003692627
step 8739, loss: 1.269952416419983
step 8740, loss: 1.2910796403884888
step 8741, loss: 1.2481663227081299
step 8742, loss: 1.3647252321243286
step 8743, loss: 1.076830267906189
step 8744, loss: 1.2407740354537964
step 8745, loss: 1.5638147592544556
step 8746, loss: 1.5493226051330566
step 8747, loss: 1.145708680152893
step 8748, loss: 1.0782699584960938
step 8749, loss: 1.144699215888977
step 8750, loss: 1.233656406402588
step 8751, loss: 1.3706560134887695
step 8752, loss: 1.237699270248413
step 8753, loss: 1.4290827512741089
step 8754, loss: 1.3070708513259888
step 8755, loss: 1.2666243314743042
step 8756, loss: 1.1952533721923828
step 8757, loss: 1.456151008605957
step 8758, loss: 1.1533812284469604
step 8759, loss: 1.0203429460525513
step 8760, loss: 1.1103848218917847
step 8761, loss: 1.2077999114990234
step 8762, loss: 1.0734935998916626
step 8763, loss: 1.286401629447937
step 8764, loss: 1.3731744289398193
step 8765, loss: 1.4556585550308228
step 8766, loss: 1.363356590270996
step 8767, loss: 1.6525884866714478
step 8768, loss: 1.4727567434310913
step 8769, loss: 1.095955729484558
step 8770, loss: 1.186141014099121
step 8771, loss: 1.024635910987854
step 8772, loss: 1.3543306589126587
step 8773, loss: 1.4318662881851196
step 8774, loss: 1.497161865234375
step 8775, loss: 1.433756947517395
step 8776, loss: 1.418849229812622
step 8777, loss: 1.3048452138900757
step 8778, loss: 1.103725552558899
step 8779, loss: 0.9916437864303589
step 8780, loss: 1.3348174095153809
step 8781, loss: 1.363029956817627
step 8782, loss: 1.1471887826919556
step 8783, loss: 1.2804193496704102
step 8784, loss: 1.343319296836853
step 8785, loss: 1.1504796743392944
step 8786, loss: 1.1321322917938232
step 8787, loss: 1.3834402561187744
step 8788, loss: 1.2279356718063354
step 8789, loss: 1.2474533319473267
step 8790, loss: 1.7000435590744019
step 8791, loss: 1.2472361326217651
step 8792, loss: 1.7823346853256226
step 8793, loss: 1.2504078149795532
step 8794, loss: 1.3594279289245605
step 8795, loss: 1.3742178678512573
step 8796, loss: 1.2143112421035767
step 8797, loss: 1.3435324430465698
step 8798, loss: 1.3098125457763672
step 8799, loss: 1.3355571031570435
step 8800, loss: 1.6462815999984741
step 8801, loss: 1.4893615245819092
step 8802, loss: 1.361318588256836
step 8803, loss: 1.2867317199707031
step 8804, loss: 1.268216609954834
step 8805, loss: 1.6043751239776611
step 8806, loss: 1.1288223266601562
step 8807, loss: 1.3197673559188843
step 8808, loss: 1.566880464553833
step 8809, loss: 1.191316843032837
step 8810, loss: 1.4762399196624756
step 8811, loss: 1.1555836200714111
step 8812, loss: 1.4415003061294556
step 8813, loss: 1.2439528703689575
step 8814, loss: 1.5953586101531982
step 8815, loss: 1.03162682056427
step 8816, loss: 1.2985831499099731
step 8817, loss: 1.3208082914352417
step 8818, loss: 0.9407705068588257
step 8819, loss: 1.2285161018371582
step 8820, loss: 1.3123005628585815
step 8821, loss: 1.6471377611160278
step 8822, loss: 1.4328912496566772
step 8823, loss: 1.3882580995559692
step 8824, loss: 1.586217999458313
step 8825, loss: 1.4954833984375
step 8826, loss: 1.1807987689971924
step 8827, loss: 1.4668059349060059
step 8828, loss: 1.3232818841934204
step 8829, loss: 1.2042216062545776
step 8830, loss: 1.5102674961090088
step 8831, loss: 1.3492017984390259
step 8832, loss: 1.2127305269241333
step 8833, loss: 1.032778263092041
step 8834, loss: 1.4758257865905762
step 8835, loss: 1.1519211530685425
step 8836, loss: 1.2861504554748535
step 8837, loss: 1.4451017379760742
step 8838, loss: 1.211708903312683
step 8839, loss: 1.4707571268081665
step 8840, loss: 1.243124008178711
step 8841, loss: 1.4556163549423218
step 8842, loss: 1.6168327331542969
step 8843, loss: 1.2837425470352173
step 8844, loss: 1.166743278503418
step 8845, loss: 1.3024044036865234
step 8846, loss: 1.4718735218048096
step 8847, loss: 1.3391478061676025
step 8848, loss: 1.0293930768966675
step 8849, loss: 1.443426489830017
step 8850, loss: 1.189080834388733
step 8851, loss: 1.261968970298767
step 8852, loss: 1.3995246887207031
step 8853, loss: 1.227816104888916
step 8854, loss: 1.4312947988510132
step 8855, loss: 1.4289029836654663
step 8856, loss: 1.0949413776397705
step 8857, loss: 1.3716485500335693
step 8858, loss: 1.3607957363128662
step 8859, loss: 1.2864503860473633
step 8860, loss: 1.4838005304336548
step 8861, loss: 1.3117167949676514
step 8862, loss: 1.315941572189331
step 8863, loss: 1.1438071727752686
step 8864, loss: 1.4918549060821533
step 8865, loss: 1.0763360261917114
step 8866, loss: 1.2643431425094604
step 8867, loss: 1.1272441148757935
step 8868, loss: 1.1839295625686646
step 8869, loss: 1.1498955488204956
step 8870, loss: 1.1539361476898193
step 8871, loss: 1.3840606212615967
step 8872, loss: 1.4901970624923706
step 8873, loss: 1.4108690023422241
step 8874, loss: 1.4538922309875488
step 8875, loss: 1.4622737169265747
step 8876, loss: 1.8682788610458374
step 8877, loss: 1.7680944204330444
step 8878, loss: 1.3813284635543823
step 8879, loss: 1.0612714290618896
step 8880, loss: 1.4720991849899292
step 8881, loss: 1.5451446771621704
step 8882, loss: 1.3819169998168945
step 8883, loss: 1.2887537479400635
step 8884, loss: 1.069859504699707
step 8885, loss: 1.7296091318130493
step 8886, loss: 1.1793915033340454
step 8887, loss: 1.551976203918457
step 8888, loss: 1.0729697942733765
step 8889, loss: 1.6675626039505005
step 8890, loss: 1.259048342704773
step 8891, loss: 1.119431734085083
step 8892, loss: 1.1559593677520752
step 8893, loss: 1.1072477102279663
step 8894, loss: 1.55618417263031
step 8895, loss: 1.6355738639831543
step 8896, loss: 1.4233027696609497
step 8897, loss: 1.8093336820602417
step 8898, loss: 1.2208462953567505
step 8899, loss: 1.5151169300079346
step 8900, loss: 1.2753713130950928
step 8901, loss: 1.3547145128250122
step 8902, loss: 1.3448238372802734
step 8903, loss: 1.4764354228973389
step 8904, loss: 1.3378708362579346
step 8905, loss: 1.2302730083465576
step 8906, loss: 1.260960340499878
step 8907, loss: 1.6099661588668823
step 8908, loss: 1.2470365762710571
step 8909, loss: 1.3398356437683105
step 8910, loss: 1.2537745237350464
step 8911, loss: 1.1107513904571533
step 8912, loss: 1.0659927129745483
step 8913, loss: 1.07468581199646
step 8914, loss: 1.1179101467132568
step 8915, loss: 1.3270039558410645
step 8916, loss: 1.1735903024673462
step 8917, loss: 1.4678314924240112
step 8918, loss: 1.3379156589508057
step 8919, loss: 1.2141308784484863
step 8920, loss: 1.1377100944519043
step 8921, loss: 0.8662444353103638
step 8922, loss: 1.025577425956726
step 8923, loss: 1.2299607992172241
step 8924, loss: 1.4624050855636597
step 8925, loss: 1.2294224500656128
step 8926, loss: 1.2625924348831177
step 8927, loss: 1.5004905462265015
step 8928, loss: 1.1113849878311157
step 8929, loss: 1.3932864665985107
step 8930, loss: 1.3304632902145386
step 8931, loss: 1.5049121379852295
step 8932, loss: 1.2561181783676147
step 8933, loss: 1.0664896965026855
step 8934, loss: 1.589682936668396
step 8935, loss: 1.3667303323745728
step 8936, loss: 1.4760199785232544
step 8937, loss: 1.369877576828003
step 8938, loss: 1.3670356273651123
step 8939, loss: 1.4596580266952515
step 8940, loss: 1.5337058305740356
step 8941, loss: 1.3377257585525513
step 8942, loss: 1.4575275182724
step 8943, loss: 1.1043001413345337
step 8944, loss: 1.2547080516815186
step 8945, loss: 1.2254793643951416
step 8946, loss: 1.2738499641418457
step 8947, loss: 1.549428105354309
step 8948, loss: 1.402940273284912
step 8949, loss: 0.866835355758667
step 8950, loss: 0.8623203635215759
step 8951, loss: 1.259081482887268
step 8952, loss: 1.2511160373687744
step 8953, loss: 1.3461359739303589
step 8954, loss: 1.4641183614730835
step 8955, loss: 1.305590033531189
step 8956, loss: 1.3233407735824585
step 8957, loss: 1.2045354843139648
step 8958, loss: 1.1700422763824463
step 8959, loss: 1.1604492664337158
step 8960, loss: 1.3647501468658447
step 8961, loss: 1.3416881561279297
step 8962, loss: 1.1388658285140991
step 8963, loss: 1.4154969453811646
step 8964, loss: 1.706869125366211
step 8965, loss: 1.211733341217041
step 8966, loss: 1.0755757093429565
step 8967, loss: 1.254300832748413
step 8968, loss: 1.1114157438278198
step 8969, loss: 1.2491225004196167
step 8970, loss: 1.2166686058044434
step 8971, loss: 1.1804500818252563
step 8972, loss: 1.491248607635498
step 8973, loss: 1.2070804834365845
step 8974, loss: 1.3251835107803345
step 8975, loss: 1.2866506576538086
step 8976, loss: 1.3432120084762573
step 8977, loss: 1.2359132766723633
step 8978, loss: 1.252076268196106
step 8979, loss: 1.3206161260604858
step 8980, loss: 1.6148415803909302
step 8981, loss: 1.3893187046051025
step 8982, loss: 1.5738943815231323
step 8983, loss: 1.451088786125183
step 8984, loss: 1.0127679109573364
step 8985, loss: 1.0957694053649902
step 8986, loss: 0.9739022254943848
step 8987, loss: 1.4437659978866577
step 8988, loss: 1.658452033996582
step 8989, loss: 1.3330925703048706
step 8990, loss: 1.4790525436401367
step 8991, loss: 1.1040453910827637
step 8992, loss: 1.5492944717407227
step 8993, loss: 1.2611546516418457
step 8994, loss: 1.4865347146987915
step 8995, loss: 1.2585633993148804
step 8996, loss: 1.386073112487793
step 8997, loss: 1.2519030570983887
step 8998, loss: 1.2362401485443115
step 8999, loss: 1.0095866918563843
step 9000, loss: 1.2832894325256348
step 9001, loss: 1.0986493825912476
step 9002, loss: 1.3361068964004517
step 9003, loss: 1.0599552392959595
step 9004, loss: 1.097247838973999
step 9005, loss: 1.358556866645813
step 9006, loss: 1.2792688608169556
step 9007, loss: 1.1576342582702637
step 9008, loss: 0.8730396628379822
step 9009, loss: 1.1317036151885986
step 9010, loss: 1.2439172267913818
step 9011, loss: 1.4148197174072266
step 9012, loss: 1.2864558696746826
step 9013, loss: 1.1185755729675293
step 9014, loss: 1.2295057773590088
step 9015, loss: 1.5328834056854248
step 9016, loss: 1.5588853359222412
step 9017, loss: 1.2699356079101562
step 9018, loss: 1.3027043342590332
step 9019, loss: 1.2821677923202515
step 9020, loss: 1.5823767185211182
step 9021, loss: 1.5099999904632568
step 9022, loss: 1.5374038219451904
step 9023, loss: 1.704488754272461
step 9024, loss: 1.3945256471633911
step 9025, loss: 1.469936490058899
step 9026, loss: 1.5049715042114258
step 9027, loss: 1.431528925895691
step 9028, loss: 1.558258056640625
step 9029, loss: 1.435105800628662
step 9030, loss: 1.8405365943908691
step 9031, loss: 1.2927098274230957
step 9032, loss: 1.4986071586608887
step 9033, loss: 1.3187153339385986
step 9034, loss: 1.5254859924316406
step 9035, loss: 1.345206379890442
step 9036, loss: 1.6738187074661255
step 9037, loss: 1.4555152654647827
step 9038, loss: 1.3960137367248535
step 9039, loss: 1.5072336196899414
step 9040, loss: 1.6242449283599854
step 9041, loss: 1.2623114585876465
step 9042, loss: 1.5563613176345825
step 9043, loss: 1.4362095594406128
step 9044, loss: 1.4104249477386475
step 9045, loss: 1.6183470487594604
step 9046, loss: 1.5307624340057373
step 9047, loss: 1.429686427116394
step 9048, loss: 1.3352018594741821
step 9049, loss: 1.2781188488006592
step 9050, loss: 1.7290759086608887
step 9051, loss: 1.497771143913269
step 9052, loss: 1.1905728578567505
step 9053, loss: 1.0873984098434448
step 9054, loss: 1.34614896774292
step 9055, loss: 1.3579164743423462
step 9056, loss: 1.1805388927459717
step 9057, loss: 1.4666029214859009
step 9058, loss: 1.3003101348876953
step 9059, loss: 1.3966952562332153
step 9060, loss: 1.540840744972229
step 9061, loss: 1.6454488039016724
step 9062, loss: 1.4877089262008667
step 9063, loss: 1.5709071159362793
step 9064, loss: 1.460866093635559
step 9065, loss: 1.3591276407241821
step 9066, loss: 1.1949654817581177
step 9067, loss: 1.4630519151687622
step 9068, loss: 1.2253538370132446
step 9069, loss: 1.3387497663497925
step 9070, loss: 1.2669442892074585
step 9071, loss: 1.3003690242767334
step 9072, loss: 1.0373384952545166
step 9073, loss: 1.4855824708938599
step 9074, loss: 1.12070894241333
step 9075, loss: 0.9944579005241394
step 9076, loss: 1.0904103517532349
step 9077, loss: 1.2963021993637085
step 9078, loss: 1.1897785663604736
step 9079, loss: 1.4944937229156494
step 9080, loss: 1.251245379447937
step 9081, loss: 1.2509685754776
step 9082, loss: 1.5842950344085693
step 9083, loss: 1.5979125499725342
step 9084, loss: 1.427292823791504
step 9085, loss: 1.1784744262695312
step 9086, loss: 1.2485980987548828
step 9087, loss: 1.5130817890167236
step 9088, loss: 1.5575076341629028
step 9089, loss: 1.2684099674224854
step 9090, loss: 1.505108118057251
step 9091, loss: 1.5172113180160522
step 9092, loss: 1.2898718118667603
step 9093, loss: 1.3838738203048706
step 9094, loss: 1.4858626127243042
step 9095, loss: 1.1463242769241333
step 9096, loss: 1.590760350227356
step 9097, loss: 1.1155133247375488
step 9098, loss: 1.3394991159439087
step 9099, loss: 1.665533423423767
step 9100, loss: 1.3234179019927979
step 9101, loss: 1.2701598405838013
step 9102, loss: 1.2868146896362305
step 9103, loss: 1.2550883293151855
step 9104, loss: 1.0855412483215332
step 9105, loss: 1.3188163042068481
step 9106, loss: 1.2844644784927368
step 9107, loss: 1.4876737594604492
step 9108, loss: 1.5426421165466309
step 9109, loss: 1.43881094455719
step 9110, loss: 1.2717711925506592
step 9111, loss: 1.5162490606307983
step 9112, loss: 1.328787922859192
step 9113, loss: 1.3300093412399292
step 9114, loss: 1.358476996421814
step 9115, loss: 1.1433161497116089
step 9116, loss: 1.3379987478256226
step 9117, loss: 1.4343836307525635
step 9118, loss: 1.1772148609161377
step 9119, loss: 1.2045105695724487
step 9120, loss: 1.1154481172561646
step 9121, loss: 1.0203197002410889
step 9122, loss: 1.427048921585083
step 9123, loss: 1.2117044925689697
step 9124, loss: 1.4009573459625244
step 9125, loss: 1.499092698097229
step 9126, loss: 1.4853155612945557
step 9127, loss: 1.1964353322982788
step 9128, loss: 1.364100456237793
step 9129, loss: 1.544015645980835
step 9130, loss: 1.4608869552612305
step 9131, loss: 1.4044666290283203
step 9132, loss: 1.2949261665344238
step 9133, loss: 1.1313822269439697
step 9134, loss: 1.4025847911834717
step 9135, loss: 1.4393439292907715
step 9136, loss: 1.5818819999694824
step 9137, loss: 1.4485623836517334
step 9138, loss: 1.57456374168396
step 9139, loss: 1.4103636741638184
step 9140, loss: 1.3429744243621826
step 9141, loss: 1.0714775323867798
step 9142, loss: 1.405190110206604
step 9143, loss: 1.4256412982940674
step 9144, loss: 1.235595941543579
step 9145, loss: 1.119560718536377
step 9146, loss: 1.2889814376831055
step 9147, loss: 1.3037559986114502
step 9148, loss: 1.228398084640503
step 9149, loss: 1.3071763515472412
step 9150, loss: 1.2018067836761475
step 9151, loss: 1.1780554056167603
step 9152, loss: 1.1978493928909302
step 9153, loss: 1.2585203647613525
step 9154, loss: 1.2811418771743774
step 9155, loss: 1.37608802318573
step 9156, loss: 1.3419042825698853
step 9157, loss: 1.5003132820129395
step 9158, loss: 1.459304928779602
step 9159, loss: 1.4344196319580078
step 9160, loss: 1.2866698503494263
step 9161, loss: 1.3848129510879517
step 9162, loss: 1.134781837463379
step 9163, loss: 1.3207874298095703
step 9164, loss: 1.2781559228897095
step 9165, loss: 1.173879861831665
step 9166, loss: 1.2860172986984253
step 9167, loss: 1.1074210405349731
step 9168, loss: 1.2563905715942383
step 9169, loss: 1.341523289680481
step 9170, loss: 1.4403914213180542
step 9171, loss: 1.3470243215560913
step 9172, loss: 1.6600399017333984
step 9173, loss: 1.204827070236206
step 9174, loss: 1.2049256563186646
step 9175, loss: 1.2342675924301147
step 9176, loss: 1.5242711305618286
step 9177, loss: 1.461356282234192
step 9178, loss: 1.2716567516326904
step 9179, loss: 1.169487714767456
step 9180, loss: 1.6794589757919312
step 9181, loss: 1.484224796295166
step 9182, loss: 1.3765543699264526
step 9183, loss: 1.5624265670776367
step 9184, loss: 1.2148689031600952
step 9185, loss: 1.3866339921951294
step 9186, loss: 1.6099950075149536
step 9187, loss: 1.3154122829437256
step 9188, loss: 1.527694582939148
step 9189, loss: 1.60096275806427
step 9190, loss: 1.4116483926773071
step 9191, loss: 0.9973196983337402
step 9192, loss: 1.4040449857711792
step 9193, loss: 1.2670015096664429
step 9194, loss: 1.4846713542938232
step 9195, loss: 1.6723148822784424
step 9196, loss: 1.6162980794906616
step 9197, loss: 1.1528383493423462
step 9198, loss: 1.1061159372329712
step 9199, loss: 1.507182240486145
step 9200, loss: 1.6012778282165527
step 9201, loss: 1.4300916194915771
step 9202, loss: 1.4644497632980347
step 9203, loss: 1.3782438039779663
step 9204, loss: 1.2156455516815186
step 9205, loss: 1.290610432624817
step 9206, loss: 1.1794559955596924
step 9207, loss: 1.2442831993103027
step 9208, loss: 1.2589787244796753
step 9209, loss: 1.40366792678833
step 9210, loss: 1.4108773469924927
step 9211, loss: 1.4712797403335571
step 9212, loss: 1.3024241924285889
step 9213, loss: 1.0395193099975586
step 9214, loss: 1.0987586975097656
step 9215, loss: 1.1847517490386963
step 9216, loss: 1.4346606731414795
step 9217, loss: 1.7673792839050293
step 9218, loss: 1.2783887386322021
step 9219, loss: 0.9571666717529297
step 9220, loss: 1.2315354347229004
step 9221, loss: 1.4755412340164185
step 9222, loss: 1.4935898780822754
step 9223, loss: 1.2445329427719116
step 9224, loss: 1.3016599416732788
step 9225, loss: 1.4706143140792847
step 9226, loss: 1.126014232635498
step 9227, loss: 1.1846554279327393
step 9228, loss: 1.2686034440994263
step 9229, loss: 1.234513759613037
step 9230, loss: 1.401229739189148
step 9231, loss: 1.13551664352417
step 9232, loss: 1.3243635892868042
step 9233, loss: 1.7477408647537231
step 9234, loss: 1.256055235862732
step 9235, loss: 1.1310088634490967
step 9236, loss: 1.4351965188980103
step 9237, loss: 1.2187113761901855
step 9238, loss: 1.185170292854309
step 9239, loss: 1.4787544012069702
step 9240, loss: 1.1356195211410522
step 9241, loss: 1.454729437828064
step 9242, loss: 1.2860265970230103
step 9243, loss: 1.2055892944335938
step 9244, loss: 1.16361403465271
step 9245, loss: 1.703853726387024
step 9246, loss: 1.1968635320663452
step 9247, loss: 1.2206151485443115
step 9248, loss: 1.3232448101043701
step 9249, loss: 1.3769819736480713
step 9250, loss: 1.456647276878357
step 9251, loss: 1.5664901733398438
step 9252, loss: 1.3287315368652344
step 9253, loss: 1.391161561012268
step 9254, loss: 1.3913545608520508
step 9255, loss: 1.402421474456787
step 9256, loss: 1.431930422782898
step 9257, loss: 1.0180917978286743
step 9258, loss: 1.1421939134597778
step 9259, loss: 1.612412691116333
step 9260, loss: 1.5856242179870605
step 9261, loss: 1.794502854347229
step 9262, loss: 1.475449800491333
step 9263, loss: 1.3915576934814453
step 9264, loss: 1.2127476930618286
step 9265, loss: 1.4236171245574951
step 9266, loss: 1.5186878442764282
step 9267, loss: 1.39694344997406
step 9268, loss: 1.3660950660705566
step 9269, loss: 1.5648938417434692
step 9270, loss: 1.483681082725525
step 9271, loss: 1.4400452375411987
step 9272, loss: 1.311791181564331
step 9273, loss: 1.2193998098373413
step 9274, loss: 1.4400360584259033
step 9275, loss: 1.3079779148101807
step 9276, loss: 1.1253384351730347
step 9277, loss: 1.5609806776046753
step 9278, loss: 1.5096830129623413
step 9279, loss: 1.5036216974258423
step 9280, loss: 1.490356206893921
step 9281, loss: 1.6401844024658203
step 9282, loss: 1.3799571990966797
step 9283, loss: 1.4224148988723755
step 9284, loss: 1.405189037322998
step 9285, loss: 1.4233880043029785
step 9286, loss: 1.2534968852996826
step 9287, loss: 1.4001864194869995
step 9288, loss: 1.249683141708374
step 9289, loss: 1.2458996772766113
step 9290, loss: 1.7677204608917236
step 9291, loss: 1.1822715997695923
step 9292, loss: 1.1047321557998657
step 9293, loss: 1.3753317594528198
step 9294, loss: 1.216441035270691
step 9295, loss: 1.2159607410430908
step 9296, loss: 0.9756494760513306
step 9297, loss: 1.0986363887786865
step 9298, loss: 1.235793113708496
step 9299, loss: 1.0394980907440186
step 9300, loss: 1.1064913272857666
step 9301, loss: 1.5645499229431152
step 9302, loss: 1.1649264097213745
step 9303, loss: 1.4822081327438354
step 9304, loss: 1.369964361190796
step 9305, loss: 1.113455057144165
step 9306, loss: 1.3196560144424438
step 9307, loss: 1.1705507040023804
step 9308, loss: 1.191172480583191
step 9309, loss: 1.1261084079742432
step 9310, loss: 1.412996768951416
step 9311, loss: 1.4705681800842285
step 9312, loss: 1.4523875713348389
step 9313, loss: 1.23543119430542
step 9314, loss: 1.3121249675750732
step 9315, loss: 1.3621617555618286
step 9316, loss: 1.3751885890960693
step 9317, loss: 1.4209481477737427
step 9318, loss: 1.5046768188476562
step 9319, loss: 1.3915796279907227
step 9320, loss: 1.0186654329299927
step 9321, loss: 0.9908611178398132
step 9322, loss: 1.3692834377288818
step 9323, loss: 1.2218621969223022
step 9324, loss: 1.4443424940109253
step 9325, loss: 1.5916904211044312
step 9326, loss: 1.2307807207107544
step 9327, loss: 1.1214585304260254
step 9328, loss: 1.5406750440597534
step 9329, loss: 1.4953562021255493
step 9330, loss: 1.606264352798462
step 9331, loss: 1.4719651937484741
step 9332, loss: 1.3430308103561401
step 9333, loss: 1.1658204793930054
step 9334, loss: 1.2245986461639404
step 9335, loss: 1.2316462993621826
step 9336, loss: 1.404692530632019
step 9337, loss: 2.006032705307007
step 9338, loss: 1.2898292541503906
step 9339, loss: 1.1976298093795776
step 9340, loss: 1.3731050491333008
step 9341, loss: 1.4486534595489502
step 9342, loss: 1.3351588249206543
step 9343, loss: 1.331437349319458
step 9344, loss: 1.6147525310516357
step 9345, loss: 1.3571174144744873
step 9346, loss: 1.4371542930603027
step 9347, loss: 1.4137576818466187
step 9348, loss: 1.5346271991729736
step 9349, loss: 1.3538049459457397
step 9350, loss: 1.4764597415924072
step 9351, loss: 1.4437789916992188
step 9352, loss: 1.3315703868865967
step 9353, loss: 1.4201374053955078
step 9354, loss: 1.2267860174179077
step 9355, loss: 1.403016209602356
step 9356, loss: 1.3999844789505005
step 9357, loss: 1.3958712816238403
step 9358, loss: 1.2006806135177612
step 9359, loss: 1.0195610523223877
step 9360, loss: 1.186693549156189
step 9361, loss: 1.4758471250534058
step 9362, loss: 1.4064078330993652
step 9363, loss: 1.5291829109191895
step 9364, loss: 1.4399415254592896
step 9365, loss: 1.4400432109832764
step 9366, loss: 1.1743067502975464
step 9367, loss: 1.143442988395691
step 9368, loss: 1.2090160846710205
step 9369, loss: 1.4508920907974243
step 9370, loss: 0.990071177482605
step 9371, loss: 1.159923791885376
step 9372, loss: 1.0381158590316772
step 9373, loss: 1.2734415531158447
step 9374, loss: 1.15354323387146
step 9375, loss: 1.2355097532272339
step 9376, loss: 1.358730673789978
step 9377, loss: 1.3946434259414673
step 9378, loss: 1.4596586227416992
step 9379, loss: 1.459513783454895
step 9380, loss: 1.3538920879364014
step 9381, loss: 1.4705005884170532
step 9382, loss: 1.1878423690795898
step 9383, loss: 1.3295328617095947
step 9384, loss: 1.0888874530792236
step 9385, loss: 1.1108561754226685
step 9386, loss: 1.098412275314331
step 9387, loss: 1.4684637784957886
step 9388, loss: 1.1058005094528198
step 9389, loss: 1.2988903522491455
step 9390, loss: 1.2509857416152954
step 9391, loss: 1.4153869152069092
step 9392, loss: 1.3923331499099731
step 9393, loss: 1.431031346321106
step 9394, loss: 1.404355525970459
step 9395, loss: 1.0271074771881104
step 9396, loss: 1.016891598701477
step 9397, loss: 1.5190662145614624
step 9398, loss: 1.2354328632354736
step 9399, loss: 1.1282751560211182
step 9400, loss: 1.3592087030410767
step 9401, loss: 1.1383212804794312
step 9402, loss: 1.2887053489685059
step 9403, loss: 1.2132201194763184
step 9404, loss: 1.402922511100769
step 9405, loss: 1.2988098859786987
step 9406, loss: 1.3770638704299927
step 9407, loss: 1.3866013288497925
step 9408, loss: 1.2978795766830444
step 9409, loss: 1.1965298652648926
step 9410, loss: 1.2305430173873901
step 9411, loss: 1.4306477308273315
step 9412, loss: 1.4386706352233887
step 9413, loss: 1.1431279182434082
step 9414, loss: 1.1727030277252197
step 9415, loss: 1.354884386062622
step 9416, loss: 1.48322594165802
step 9417, loss: 1.2189137935638428
step 9418, loss: 1.2394728660583496
step 9419, loss: 0.973701536655426
step 9420, loss: 1.5602359771728516
step 9421, loss: 1.35074782371521
step 9422, loss: 1.528708577156067
step 9423, loss: 1.2048094272613525
step 9424, loss: 1.0379043817520142
step 9425, loss: 1.1522116661071777
step 9426, loss: 0.9610366225242615
step 9427, loss: 1.0814793109893799
step 9428, loss: 1.211603045463562
step 9429, loss: 0.993421196937561
step 9430, loss: 0.9699498414993286
step 9431, loss: 1.1109215021133423
step 9432, loss: 1.1511651277542114
step 9433, loss: 1.2726647853851318
step 9434, loss: 1.0981534719467163
step 9435, loss: 1.3097001314163208
step 9436, loss: 1.3924771547317505
step 9437, loss: 1.3247990608215332
step 9438, loss: 1.3877073526382446
step 9439, loss: 1.1896605491638184
step 9440, loss: 1.3633888959884644
step 9441, loss: 1.0557423830032349
step 9442, loss: 1.2137926816940308
step 9443, loss: 1.2667691707611084
step 9444, loss: 0.9970788955688477
step 9445, loss: 1.4226526021957397
step 9446, loss: 1.0712252855300903
step 9447, loss: 1.0205730199813843
step 9448, loss: 1.1942745447158813
step 9449, loss: 1.5147722959518433
step 9450, loss: 1.1128349304199219
step 9451, loss: 1.4338005781173706
step 9452, loss: 1.3378483057022095
step 9453, loss: 1.001120924949646
step 9454, loss: 1.4754939079284668
step 9455, loss: 1.2402012348175049
step 9456, loss: 1.2409536838531494
step 9457, loss: 1.4230397939682007
step 9458, loss: 1.3694708347320557
step 9459, loss: 1.1350195407867432
step 9460, loss: 1.5878058671951294
step 9461, loss: 1.6553406715393066
step 9462, loss: 1.28157377243042
step 9463, loss: 1.28116774559021
step 9464, loss: 1.096486210823059
step 9465, loss: 0.9277822375297546
step 9466, loss: 1.3598108291625977
step 9467, loss: 1.416546106338501
step 9468, loss: 1.3152167797088623
step 9469, loss: 1.4751940965652466
step 9470, loss: 1.383811116218567
step 9471, loss: 1.2108161449432373
step 9472, loss: 1.2091976404190063
step 9473, loss: 1.2905343770980835
step 9474, loss: 1.3093087673187256
step 9475, loss: 1.3779078722000122
step 9476, loss: 1.2304868698120117
step 9477, loss: 1.373919129371643
step 9478, loss: 1.3881038427352905
step 9479, loss: 0.8517759442329407
step 9480, loss: 1.1520735025405884
step 9481, loss: 1.5425364971160889
step 9482, loss: 1.188868522644043
step 9483, loss: 1.4463164806365967
step 9484, loss: 1.1545813083648682
step 9485, loss: 1.369215965270996
step 9486, loss: 1.1739214658737183
step 9487, loss: 1.4665873050689697
step 9488, loss: 1.0467886924743652
step 9489, loss: 1.3126220703125
step 9490, loss: 1.2826191186904907
step 9491, loss: 0.9832143187522888
step 9492, loss: 1.6556495428085327
step 9493, loss: 1.0115910768508911
step 9494, loss: 1.0220447778701782
step 9495, loss: 1.2317235469818115
step 9496, loss: 1.314545750617981
step 9497, loss: 1.4244778156280518
step 9498, loss: 1.2829445600509644
step 9499, loss: 1.0621826648712158
step 9500, loss: 1.1902782917022705
step 9501, loss: 1.144067406654358
step 9502, loss: 1.2927106618881226
step 9503, loss: 1.2864582538604736
step 9504, loss: 1.0630155801773071
step 9505, loss: 1.077056884765625
step 9506, loss: 1.2511645555496216
step 9507, loss: 1.3001093864440918
step 9508, loss: 1.2720082998275757
step 9509, loss: 1.0777987241744995
step 9510, loss: 1.3169755935668945
step 9511, loss: 1.1976490020751953
step 9512, loss: 0.9875118732452393
step 9513, loss: 1.2407411336898804
step 9514, loss: 1.3112651109695435
step 9515, loss: 0.964512288570404
step 9516, loss: 1.1448097229003906
step 9517, loss: 1.1214749813079834
step 9518, loss: 1.3390182256698608
step 9519, loss: 1.269126296043396
step 9520, loss: 1.251291036605835
step 9521, loss: 1.1520060300827026
step 9522, loss: 1.163652777671814
step 9523, loss: 1.1328285932540894
step 9524, loss: 1.1250147819519043
step 9525, loss: 1.1293559074401855
step 9526, loss: 1.1742485761642456
step 9527, loss: 1.1882679462432861
step 9528, loss: 0.9736009836196899
step 9529, loss: 1.1109634637832642
step 9530, loss: 1.3740876913070679
step 9531, loss: 1.3249750137329102
step 9532, loss: 1.4373656511306763
step 9533, loss: 1.2044752836227417
step 9534, loss: 1.1843442916870117
step 9535, loss: 1.111411213874817
step 9536, loss: 0.9571712017059326
step 9537, loss: 1.2494637966156006
step 9538, loss: 1.2846198081970215
step 9539, loss: 1.3815902471542358
step 9540, loss: 1.0636343955993652
step 9541, loss: 1.4401886463165283
step 9542, loss: 1.0647937059402466
step 9543, loss: 1.5130808353424072
step 9544, loss: 1.502192735671997
step 9545, loss: 1.3441001176834106
step 9546, loss: 1.1022617816925049
step 9547, loss: 1.144216775894165
step 9548, loss: 1.0672374963760376
step 9549, loss: 1.494335412979126
step 9550, loss: 1.4371521472930908
step 9551, loss: 1.422823190689087
step 9552, loss: 1.5103951692581177
step 9553, loss: 1.5886400938034058
step 9554, loss: 1.3546100854873657
step 9555, loss: 1.1283378601074219
step 9556, loss: 1.4690380096435547
step 9557, loss: 1.1140737533569336
step 9558, loss: 1.2127341032028198
step 9559, loss: 1.3947945833206177
step 9560, loss: 1.143237829208374
step 9561, loss: 1.3768022060394287
step 9562, loss: 1.210270643234253
step 9563, loss: 1.2684739828109741
step 9564, loss: 1.4280941486358643
step 9565, loss: 1.170325517654419
step 9566, loss: 1.3122432231903076
step 9567, loss: 1.1682593822479248
step 9568, loss: 1.3334171772003174
step 9569, loss: 0.9976686239242554
step 9570, loss: 1.3477169275283813
step 9571, loss: 1.1719226837158203
step 9572, loss: 1.0809462070465088
step 9573, loss: 1.2581373453140259
step 9574, loss: 1.3810021877288818
step 9575, loss: 1.5592156648635864
step 9576, loss: 1.4261298179626465
step 9577, loss: 1.6281777620315552
step 9578, loss: 1.1593248844146729
step 9579, loss: 1.277437448501587
step 9580, loss: 1.1464811563491821
step 9581, loss: 1.0866340398788452
step 9582, loss: 1.2529579401016235
step 9583, loss: 1.4537488222122192
step 9584, loss: 1.2989312410354614
step 9585, loss: 1.240928053855896
step 9586, loss: 1.3405784368515015
step 9587, loss: 1.5900839567184448
step 9588, loss: 1.1976683139801025
step 9589, loss: 1.5924326181411743
step 9590, loss: 1.6164878606796265
step 9591, loss: 1.6549561023712158
step 9592, loss: 1.6099056005477905
step 9593, loss: 1.618996262550354
step 9594, loss: 1.5385509729385376
step 9595, loss: 1.448699951171875
step 9596, loss: 1.5599502325057983
step 9597, loss: 1.4032331705093384
step 9598, loss: 1.4760792255401611
step 9599, loss: 1.5517096519470215
step 9600, loss: 1.4682621955871582
step 9601, loss: 1.4754582643508911
step 9602, loss: 1.7923206090927124
step 9603, loss: 1.1653388738632202
step 9604, loss: 1.6196640729904175
step 9605, loss: 1.4379122257232666
step 9606, loss: 1.5985723733901978
step 9607, loss: 1.8609310388565063
step 9608, loss: 1.8382216691970825
step 9609, loss: 1.6082892417907715
step 9610, loss: 1.183983325958252
step 9611, loss: 1.7432527542114258
step 9612, loss: 1.498476505279541
step 9613, loss: 1.4123932123184204
step 9614, loss: 1.4855858087539673
step 9615, loss: 1.9204092025756836
step 9616, loss: 1.4626080989837646
step 9617, loss: 1.7400875091552734
step 9618, loss: 1.528074026107788
step 9619, loss: 1.4384675025939941
step 9620, loss: 1.4062426090240479
step 9621, loss: 1.525348424911499
step 9622, loss: 1.3284341096878052
step 9623, loss: 1.659468173980713
step 9624, loss: 1.1953083276748657
step 9625, loss: 1.6985405683517456
step 9626, loss: 1.240277647972107
step 9627, loss: 1.4042329788208008
step 9628, loss: 1.1764276027679443
step 9629, loss: 1.255401611328125
step 9630, loss: 1.3586732149124146
step 9631, loss: 1.0414681434631348
step 9632, loss: 1.4933807849884033
step 9633, loss: 1.0262531042099
step 9634, loss: 1.3388102054595947
step 9635, loss: 1.2963374853134155
step 9636, loss: 1.2004905939102173
step 9637, loss: 1.1157830953598022
step 9638, loss: 0.9008403420448303
step 9639, loss: 1.3187346458435059
step 9640, loss: 1.4309141635894775
step 9641, loss: 1.44345223903656
step 9642, loss: 1.5337718725204468
step 9643, loss: 1.3207404613494873
step 9644, loss: 1.4241911172866821
step 9645, loss: 1.7010618448257446
step 9646, loss: 1.774686574935913
step 9647, loss: 1.5930523872375488
step 9648, loss: 1.56112539768219
step 9649, loss: 1.4141021966934204
step 9650, loss: 1.7665351629257202
step 9651, loss: 1.9063528776168823
step 9652, loss: 1.6246483325958252
step 9653, loss: 1.8135554790496826
step 9654, loss: 1.5238418579101562
step 9655, loss: 1.8053041696548462
step 9656, loss: 1.4242641925811768
step 9657, loss: 1.6282272338867188
step 9658, loss: 1.8382972478866577
step 9659, loss: 1.893398404121399
step 9660, loss: 1.6301052570343018
step 9661, loss: 1.4996412992477417
step 9662, loss: 1.5615569353103638
step 9663, loss: 1.471298336982727
step 9664, loss: 1.6748226881027222
step 9665, loss: 1.617918848991394
step 9666, loss: 1.4723618030548096
step 9667, loss: 1.5989065170288086
step 9668, loss: 1.8193975687026978
step 9669, loss: 1.3151651620864868
step 9670, loss: 1.2044105529785156
step 9671, loss: 1.330310344696045
step 9672, loss: 1.2390141487121582
step 9673, loss: 1.7577565908432007
step 9674, loss: 1.3024712800979614
step 9675, loss: 1.546838402748108
step 9676, loss: 1.235565423965454
step 9677, loss: 1.4363547563552856
step 9678, loss: 1.3974323272705078
step 9679, loss: 1.2051397562026978
step 9680, loss: 1.5891565084457397
step 9681, loss: 1.7344335317611694
step 9682, loss: 1.4385192394256592
step 9683, loss: 1.4629348516464233
step 9684, loss: 1.307703971862793
step 9685, loss: 1.5983242988586426
step 9686, loss: 1.4522708654403687
step 9687, loss: 1.4503840208053589
step 9688, loss: 1.1420987844467163
step 9689, loss: 1.6353265047073364
step 9690, loss: 1.6713314056396484
step 9691, loss: 1.355078935623169
step 9692, loss: 1.8254519701004028
step 9693, loss: 1.5442196130752563
step 9694, loss: 1.430891513824463
step 9695, loss: 1.3028161525726318
step 9696, loss: 1.8988274335861206
step 9697, loss: 1.5533943176269531
step 9698, loss: 1.4029977321624756
step 9699, loss: 1.6343523263931274
step 9700, loss: 1.6922842264175415
step 9701, loss: 1.2549253702163696
step 9702, loss: 0.9445534348487854
step 9703, loss: 1.18837308883667
step 9704, loss: 1.4139271974563599
step 9705, loss: 1.566860318183899
step 9706, loss: 1.560912013053894
step 9707, loss: 1.6889466047286987
step 9708, loss: 1.6204438209533691
step 9709, loss: 1.413057565689087
step 9710, loss: 1.4510418176651
step 9711, loss: 1.4830657243728638
step 9712, loss: 1.638248085975647
step 9713, loss: 1.1985071897506714
step 9714, loss: 1.6401543617248535
step 9715, loss: 1.559049129486084
step 9716, loss: 1.522315263748169
step 9717, loss: 1.4098554849624634
step 9718, loss: 1.683424711227417
step 9719, loss: 1.6281322240829468
step 9720, loss: 1.1358851194381714
step 9721, loss: 1.4813921451568604
step 9722, loss: 1.3919497728347778
step 9723, loss: 1.2736859321594238
step 9724, loss: 1.338889718055725
step 9725, loss: 1.4802833795547485
step 9726, loss: 1.5341192483901978
step 9727, loss: 1.7292494773864746
step 9728, loss: 1.4677857160568237
step 9729, loss: 1.8812255859375
step 9730, loss: 1.3025810718536377
step 9731, loss: 1.3104976415634155
step 9732, loss: 1.3924583196640015
step 9733, loss: 1.370062232017517
step 9734, loss: 1.2892706394195557
step 9735, loss: 1.0453964471817017
step 9736, loss: 1.2617647647857666
step 9737, loss: 1.3081893920898438
step 9738, loss: 1.57968008518219
step 9739, loss: 0.9491638541221619
step 9740, loss: 1.2579736709594727
step 9741, loss: 1.1034735441207886
step 9742, loss: 1.0561443567276
step 9743, loss: 1.1765546798706055
step 9744, loss: 1.049033522605896
step 9745, loss: 1.2418243885040283
step 9746, loss: 1.1403955221176147
step 9747, loss: 1.1527562141418457
step 9748, loss: 1.3634988069534302
step 9749, loss: 1.454591989517212
step 9750, loss: 1.4494860172271729
step 9751, loss: 1.5261332988739014
step 9752, loss: 1.4888954162597656
step 9753, loss: 1.2124698162078857
step 9754, loss: 1.6941769123077393
step 9755, loss: 1.420307993888855
step 9756, loss: 1.497459053993225
step 9757, loss: 1.1844329833984375
step 9758, loss: 1.4211705923080444
step 9759, loss: 1.4853928089141846
step 9760, loss: 1.736905813217163
step 9761, loss: 1.5672125816345215
step 9762, loss: 1.4238933324813843
step 9763, loss: 1.3201614618301392
step 9764, loss: 1.5706148147583008
step 9765, loss: 1.5215591192245483
step 9766, loss: 1.2413125038146973
step 9767, loss: 1.3723340034484863
step 9768, loss: 1.7582772970199585
step 9769, loss: 1.5274701118469238
step 9770, loss: 1.5501278638839722
step 9771, loss: 1.0391775369644165
step 9772, loss: 1.3175956010818481
step 9773, loss: 1.1245348453521729
step 9774, loss: 1.2955405712127686
step 9775, loss: 1.1556848287582397
step 9776, loss: 1.2286291122436523
step 9777, loss: 1.0961887836456299
step 9778, loss: 1.150256633758545
step 9779, loss: 1.308709979057312
step 9780, loss: 1.225817322731018
step 9781, loss: 1.2191437482833862
step 9782, loss: 1.0587204694747925
step 9783, loss: 1.3408023118972778
step 9784, loss: 1.3984553813934326
step 9785, loss: 1.3063753843307495
step 9786, loss: 1.6011353731155396
step 9787, loss: 1.5870048999786377
step 9788, loss: 1.3451640605926514
step 9789, loss: 1.5982099771499634
step 9790, loss: 1.5118653774261475
step 9791, loss: 1.406422734260559
step 9792, loss: 1.2196741104125977
step 9793, loss: 1.6475502252578735
step 9794, loss: 1.1203759908676147
step 9795, loss: 1.5738970041275024
step 9796, loss: 1.815686583518982
step 9797, loss: 1.6913316249847412
step 9798, loss: 1.9541572332382202
step 9799, loss: 1.6302956342697144
step 9800, loss: 1.6162391901016235
step 9801, loss: 1.5577800273895264
step 9802, loss: 1.4514535665512085
step 9803, loss: 1.5043728351593018
step 9804, loss: 1.6032507419586182
step 9805, loss: 1.2126845121383667
step 9806, loss: 1.6885485649108887
step 9807, loss: 1.1828502416610718
step 9808, loss: 1.4526150226593018
step 9809, loss: 1.2513610124588013
step 9810, loss: 1.5004327297210693
step 9811, loss: 1.1988825798034668
step 9812, loss: 1.5528448820114136
step 9813, loss: 1.4418025016784668
step 9814, loss: 1.1963869333267212
step 9815, loss: 1.2656692266464233
step 9816, loss: 1.1734387874603271
step 9817, loss: 1.4914723634719849
step 9818, loss: 1.3704851865768433
step 9819, loss: 1.3778516054153442
step 9820, loss: 1.329334020614624
step 9821, loss: 1.1618643999099731
step 9822, loss: 1.4086984395980835
step 9823, loss: 1.4125770330429077
step 9824, loss: 1.286295771598816
step 9825, loss: 1.354978084564209
step 9826, loss: 1.1828770637512207
step 9827, loss: 1.0973701477050781
step 9828, loss: 1.2652900218963623
step 9829, loss: 1.3877192735671997
step 9830, loss: 1.2028334140777588
step 9831, loss: 1.4631372690200806
step 9832, loss: 1.1761891841888428
step 9833, loss: 1.5218828916549683
step 9834, loss: 1.2496601343154907
step 9835, loss: 1.3045578002929688
step 9836, loss: 1.284848928451538
step 9837, loss: 1.1622869968414307
step 9838, loss: 1.397199273109436
step 9839, loss: 1.4336034059524536
step 9840, loss: 1.6103146076202393
step 9841, loss: 1.7572966814041138
step 9842, loss: 1.5656018257141113
step 9843, loss: 1.5614551305770874
step 9844, loss: 1.703758955001831
step 9845, loss: 1.4819729328155518
step 9846, loss: 1.2393651008605957
step 9847, loss: 1.2825703620910645
step 9848, loss: 1.7465325593948364
step 9849, loss: 1.3666175603866577
step 9850, loss: 1.4238611459732056
step 9851, loss: 1.306333065032959
step 9852, loss: 1.538121223449707
step 9853, loss: 1.9663331508636475
step 9854, loss: 1.8929100036621094
step 9855, loss: 1.699488878250122
step 9856, loss: 1.5430656671524048
step 9857, loss: 1.6961359977722168
step 9858, loss: 1.766498327255249
step 9859, loss: 1.7203643321990967
step 9860, loss: 1.4112861156463623
step 9861, loss: 1.1923056840896606
step 9862, loss: 1.6024081707000732
step 9863, loss: 1.4265934228897095
step 9864, loss: 1.4880105257034302
step 9865, loss: 1.5425187349319458
step 9866, loss: 1.642037034034729
step 9867, loss: 1.3148987293243408
step 9868, loss: 1.527411699295044
step 9869, loss: 1.458244800567627
step 9870, loss: 1.5849274396896362
step 9871, loss: 1.3408032655715942
step 9872, loss: 1.4546420574188232
step 9873, loss: 1.6101282835006714
step 9874, loss: 1.429695963859558
step 9875, loss: 1.2349202632904053
step 9876, loss: 1.3364472389221191
step 9877, loss: 1.6148306131362915
step 9878, loss: 1.269162654876709
step 9879, loss: 1.0774425268173218
step 9880, loss: 1.1106288433074951
step 9881, loss: 1.4324179887771606
step 9882, loss: 1.3230606317520142
step 9883, loss: 1.5557259321212769
step 9884, loss: 1.7778223752975464
step 9885, loss: 1.4742761850357056
step 9886, loss: 1.5288844108581543
step 9887, loss: 1.3708009719848633
step 9888, loss: 1.5409709215164185
step 9889, loss: 1.2336316108703613
step 9890, loss: 1.4368231296539307
step 9891, loss: 1.3031384944915771
step 9892, loss: 1.615069031715393
step 9893, loss: 1.817564606666565
step 9894, loss: 0.977783203125
step 9895, loss: 1.4866734743118286
step 9896, loss: 1.798821210861206
step 9897, loss: 1.4269428253173828
step 9898, loss: 1.4930295944213867
step 9899, loss: 1.51118803024292
step 9900, loss: 1.8457927703857422
step 9901, loss: 1.264245867729187
step 9902, loss: 1.2644261121749878
step 9903, loss: 1.1996822357177734
step 9904, loss: 1.3833506107330322
step 9905, loss: 1.379232406616211
step 9906, loss: 1.4406293630599976
step 9907, loss: 1.1089298725128174
step 9908, loss: 1.606857419013977
step 9909, loss: 1.2073403596878052
step 9910, loss: 1.4435168504714966
step 9911, loss: 1.3384528160095215
step 9912, loss: 1.232820987701416
step 9913, loss: 1.2143195867538452
step 9914, loss: 1.3148571252822876
step 9915, loss: 1.256435513496399
step 9916, loss: 1.2842637300491333
step 9917, loss: 1.2631608247756958
step 9918, loss: 1.3325382471084595
step 9919, loss: 1.4256178140640259
step 9920, loss: 1.547967791557312
step 9921, loss: 1.205387830734253
step 9922, loss: 1.414326548576355
step 9923, loss: 1.3931676149368286
step 9924, loss: 1.3918211460113525
step 9925, loss: 1.84218430519104
step 9926, loss: 1.185455083847046
step 9927, loss: 1.3519302606582642
step 9928, loss: 1.1341551542282104
step 9929, loss: 1.3189435005187988
step 9930, loss: 1.367388129234314
step 9931, loss: 1.3653347492218018
step 9932, loss: 1.0792683362960815
step 9933, loss: 1.3954908847808838
step 9934, loss: 1.5250684022903442
step 9935, loss: 1.513336420059204
step 9936, loss: 1.3273756504058838
step 9937, loss: 1.1872355937957764
step 9938, loss: 1.4098833799362183
step 9939, loss: 1.3636976480484009
step 9940, loss: 1.3412635326385498
step 9941, loss: 1.1541264057159424
step 9942, loss: 1.483919620513916
step 9943, loss: 1.4706487655639648
step 9944, loss: 1.6186246871948242
step 9945, loss: 1.526649832725525
step 9946, loss: 1.398433804512024
step 9947, loss: 1.2822067737579346
step 9948, loss: 1.4595577716827393
step 9949, loss: 1.1594374179840088
step 9950, loss: 1.0824774503707886
step 9951, loss: 1.3899348974227905
step 9952, loss: 1.3391540050506592
step 9953, loss: 1.3393968343734741
step 9954, loss: 1.3658479452133179
step 9955, loss: 1.3330018520355225
step 9956, loss: 1.2738828659057617
step 9957, loss: 1.0217880010604858
step 9958, loss: 1.1030586957931519
step 9959, loss: 1.3139121532440186
step 9960, loss: 1.1235259771347046
step 9961, loss: 1.3794851303100586
step 9962, loss: 1.340986728668213
step 9963, loss: 1.0171136856079102
step 9964, loss: 0.9337508678436279
step 9965, loss: 1.3622872829437256
step 9966, loss: 0.9172186851501465
step 9967, loss: 1.1026872396469116
step 9968, loss: 1.1985399723052979
step 9969, loss: 0.9267585277557373
step 9970, loss: 1.0617986917495728
step 9971, loss: 1.0783549547195435
step 9972, loss: 1.4284656047821045
step 9973, loss: 1.2649809122085571
step 9974, loss: 1.385633945465088
step 9975, loss: 1.2026201486587524
step 9976, loss: 1.171042561531067
step 9977, loss: 1.093946099281311
step 9978, loss: 1.178773283958435
step 9979, loss: 1.5191311836242676
step 9980, loss: 1.1645852327346802
step 9981, loss: 1.4455665349960327
step 9982, loss: 1.3434382677078247
step 9983, loss: 1.158686637878418
step 9984, loss: 1.4559640884399414
step 9985, loss: 1.421784520149231
step 9986, loss: 1.0145270824432373
step 9987, loss: 1.2584177255630493
step 9988, loss: 1.1008981466293335
step 9989, loss: 0.8924334049224854
step 9990, loss: 1.5086923837661743
step 9991, loss: 1.3035095930099487
step 9992, loss: 1.5160884857177734
step 9993, loss: 1.3083901405334473
step 9994, loss: 1.0552489757537842
step 9995, loss: 1.0486507415771484
step 9996, loss: 1.4034115076065063
step 9997, loss: 1.2166180610656738
step 9998, loss: 1.4622477293014526
step 9999, loss: 1.38993501663208
Final loss: 1.38993501663208
