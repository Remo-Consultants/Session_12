using device: cpu
Loading pretrained GPT-2 model: gpt2
loaded 338025 tokens
1 epoch = 165 batches
Using OneCycleLR scheduler with max_lr=0.0007 and total_steps=10000
step 0, loss: 4.9285, current_lr: 0.000028
Saved best model with loss: 4.9285 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.9285.pt
step 1, loss: 4.4606, current_lr: 0.000028
Saved best model with loss: 4.4606 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.4606.pt
step 2, loss: 4.0646, current_lr: 0.000028
Saved best model with loss: 4.0646 to fine_tuned_gpt2\gpt2_finetuned_best_loss_4.0646.pt
step 3, loss: 4.3851, current_lr: 0.000028
step 4, loss: 4.3394, current_lr: 0.000028
step 5, loss: 4.1568, current_lr: 0.000028
step 6, loss: 4.1171, current_lr: 0.000028
step 7, loss: 4.1456, current_lr: 0.000028
step 8, loss: 3.8273, current_lr: 0.000028
Saved best model with loss: 3.8273 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.8273.pt
step 9, loss: 3.9954, current_lr: 0.000028
step 10, loss: 3.9946, current_lr: 0.000028
step 11, loss: 3.5210, current_lr: 0.000028
Saved best model with loss: 3.5210 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.5210.pt
step 12, loss: 3.9006, current_lr: 0.000028
step 13, loss: 3.8258, current_lr: 0.000028
step 14, loss: 3.8448, current_lr: 0.000028
step 15, loss: 3.5075, current_lr: 0.000028
Saved best model with loss: 3.5075 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.5075.pt
step 16, loss: 3.7529, current_lr: 0.000028
step 17, loss: 3.7006, current_lr: 0.000028
step 18, loss: 3.8213, current_lr: 0.000028
step 19, loss: 3.7428, current_lr: 0.000028
step 20, loss: 4.0522, current_lr: 0.000028
step 21, loss: 3.8356, current_lr: 0.000028
step 22, loss: 3.8521, current_lr: 0.000028
step 23, loss: 3.9134, current_lr: 0.000028
step 24, loss: 3.6694, current_lr: 0.000028
step 25, loss: 3.5051, current_lr: 0.000028
Saved best model with loss: 3.5051 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.5051.pt
step 26, loss: 3.8200, current_lr: 0.000028
step 27, loss: 3.8111, current_lr: 0.000028
step 28, loss: 3.8488, current_lr: 0.000028
step 29, loss: 3.3720, current_lr: 0.000028
Saved best model with loss: 3.3720 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3720.pt
step 30, loss: 3.7659, current_lr: 0.000028
step 31, loss: 3.7902, current_lr: 0.000028
step 32, loss: 3.5454, current_lr: 0.000028
step 33, loss: 3.3803, current_lr: 0.000028
step 34, loss: 3.6105, current_lr: 0.000028
step 35, loss: 3.5515, current_lr: 0.000028
step 36, loss: 3.9309, current_lr: 0.000028
step 37, loss: 3.8781, current_lr: 0.000028
step 38, loss: 3.7904, current_lr: 0.000028
step 39, loss: 3.4271, current_lr: 0.000028
step 40, loss: 4.0309, current_lr: 0.000028
step 41, loss: 3.5446, current_lr: 0.000028
step 42, loss: 3.3731, current_lr: 0.000028
step 43, loss: 3.4532, current_lr: 0.000028
step 44, loss: 3.8297, current_lr: 0.000028
step 45, loss: 3.7358, current_lr: 0.000028
step 46, loss: 3.7729, current_lr: 0.000028
step 47, loss: 4.1466, current_lr: 0.000028
step 48, loss: 3.9948, current_lr: 0.000028
step 49, loss: 4.0316, current_lr: 0.000028
step 50, loss: 3.9911, current_lr: 0.000028
step 51, loss: 4.0517, current_lr: 0.000028
step 52, loss: 3.9405, current_lr: 0.000029
step 53, loss: 3.6925, current_lr: 0.000029
step 54, loss: 3.8100, current_lr: 0.000029
step 55, loss: 4.0469, current_lr: 0.000029
step 56, loss: 3.9549, current_lr: 0.000029
step 57, loss: 3.8169, current_lr: 0.000029
step 58, loss: 3.8368, current_lr: 0.000029
step 59, loss: 3.7843, current_lr: 0.000029
step 60, loss: 3.6581, current_lr: 0.000029
step 61, loss: 3.7035, current_lr: 0.000029
step 62, loss: 3.3486, current_lr: 0.000029
Saved best model with loss: 3.3486 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3486.pt
step 63, loss: 3.8125, current_lr: 0.000029
step 64, loss: 3.3890, current_lr: 0.000029
step 65, loss: 3.8426, current_lr: 0.000029
step 66, loss: 4.0222, current_lr: 0.000029
step 67, loss: 3.9387, current_lr: 0.000029
step 68, loss: 4.0973, current_lr: 0.000029
step 69, loss: 3.7918, current_lr: 0.000029
step 70, loss: 3.5725, current_lr: 0.000029
step 71, loss: 4.0790, current_lr: 0.000029
step 72, loss: 3.5115, current_lr: 0.000029
step 73, loss: 3.8014, current_lr: 0.000029
step 74, loss: 3.6436, current_lr: 0.000029
step 75, loss: 3.9169, current_lr: 0.000029
step 76, loss: 3.6830, current_lr: 0.000029
step 77, loss: 3.6686, current_lr: 0.000029
step 78, loss: 3.5063, current_lr: 0.000029
step 79, loss: 3.5262, current_lr: 0.000029
step 80, loss: 3.6661, current_lr: 0.000029
step 81, loss: 3.5856, current_lr: 0.000029
step 82, loss: 3.9561, current_lr: 0.000029
step 83, loss: 3.7593, current_lr: 0.000029
step 84, loss: 3.7439, current_lr: 0.000029
step 85, loss: 3.3404, current_lr: 0.000029
Saved best model with loss: 3.3404 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3404.pt
step 86, loss: 3.3253, current_lr: 0.000029
Saved best model with loss: 3.3253 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.3253.pt
step 87, loss: 3.7132, current_lr: 0.000029
step 88, loss: 3.8619, current_lr: 0.000029
step 89, loss: 3.9628, current_lr: 0.000029
step 90, loss: 3.6057, current_lr: 0.000030
step 91, loss: 3.8780, current_lr: 0.000030
step 92, loss: 3.8212, current_lr: 0.000030
step 93, loss: 3.5296, current_lr: 0.000030
step 94, loss: 3.0305, current_lr: 0.000030
Saved best model with loss: 3.0305 to fine_tuned_gpt2\gpt2_finetuned_best_loss_3.0305.pt
step 95, loss: 3.6701, current_lr: 0.000030
step 96, loss: 3.6071, current_lr: 0.000030
step 97, loss: 3.7040, current_lr: 0.000030
step 98, loss: 3.5210, current_lr: 0.000030
step 99, loss: 3.4597, current_lr: 0.000030
step 100, loss: 3.4766, current_lr: 0.000030
step 101, loss: 3.7787, current_lr: 0.000030
step 102, loss: 3.3690, current_lr: 0.000030
step 103, loss: 3.6785, current_lr: 0.000030
step 104, loss: 4.1530, current_lr: 0.000030
step 105, loss: 4.2381, current_lr: 0.000030
step 106, loss: 4.1450, current_lr: 0.000030
step 107, loss: 4.0363, current_lr: 0.000030
step 108, loss: 4.1057, current_lr: 0.000030
step 109, loss: 3.9132, current_lr: 0.000030
step 110, loss: 4.1087, current_lr: 0.000030
step 111, loss: 4.1662, current_lr: 0.000030
step 112, loss: 4.1219, current_lr: 0.000030
step 113, loss: 4.2024, current_lr: 0.000030
step 114, loss: 3.9696, current_lr: 0.000030
step 115, loss: 4.1712, current_lr: 0.000030
step 116, loss: 3.7718, current_lr: 0.000031
step 117, loss: 3.9838, current_lr: 0.000031
step 118, loss: 3.8847, current_lr: 0.000031
step 119, loss: 3.7684, current_lr: 0.000031
step 120, loss: 3.9822, current_lr: 0.000031
step 121, loss: 4.0525, current_lr: 0.000031
step 122, loss: 3.8908, current_lr: 0.000031
step 123, loss: 3.9893, current_lr: 0.000031
step 124, loss: 3.4646, current_lr: 0.000031
step 125, loss: 3.9613, current_lr: 0.000031
step 126, loss: 3.6398, current_lr: 0.000031
step 127, loss: 3.2813, current_lr: 0.000031
step 128, loss: 3.4791, current_lr: 0.000031
step 129, loss: 3.6168, current_lr: 0.000031
step 130, loss: 3.7359, current_lr: 0.000031
step 131, loss: 3.8189, current_lr: 0.000031
step 132, loss: 3.7838, current_lr: 0.000031
step 133, loss: 3.4099, current_lr: 0.000031
step 134, loss: 3.9136, current_lr: 0.000031
step 135, loss: 3.4170, current_lr: 0.000031
step 136, loss: 3.5586, current_lr: 0.000031
step 137, loss: 3.6055, current_lr: 0.000032
step 138, loss: 3.4511, current_lr: 0.000032
step 139, loss: 3.4839, current_lr: 0.000032
step 140, loss: 3.4384, current_lr: 0.000032
step 141, loss: 3.5932, current_lr: 0.000032
step 142, loss: 4.0499, current_lr: 0.000032
step 143, loss: 3.9207, current_lr: 0.000032
step 144, loss: 3.8933, current_lr: 0.000032
step 145, loss: 3.9022, current_lr: 0.000032
step 146, loss: 3.6710, current_lr: 0.000032
step 147, loss: 3.5949, current_lr: 0.000032
step 148, loss: 3.2464, current_lr: 0.000032
step 149, loss: 3.7826, current_lr: 0.000032
step 150, loss: 3.7955, current_lr: 0.000032
step 151, loss: 3.6898, current_lr: 0.000032
step 152, loss: 3.4592, current_lr: 0.000032
step 153, loss: 3.5538, current_lr: 0.000032
step 154, loss: 3.3230, current_lr: 0.000032
step 155, loss: 3.3288, current_lr: 0.000032
step 156, loss: 3.3984, current_lr: 0.000033
step 157, loss: 3.2722, current_lr: 0.000033
step 158, loss: 3.0447, current_lr: 0.000033
step 159, loss: 3.6175, current_lr: 0.000033
step 160, loss: 3.9180, current_lr: 0.000033
step 161, loss: 3.9631, current_lr: 0.000033
step 162, loss: 4.0681, current_lr: 0.000033
step 163, loss: 3.4898, current_lr: 0.000033
step 164, loss: 3.4687, current_lr: 0.000033
step 165, loss: 3.7182, current_lr: 0.000033
step 166, loss: 3.6551, current_lr: 0.000033
step 167, loss: 3.3823, current_lr: 0.000033
step 168, loss: 3.7169, current_lr: 0.000033
step 169, loss: 3.7340, current_lr: 0.000033
step 170, loss: 3.5807, current_lr: 0.000033
step 171, loss: 3.5571, current_lr: 0.000033
step 172, loss: 3.6698, current_lr: 0.000034
step 173, loss: 3.3743, current_lr: 0.000034
step 174, loss: 3.5151, current_lr: 0.000034
step 175, loss: 3.5081, current_lr: 0.000034
step 176, loss: 3.0638, current_lr: 0.000034
step 177, loss: 3.4744, current_lr: 0.000034
step 178, loss: 3.3915, current_lr: 0.000034
step 179, loss: 3.4524, current_lr: 0.000034
step 180, loss: 3.1486, current_lr: 0.000034
step 181, loss: 3.3775, current_lr: 0.000034
step 182, loss: 3.3177, current_lr: 0.000034
step 183, loss: 3.5074, current_lr: 0.000034
step 184, loss: 3.4587, current_lr: 0.000034
step 185, loss: 3.7227, current_lr: 0.000034
step 186, loss: 3.5203, current_lr: 0.000034
step 187, loss: 3.4749, current_lr: 0.000034
step 188, loss: 3.5616, current_lr: 0.000035
step 189, loss: 3.3363, current_lr: 0.000035
step 190, loss: 3.2089, current_lr: 0.000035
step 191, loss: 3.4073, current_lr: 0.000035
step 192, loss: 3.4427, current_lr: 0.000035
step 193, loss: 3.5028, current_lr: 0.000035
step 194, loss: 3.1156, current_lr: 0.000035
step 195, loss: 3.4547, current_lr: 0.000035
step 196, loss: 3.4297, current_lr: 0.000035
step 197, loss: 3.1529, current_lr: 0.000035
step 198, loss: 3.0589, current_lr: 0.000035
step 199, loss: 3.2848, current_lr: 0.000035
step 200, loss: 3.2269, current_lr: 0.000035
step 201, loss: 3.6253, current_lr: 0.000035
step 202, loss: 3.5967, current_lr: 0.000036
step 203, loss: 3.4594, current_lr: 0.000036
step 204, loss: 3.1060, current_lr: 0.000036
step 205, loss: 3.6774, current_lr: 0.000036
step 206, loss: 3.2071, current_lr: 0.000036
step 207, loss: 3.1017, current_lr: 0.000036
step 208, loss: 3.1617, current_lr: 0.000036
step 209, loss: 3.5500, current_lr: 0.000036
step 210, loss: 3.4604, current_lr: 0.000036
step 211, loss: 3.4972, current_lr: 0.000036
step 212, loss: 3.8454, current_lr: 0.000036
step 213, loss: 3.6857, current_lr: 0.000036
step 214, loss: 3.7441, current_lr: 0.000036
step 215, loss: 3.7050, current_lr: 0.000037
step 216, loss: 3.8004, current_lr: 0.000037
step 217, loss: 3.6623, current_lr: 0.000037
step 218, loss: 3.4108, current_lr: 0.000037
step 219, loss: 3.4552, current_lr: 0.000037
step 220, loss: 3.7873, current_lr: 0.000037
step 221, loss: 3.6632, current_lr: 0.000037
step 222, loss: 3.5689, current_lr: 0.000037
step 223, loss: 3.5757, current_lr: 0.000037
step 224, loss: 3.4998, current_lr: 0.000037
step 225, loss: 3.4123, current_lr: 0.000037
step 226, loss: 3.4493, current_lr: 0.000037
step 227, loss: 2.9963, current_lr: 0.000038
Saved best model with loss: 2.9963 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.9963.pt
step 228, loss: 3.5546, current_lr: 0.000038
step 229, loss: 3.1746, current_lr: 0.000038
step 230, loss: 3.5017, current_lr: 0.000038
step 231, loss: 3.6319, current_lr: 0.000038
step 232, loss: 3.5901, current_lr: 0.000038
step 233, loss: 3.7493, current_lr: 0.000038
step 234, loss: 3.4728, current_lr: 0.000038
step 235, loss: 3.2990, current_lr: 0.000038
step 236, loss: 3.7352, current_lr: 0.000038
step 237, loss: 3.2526, current_lr: 0.000038
step 238, loss: 3.4950, current_lr: 0.000038
step 239, loss: 3.3190, current_lr: 0.000039
step 240, loss: 3.6508, current_lr: 0.000039
step 241, loss: 3.3806, current_lr: 0.000039
step 242, loss: 3.4078, current_lr: 0.000039
step 243, loss: 3.2432, current_lr: 0.000039
step 244, loss: 3.2832, current_lr: 0.000039
step 245, loss: 3.3844, current_lr: 0.000039
step 246, loss: 3.3398, current_lr: 0.000039
step 247, loss: 3.7065, current_lr: 0.000039
step 248, loss: 3.4929, current_lr: 0.000039
step 249, loss: 3.5043, current_lr: 0.000039
step 250, loss: 3.0754, current_lr: 0.000040
step 251, loss: 3.1041, current_lr: 0.000040
step 252, loss: 3.4855, current_lr: 0.000040
step 253, loss: 3.6472, current_lr: 0.000040
step 254, loss: 3.7322, current_lr: 0.000040
step 255, loss: 3.3560, current_lr: 0.000040
step 256, loss: 3.6558, current_lr: 0.000040
step 257, loss: 3.5910, current_lr: 0.000040
step 258, loss: 3.2840, current_lr: 0.000040
step 259, loss: 2.8411, current_lr: 0.000040
Saved best model with loss: 2.8411 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.8411.pt
step 260, loss: 3.4266, current_lr: 0.000040
step 261, loss: 3.3487, current_lr: 0.000041
step 262, loss: 3.4692, current_lr: 0.000041
step 263, loss: 3.3017, current_lr: 0.000041
step 264, loss: 3.2707, current_lr: 0.000041
step 265, loss: 3.2535, current_lr: 0.000041
step 266, loss: 3.5403, current_lr: 0.000041
step 267, loss: 3.1720, current_lr: 0.000041
step 268, loss: 3.4813, current_lr: 0.000041
step 269, loss: 3.8229, current_lr: 0.000041
step 270, loss: 3.9403, current_lr: 0.000041
step 271, loss: 3.8049, current_lr: 0.000042
step 272, loss: 3.7491, current_lr: 0.000042
step 273, loss: 3.8340, current_lr: 0.000042
step 274, loss: 3.6127, current_lr: 0.000042
step 275, loss: 3.8516, current_lr: 0.000042
step 276, loss: 3.8861, current_lr: 0.000042
step 277, loss: 3.8970, current_lr: 0.000042
step 278, loss: 3.9331, current_lr: 0.000042
step 279, loss: 3.6529, current_lr: 0.000042
step 280, loss: 3.9046, current_lr: 0.000042
step 281, loss: 3.4680, current_lr: 0.000043
step 282, loss: 3.7039, current_lr: 0.000043
step 283, loss: 3.5809, current_lr: 0.000043
step 284, loss: 3.4897, current_lr: 0.000043
step 285, loss: 3.7646, current_lr: 0.000043
step 286, loss: 3.7989, current_lr: 0.000043
step 287, loss: 3.6593, current_lr: 0.000043
step 288, loss: 3.7872, current_lr: 0.000043
step 289, loss: 3.2423, current_lr: 0.000043
step 290, loss: 3.8010, current_lr: 0.000043
step 291, loss: 3.4060, current_lr: 0.000044
step 292, loss: 3.0035, current_lr: 0.000044
step 293, loss: 3.2366, current_lr: 0.000044
step 294, loss: 3.4195, current_lr: 0.000044
step 295, loss: 3.5251, current_lr: 0.000044
step 296, loss: 3.5920, current_lr: 0.000044
step 297, loss: 3.5254, current_lr: 0.000044
step 298, loss: 3.1760, current_lr: 0.000044
step 299, loss: 3.6626, current_lr: 0.000044
step 300, loss: 3.1680, current_lr: 0.000045
step 301, loss: 3.3363, current_lr: 0.000045
step 302, loss: 3.3933, current_lr: 0.000045
step 303, loss: 3.2489, current_lr: 0.000045
step 304, loss: 3.2952, current_lr: 0.000045
step 305, loss: 3.2225, current_lr: 0.000045
step 306, loss: 3.3779, current_lr: 0.000045
step 307, loss: 3.8675, current_lr: 0.000045
step 308, loss: 3.6687, current_lr: 0.000045
step 309, loss: 3.5648, current_lr: 0.000046
step 310, loss: 3.5086, current_lr: 0.000046
step 311, loss: 3.2606, current_lr: 0.000046
step 312, loss: 3.2207, current_lr: 0.000046
step 313, loss: 2.9628, current_lr: 0.000046
step 314, loss: 3.4424, current_lr: 0.000046
step 315, loss: 3.4660, current_lr: 0.000046
step 316, loss: 3.3860, current_lr: 0.000046
step 317, loss: 3.2026, current_lr: 0.000046
step 318, loss: 3.2883, current_lr: 0.000047
step 319, loss: 3.0420, current_lr: 0.000047
step 320, loss: 3.0901, current_lr: 0.000047
step 321, loss: 3.1344, current_lr: 0.000047
step 322, loss: 2.9707, current_lr: 0.000047
step 323, loss: 2.7827, current_lr: 0.000047
Saved best model with loss: 2.7827 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.7827.pt
step 324, loss: 3.4075, current_lr: 0.000047
step 325, loss: 3.6979, current_lr: 0.000047
step 326, loss: 3.7417, current_lr: 0.000048
step 327, loss: 3.8339, current_lr: 0.000048
step 328, loss: 3.2396, current_lr: 0.000048
step 329, loss: 3.3042, current_lr: 0.000048
step 330, loss: 3.5949, current_lr: 0.000048
step 331, loss: 3.5045, current_lr: 0.000048
step 332, loss: 3.1923, current_lr: 0.000048
step 333, loss: 3.5527, current_lr: 0.000048
step 334, loss: 3.5879, current_lr: 0.000048
step 335, loss: 3.4341, current_lr: 0.000049
step 336, loss: 3.4087, current_lr: 0.000049
step 337, loss: 3.5215, current_lr: 0.000049
step 338, loss: 3.2521, current_lr: 0.000049
step 339, loss: 3.3792, current_lr: 0.000049
step 340, loss: 3.3783, current_lr: 0.000049
step 341, loss: 2.9364, current_lr: 0.000049
step 342, loss: 3.3574, current_lr: 0.000049
step 343, loss: 3.2520, current_lr: 0.000050
step 344, loss: 3.3419, current_lr: 0.000050
step 345, loss: 3.0124, current_lr: 0.000050
step 346, loss: 3.2253, current_lr: 0.000050
step 347, loss: 3.1564, current_lr: 0.000050
step 348, loss: 3.3853, current_lr: 0.000050
step 349, loss: 3.3267, current_lr: 0.000050
step 350, loss: 3.5841, current_lr: 0.000050
step 351, loss: 3.3783, current_lr: 0.000051
step 352, loss: 3.3136, current_lr: 0.000051
step 353, loss: 3.4455, current_lr: 0.000051
step 354, loss: 3.2120, current_lr: 0.000051
step 355, loss: 3.1051, current_lr: 0.000051
step 356, loss: 3.2940, current_lr: 0.000051
step 357, loss: 3.3132, current_lr: 0.000051
step 358, loss: 3.3500, current_lr: 0.000051
step 359, loss: 2.9828, current_lr: 0.000052
step 360, loss: 3.3241, current_lr: 0.000052
step 361, loss: 3.3004, current_lr: 0.000052
step 362, loss: 3.0265, current_lr: 0.000052
step 363, loss: 2.9738, current_lr: 0.000052
step 364, loss: 3.1608, current_lr: 0.000052
step 365, loss: 3.0742, current_lr: 0.000052
step 366, loss: 3.4743, current_lr: 0.000053
step 367, loss: 3.4506, current_lr: 0.000053
step 368, loss: 3.3015, current_lr: 0.000053
step 369, loss: 2.9816, current_lr: 0.000053
step 370, loss: 3.5224, current_lr: 0.000053
step 371, loss: 3.0796, current_lr: 0.000053
step 372, loss: 2.9766, current_lr: 0.000053
step 373, loss: 3.0110, current_lr: 0.000053
step 374, loss: 3.4021, current_lr: 0.000054
step 375, loss: 3.3041, current_lr: 0.000054
step 376, loss: 3.3606, current_lr: 0.000054
step 377, loss: 3.6995, current_lr: 0.000054
step 378, loss: 3.5201, current_lr: 0.000054
step 379, loss: 3.5651, current_lr: 0.000054
step 380, loss: 3.5399, current_lr: 0.000054
step 381, loss: 3.6167, current_lr: 0.000055
step 382, loss: 3.4772, current_lr: 0.000055
step 383, loss: 3.2333, current_lr: 0.000055
step 384, loss: 3.2987, current_lr: 0.000055
step 385, loss: 3.6094, current_lr: 0.000055
step 386, loss: 3.4806, current_lr: 0.000055
step 387, loss: 3.4243, current_lr: 0.000055
step 388, loss: 3.3975, current_lr: 0.000056
step 389, loss: 3.3322, current_lr: 0.000056
step 390, loss: 3.2757, current_lr: 0.000056
step 391, loss: 3.2911, current_lr: 0.000056
step 392, loss: 2.8593, current_lr: 0.000056
step 393, loss: 3.4128, current_lr: 0.000056
step 394, loss: 3.0395, current_lr: 0.000056
step 395, loss: 3.3614, current_lr: 0.000056
step 396, loss: 3.4761, current_lr: 0.000057
step 397, loss: 3.4365, current_lr: 0.000057
step 398, loss: 3.5868, current_lr: 0.000057
step 399, loss: 3.3294, current_lr: 0.000057
step 400, loss: 3.1614, current_lr: 0.000057
step 401, loss: 3.5850, current_lr: 0.000057
step 402, loss: 3.1300, current_lr: 0.000057
step 403, loss: 3.3313, current_lr: 0.000058
step 404, loss: 3.1640, current_lr: 0.000058
step 405, loss: 3.5140, current_lr: 0.000058
step 406, loss: 3.2330, current_lr: 0.000058
step 407, loss: 3.2570, current_lr: 0.000058
step 408, loss: 3.0782, current_lr: 0.000058
step 409, loss: 3.1531, current_lr: 0.000059
step 410, loss: 3.2316, current_lr: 0.000059
step 411, loss: 3.1638, current_lr: 0.000059
step 412, loss: 3.5366, current_lr: 0.000059
step 413, loss: 3.3313, current_lr: 0.000059
step 414, loss: 3.3855, current_lr: 0.000059
step 415, loss: 2.9713, current_lr: 0.000059
step 416, loss: 3.0028, current_lr: 0.000060
step 417, loss: 3.3368, current_lr: 0.000060
step 418, loss: 3.5095, current_lr: 0.000060
step 419, loss: 3.5769, current_lr: 0.000060
step 420, loss: 3.2248, current_lr: 0.000060
step 421, loss: 3.4992, current_lr: 0.000060
step 422, loss: 3.4335, current_lr: 0.000060
step 423, loss: 3.1281, current_lr: 0.000061
step 424, loss: 2.7500, current_lr: 0.000061
Saved best model with loss: 2.7500 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.7500.pt
step 425, loss: 3.2700, current_lr: 0.000061
step 426, loss: 3.1862, current_lr: 0.000061
step 427, loss: 3.3114, current_lr: 0.000061
step 428, loss: 3.1522, current_lr: 0.000061
step 429, loss: 3.1151, current_lr: 0.000062
step 430, loss: 3.1019, current_lr: 0.000062
step 431, loss: 3.3663, current_lr: 0.000062
step 432, loss: 3.0301, current_lr: 0.000062
step 433, loss: 3.3168, current_lr: 0.000062
step 434, loss: 3.6530, current_lr: 0.000062
step 435, loss: 3.7740, current_lr: 0.000062
step 436, loss: 3.6433, current_lr: 0.000063
step 437, loss: 3.5655, current_lr: 0.000063
step 438, loss: 3.6715, current_lr: 0.000063
step 439, loss: 3.4472, current_lr: 0.000063
step 440, loss: 3.6842, current_lr: 0.000063
step 441, loss: 3.7157, current_lr: 0.000063
step 442, loss: 3.7359, current_lr: 0.000064
step 443, loss: 3.7702, current_lr: 0.000064
step 444, loss: 3.4708, current_lr: 0.000064
step 445, loss: 3.7188, current_lr: 0.000064
step 446, loss: 3.2971, current_lr: 0.000064
step 447, loss: 3.5578, current_lr: 0.000064
step 448, loss: 3.4119, current_lr: 0.000064
step 449, loss: 3.3202, current_lr: 0.000065
step 450, loss: 3.6142, current_lr: 0.000065
step 451, loss: 3.6339, current_lr: 0.000065
step 452, loss: 3.4987, current_lr: 0.000065
step 453, loss: 3.6289, current_lr: 0.000065
step 454, loss: 3.0437, current_lr: 0.000065
step 455, loss: 3.6150, current_lr: 0.000066
step 456, loss: 3.2290, current_lr: 0.000066
step 457, loss: 2.8170, current_lr: 0.000066
step 458, loss: 3.0903, current_lr: 0.000066
step 459, loss: 3.2886, current_lr: 0.000066
step 460, loss: 3.3831, current_lr: 0.000066
step 461, loss: 3.4456, current_lr: 0.000067
step 462, loss: 3.3863, current_lr: 0.000067
step 463, loss: 3.0536, current_lr: 0.000067
step 464, loss: 3.5399, current_lr: 0.000067
step 465, loss: 3.0097, current_lr: 0.000067
step 466, loss: 3.1661, current_lr: 0.000067
step 467, loss: 3.2675, current_lr: 0.000068
step 468, loss: 3.1009, current_lr: 0.000068
step 469, loss: 3.1513, current_lr: 0.000068
step 470, loss: 3.0813, current_lr: 0.000068
step 471, loss: 3.2376, current_lr: 0.000068
step 472, loss: 3.7055, current_lr: 0.000068
step 473, loss: 3.4969, current_lr: 0.000069
step 474, loss: 3.3767, current_lr: 0.000069
step 475, loss: 3.3070, current_lr: 0.000069
step 476, loss: 3.0865, current_lr: 0.000069
step 477, loss: 3.0747, current_lr: 0.000069
step 478, loss: 2.8323, current_lr: 0.000069
step 479, loss: 3.2760, current_lr: 0.000070
step 480, loss: 3.2691, current_lr: 0.000070
step 481, loss: 3.2213, current_lr: 0.000070
step 482, loss: 3.0337, current_lr: 0.000070
step 483, loss: 3.1153, current_lr: 0.000070
step 484, loss: 2.8614, current_lr: 0.000070
step 485, loss: 2.9225, current_lr: 0.000071
step 486, loss: 2.9478, current_lr: 0.000071
step 487, loss: 2.8050, current_lr: 0.000071
step 488, loss: 2.6288, current_lr: 0.000071
Saved best model with loss: 2.6288 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.6288.pt
step 489, loss: 3.2249, current_lr: 0.000071
step 490, loss: 3.5384, current_lr: 0.000071
step 491, loss: 3.5741, current_lr: 0.000072
step 492, loss: 3.6247, current_lr: 0.000072
step 493, loss: 3.0546, current_lr: 0.000072
step 494, loss: 3.1203, current_lr: 0.000072
step 495, loss: 3.4592, current_lr: 0.000072
step 496, loss: 3.3771, current_lr: 0.000073
step 497, loss: 3.0835, current_lr: 0.000073
step 498, loss: 3.4121, current_lr: 0.000073
step 499, loss: 3.4631, current_lr: 0.000073
step 500, loss: 3.3089, current_lr: 0.000073
step 501, loss: 3.2816, current_lr: 0.000073
step 502, loss: 3.3734, current_lr: 0.000074
step 503, loss: 3.1242, current_lr: 0.000074
step 504, loss: 3.2441, current_lr: 0.000074
step 505, loss: 3.2485, current_lr: 0.000074
step 506, loss: 2.8018, current_lr: 0.000074
step 507, loss: 3.2265, current_lr: 0.000074
step 508, loss: 3.1033, current_lr: 0.000075
step 509, loss: 3.2142, current_lr: 0.000075
step 510, loss: 2.8836, current_lr: 0.000075
step 511, loss: 3.0882, current_lr: 0.000075
step 512, loss: 3.0028, current_lr: 0.000075
step 513, loss: 3.2316, current_lr: 0.000076
step 514, loss: 3.1814, current_lr: 0.000076
step 515, loss: 3.4426, current_lr: 0.000076
step 516, loss: 3.2186, current_lr: 0.000076
step 517, loss: 3.1556, current_lr: 0.000076
step 518, loss: 3.2949, current_lr: 0.000076
step 519, loss: 3.0754, current_lr: 0.000077
step 520, loss: 2.9734, current_lr: 0.000077
step 521, loss: 3.1519, current_lr: 0.000077
step 522, loss: 3.1522, current_lr: 0.000077
step 523, loss: 3.1915, current_lr: 0.000077
step 524, loss: 2.8303, current_lr: 0.000078
step 525, loss: 3.1681, current_lr: 0.000078
step 526, loss: 3.1420, current_lr: 0.000078
step 527, loss: 2.9086, current_lr: 0.000078
step 528, loss: 2.7889, current_lr: 0.000078
step 529, loss: 3.0040, current_lr: 0.000078
step 530, loss: 2.9100, current_lr: 0.000079
step 531, loss: 3.3034, current_lr: 0.000079
step 532, loss: 3.2788, current_lr: 0.000079
step 533, loss: 3.1259, current_lr: 0.000079
step 534, loss: 2.8326, current_lr: 0.000079
step 535, loss: 3.3483, current_lr: 0.000080
step 536, loss: 2.9238, current_lr: 0.000080
step 537, loss: 2.8384, current_lr: 0.000080
step 538, loss: 2.8311, current_lr: 0.000080
step 539, loss: 3.2256, current_lr: 0.000080
step 540, loss: 3.1225, current_lr: 0.000081
step 541, loss: 3.1923, current_lr: 0.000081
step 542, loss: 3.5116, current_lr: 0.000081
step 543, loss: 3.3249, current_lr: 0.000081
step 544, loss: 3.3475, current_lr: 0.000081
step 545, loss: 3.3370, current_lr: 0.000081
step 546, loss: 3.4365, current_lr: 0.000082
step 547, loss: 3.2892, current_lr: 0.000082
step 548, loss: 3.0654, current_lr: 0.000082
step 549, loss: 3.1168, current_lr: 0.000082
step 550, loss: 3.4061, current_lr: 0.000082
step 551, loss: 3.2817, current_lr: 0.000083
step 552, loss: 3.2443, current_lr: 0.000083
step 553, loss: 3.1921, current_lr: 0.000083
step 554, loss: 3.1325, current_lr: 0.000083
step 555, loss: 3.1050, current_lr: 0.000083
step 556, loss: 3.1200, current_lr: 0.000084
step 557, loss: 2.7049, current_lr: 0.000084
step 558, loss: 3.2279, current_lr: 0.000084
step 559, loss: 2.8726, current_lr: 0.000084
step 560, loss: 3.1735, current_lr: 0.000084
step 561, loss: 3.2958, current_lr: 0.000085
step 562, loss: 3.2538, current_lr: 0.000085
step 563, loss: 3.3828, current_lr: 0.000085
step 564, loss: 3.1383, current_lr: 0.000085
step 565, loss: 2.9925, current_lr: 0.000085
step 566, loss: 3.3795, current_lr: 0.000086
step 567, loss: 2.9650, current_lr: 0.000086
step 568, loss: 3.1365, current_lr: 0.000086
step 569, loss: 2.9553, current_lr: 0.000086
step 570, loss: 3.3098, current_lr: 0.000086
step 571, loss: 3.0422, current_lr: 0.000087
step 572, loss: 3.0665, current_lr: 0.000087
step 573, loss: 2.9222, current_lr: 0.000087
step 574, loss: 2.9875, current_lr: 0.000087
step 575, loss: 3.0542, current_lr: 0.000087
step 576, loss: 3.0094, current_lr: 0.000088
step 577, loss: 3.3358, current_lr: 0.000088
step 578, loss: 3.1676, current_lr: 0.000088
step 579, loss: 3.2044, current_lr: 0.000088
step 580, loss: 2.8202, current_lr: 0.000088
step 581, loss: 2.8482, current_lr: 0.000089
step 582, loss: 3.1444, current_lr: 0.000089
step 583, loss: 3.3169, current_lr: 0.000089
step 584, loss: 3.3737, current_lr: 0.000089
step 585, loss: 3.0251, current_lr: 0.000089
step 586, loss: 3.2955, current_lr: 0.000090
step 587, loss: 3.2375, current_lr: 0.000090
step 588, loss: 2.9424, current_lr: 0.000090
step 589, loss: 2.5633, current_lr: 0.000090
Saved best model with loss: 2.5633 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.5633.pt
step 590, loss: 3.0757, current_lr: 0.000090
step 591, loss: 2.9940, current_lr: 0.000091
step 592, loss: 3.1077, current_lr: 0.000091
step 593, loss: 2.9817, current_lr: 0.000091
step 594, loss: 2.9142, current_lr: 0.000091
step 595, loss: 2.9035, current_lr: 0.000091
step 596, loss: 3.1422, current_lr: 0.000092
step 597, loss: 2.8235, current_lr: 0.000092
step 598, loss: 3.1046, current_lr: 0.000092
step 599, loss: 3.4312, current_lr: 0.000092
step 600, loss: 3.5448, current_lr: 0.000092
step 601, loss: 3.4086, current_lr: 0.000093
step 602, loss: 3.3375, current_lr: 0.000093
step 603, loss: 3.4601, current_lr: 0.000093
step 604, loss: 3.2384, current_lr: 0.000093
step 605, loss: 3.4646, current_lr: 0.000093
step 606, loss: 3.5080, current_lr: 0.000094
step 607, loss: 3.5043, current_lr: 0.000094
step 608, loss: 3.5593, current_lr: 0.000094
step 609, loss: 3.2699, current_lr: 0.000094
step 610, loss: 3.4771, current_lr: 0.000095
step 611, loss: 3.1030, current_lr: 0.000095
step 612, loss: 3.3545, current_lr: 0.000095
step 613, loss: 3.2106, current_lr: 0.000095
step 614, loss: 3.0950, current_lr: 0.000095
step 615, loss: 3.4290, current_lr: 0.000096
step 616, loss: 3.4357, current_lr: 0.000096
step 617, loss: 3.3000, current_lr: 0.000096
step 618, loss: 3.4422, current_lr: 0.000096
step 619, loss: 2.8618, current_lr: 0.000096
step 620, loss: 3.4350, current_lr: 0.000097
step 621, loss: 3.0489, current_lr: 0.000097
step 622, loss: 2.6177, current_lr: 0.000097
step 623, loss: 2.9192, current_lr: 0.000097
step 624, loss: 3.1181, current_lr: 0.000097
step 625, loss: 3.1956, current_lr: 0.000098
step 626, loss: 3.2467, current_lr: 0.000098
step 627, loss: 3.1708, current_lr: 0.000098
step 628, loss: 2.8521, current_lr: 0.000098
step 629, loss: 3.3274, current_lr: 0.000099
step 630, loss: 2.8261, current_lr: 0.000099
step 631, loss: 2.9420, current_lr: 0.000099
step 632, loss: 3.0655, current_lr: 0.000099
step 633, loss: 2.9048, current_lr: 0.000099
step 634, loss: 2.9861, current_lr: 0.000100
step 635, loss: 2.8925, current_lr: 0.000100
step 636, loss: 3.0407, current_lr: 0.000100
step 637, loss: 3.4722, current_lr: 0.000100
step 638, loss: 3.3015, current_lr: 0.000101
step 639, loss: 3.1948, current_lr: 0.000101
step 640, loss: 3.0917, current_lr: 0.000101
step 641, loss: 2.8931, current_lr: 0.000101
step 642, loss: 2.9038, current_lr: 0.000101
step 643, loss: 2.6691, current_lr: 0.000102
step 644, loss: 3.0691, current_lr: 0.000102
step 645, loss: 3.0348, current_lr: 0.000102
step 646, loss: 3.0239, current_lr: 0.000102
step 647, loss: 2.8633, current_lr: 0.000102
step 648, loss: 2.9347, current_lr: 0.000103
step 649, loss: 2.6771, current_lr: 0.000103
step 650, loss: 2.7359, current_lr: 0.000103
step 651, loss: 2.7692, current_lr: 0.000103
step 652, loss: 2.6040, current_lr: 0.000104
step 653, loss: 2.4582, current_lr: 0.000104
Saved best model with loss: 2.4582 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.4582.pt
step 654, loss: 3.0057, current_lr: 0.000104
step 655, loss: 3.3353, current_lr: 0.000104
step 656, loss: 3.3412, current_lr: 0.000104
step 657, loss: 3.3764, current_lr: 0.000105
step 658, loss: 2.8705, current_lr: 0.000105
step 659, loss: 2.9411, current_lr: 0.000105
step 660, loss: 3.2475, current_lr: 0.000105
step 661, loss: 3.1661, current_lr: 0.000106
step 662, loss: 2.9083, current_lr: 0.000106
step 663, loss: 3.2112, current_lr: 0.000106
step 664, loss: 3.2872, current_lr: 0.000106
step 665, loss: 3.0996, current_lr: 0.000107
step 666, loss: 3.0735, current_lr: 0.000107
step 667, loss: 3.1554, current_lr: 0.000107
step 668, loss: 2.9005, current_lr: 0.000107
step 669, loss: 3.0405, current_lr: 0.000107
step 670, loss: 3.0498, current_lr: 0.000108
step 671, loss: 2.6106, current_lr: 0.000108
step 672, loss: 3.0231, current_lr: 0.000108
step 673, loss: 2.8891, current_lr: 0.000108
step 674, loss: 3.0105, current_lr: 0.000109
step 675, loss: 2.6974, current_lr: 0.000109
step 676, loss: 2.8922, current_lr: 0.000109
step 677, loss: 2.7918, current_lr: 0.000109
step 678, loss: 3.0232, current_lr: 0.000109
step 679, loss: 2.9882, current_lr: 0.000110
step 680, loss: 3.2480, current_lr: 0.000110
step 681, loss: 3.0012, current_lr: 0.000110
step 682, loss: 2.9471, current_lr: 0.000110
step 683, loss: 3.0791, current_lr: 0.000111
step 684, loss: 2.8265, current_lr: 0.000111
step 685, loss: 2.7575, current_lr: 0.000111
step 686, loss: 2.9260, current_lr: 0.000111
step 687, loss: 2.9288, current_lr: 0.000112
step 688, loss: 2.9779, current_lr: 0.000112
step 689, loss: 2.6358, current_lr: 0.000112
step 690, loss: 2.9222, current_lr: 0.000112
step 691, loss: 2.9047, current_lr: 0.000112
step 692, loss: 2.7267, current_lr: 0.000113
step 693, loss: 2.5786, current_lr: 0.000113
step 694, loss: 2.7615, current_lr: 0.000113
step 695, loss: 2.6858, current_lr: 0.000113
step 696, loss: 3.0431, current_lr: 0.000114
step 697, loss: 3.0153, current_lr: 0.000114
step 698, loss: 2.8903, current_lr: 0.000114
step 699, loss: 2.6492, current_lr: 0.000114
step 700, loss: 3.1075, current_lr: 0.000115
step 701, loss: 2.7147, current_lr: 0.000115
step 702, loss: 2.6482, current_lr: 0.000115
step 703, loss: 2.5908, current_lr: 0.000115
step 704, loss: 2.9374, current_lr: 0.000116
step 705, loss: 2.8471, current_lr: 0.000116
step 706, loss: 2.9366, current_lr: 0.000116
step 707, loss: 3.2562, current_lr: 0.000116
step 708, loss: 3.0517, current_lr: 0.000116
step 709, loss: 3.0566, current_lr: 0.000117
step 710, loss: 3.0745, current_lr: 0.000117
step 711, loss: 3.1606, current_lr: 0.000117
step 712, loss: 3.0014, current_lr: 0.000117
step 713, loss: 2.8167, current_lr: 0.000118
step 714, loss: 2.8496, current_lr: 0.000118
step 715, loss: 3.1068, current_lr: 0.000118
step 716, loss: 2.9923, current_lr: 0.000118
step 717, loss: 2.9646, current_lr: 0.000119
step 718, loss: 2.8855, current_lr: 0.000119
step 719, loss: 2.8123, current_lr: 0.000119
step 720, loss: 2.8313, current_lr: 0.000119
step 721, loss: 2.8552, current_lr: 0.000120
step 722, loss: 2.4631, current_lr: 0.000120
step 723, loss: 2.9557, current_lr: 0.000120
step 724, loss: 2.6270, current_lr: 0.000120
step 725, loss: 2.9095, current_lr: 0.000121
step 726, loss: 2.9971, current_lr: 0.000121
step 727, loss: 2.9470, current_lr: 0.000121
step 728, loss: 3.0514, current_lr: 0.000121
step 729, loss: 2.8658, current_lr: 0.000122
step 730, loss: 2.7344, current_lr: 0.000122
step 731, loss: 3.0375, current_lr: 0.000122
step 732, loss: 2.6503, current_lr: 0.000122
step 733, loss: 2.8283, current_lr: 0.000123
step 734, loss: 2.6462, current_lr: 0.000123
step 735, loss: 3.0337, current_lr: 0.000123
step 736, loss: 2.7551, current_lr: 0.000123
step 737, loss: 2.7740, current_lr: 0.000124
step 738, loss: 2.6796, current_lr: 0.000124
step 739, loss: 2.7172, current_lr: 0.000124
step 740, loss: 2.8014, current_lr: 0.000124
step 741, loss: 2.7013, current_lr: 0.000124
step 742, loss: 3.0175, current_lr: 0.000125
step 743, loss: 2.9251, current_lr: 0.000125
step 744, loss: 2.9307, current_lr: 0.000125
step 745, loss: 2.5978, current_lr: 0.000125
step 746, loss: 2.6299, current_lr: 0.000126
step 747, loss: 2.8593, current_lr: 0.000126
step 748, loss: 3.0014, current_lr: 0.000126
step 749, loss: 3.0640, current_lr: 0.000126
step 750, loss: 2.7475, current_lr: 0.000127
step 751, loss: 3.0005, current_lr: 0.000127
step 752, loss: 2.9430, current_lr: 0.000127
step 753, loss: 2.6869, current_lr: 0.000127
step 754, loss: 2.3564, current_lr: 0.000128
Saved best model with loss: 2.3564 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.3564.pt
step 755, loss: 2.8092, current_lr: 0.000128
step 756, loss: 2.7419, current_lr: 0.000128
step 757, loss: 2.8387, current_lr: 0.000128
step 758, loss: 2.7256, current_lr: 0.000129
step 759, loss: 2.6531, current_lr: 0.000129
step 760, loss: 2.6419, current_lr: 0.000129
step 761, loss: 2.8585, current_lr: 0.000129
step 762, loss: 2.5605, current_lr: 0.000130
step 763, loss: 2.7746, current_lr: 0.000130
step 764, loss: 3.0839, current_lr: 0.000130
step 765, loss: 3.1949, current_lr: 0.000130
step 766, loss: 3.0625, current_lr: 0.000131
step 767, loss: 3.0124, current_lr: 0.000131
step 768, loss: 3.1840, current_lr: 0.000131
step 769, loss: 2.9578, current_lr: 0.000132
step 770, loss: 3.1210, current_lr: 0.000132
step 771, loss: 3.1834, current_lr: 0.000132
step 772, loss: 3.1286, current_lr: 0.000132
step 773, loss: 3.1615, current_lr: 0.000133
step 774, loss: 2.9420, current_lr: 0.000133
step 775, loss: 3.1001, current_lr: 0.000133
step 776, loss: 2.8099, current_lr: 0.000133
step 777, loss: 3.0429, current_lr: 0.000134
step 778, loss: 2.9025, current_lr: 0.000134
step 779, loss: 2.7629, current_lr: 0.000134
step 780, loss: 3.1511, current_lr: 0.000134
step 781, loss: 3.1341, current_lr: 0.000135
step 782, loss: 2.9850, current_lr: 0.000135
step 783, loss: 3.1622, current_lr: 0.000135
step 784, loss: 2.6163, current_lr: 0.000135
step 785, loss: 3.2070, current_lr: 0.000136
step 786, loss: 2.8320, current_lr: 0.000136
step 787, loss: 2.3761, current_lr: 0.000136
step 788, loss: 2.6806, current_lr: 0.000136
step 789, loss: 2.8867, current_lr: 0.000137
step 790, loss: 2.9292, current_lr: 0.000137
step 791, loss: 2.9795, current_lr: 0.000137
step 792, loss: 2.9048, current_lr: 0.000137
step 793, loss: 2.6524, current_lr: 0.000138
step 794, loss: 3.0337, current_lr: 0.000138
step 795, loss: 2.5757, current_lr: 0.000138
step 796, loss: 2.6779, current_lr: 0.000138
step 797, loss: 2.8185, current_lr: 0.000139
step 798, loss: 2.6537, current_lr: 0.000139
step 799, loss: 2.7083, current_lr: 0.000139
step 800, loss: 2.6470, current_lr: 0.000140
step 801, loss: 2.7845, current_lr: 0.000140
step 802, loss: 3.1652, current_lr: 0.000140
step 803, loss: 3.0028, current_lr: 0.000140
step 804, loss: 2.9183, current_lr: 0.000141
step 805, loss: 2.7994, current_lr: 0.000141
step 806, loss: 2.6570, current_lr: 0.000141
step 807, loss: 2.6671, current_lr: 0.000141
step 808, loss: 2.4517, current_lr: 0.000142
step 809, loss: 2.7888, current_lr: 0.000142
step 810, loss: 2.7230, current_lr: 0.000142
step 811, loss: 2.7551, current_lr: 0.000142
step 812, loss: 2.5867, current_lr: 0.000143
step 813, loss: 2.6605, current_lr: 0.000143
step 814, loss: 2.4342, current_lr: 0.000143
step 815, loss: 2.4884, current_lr: 0.000143
step 816, loss: 2.5248, current_lr: 0.000144
step 817, loss: 2.3609, current_lr: 0.000144
step 818, loss: 2.2259, current_lr: 0.000144
Saved best model with loss: 2.2259 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.2259.pt
step 819, loss: 2.7134, current_lr: 0.000145
step 820, loss: 3.0648, current_lr: 0.000145
step 821, loss: 3.0448, current_lr: 0.000145
step 822, loss: 3.0830, current_lr: 0.000145
step 823, loss: 2.5993, current_lr: 0.000146
step 824, loss: 2.6908, current_lr: 0.000146
step 825, loss: 2.9567, current_lr: 0.000146
step 826, loss: 2.8842, current_lr: 0.000146
step 827, loss: 2.6938, current_lr: 0.000147
step 828, loss: 2.9334, current_lr: 0.000147
step 829, loss: 2.9879, current_lr: 0.000147
step 830, loss: 2.7810, current_lr: 0.000147
step 831, loss: 2.7582, current_lr: 0.000148
step 832, loss: 2.8783, current_lr: 0.000148
step 833, loss: 2.6103, current_lr: 0.000148
step 834, loss: 2.7430, current_lr: 0.000149
step 835, loss: 2.7616, current_lr: 0.000149
step 836, loss: 2.3309, current_lr: 0.000149
step 837, loss: 2.7612, current_lr: 0.000149
step 838, loss: 2.5840, current_lr: 0.000150
step 839, loss: 2.7050, current_lr: 0.000150
step 840, loss: 2.4170, current_lr: 0.000150
step 841, loss: 2.5787, current_lr: 0.000150
step 842, loss: 2.4739, current_lr: 0.000151
step 843, loss: 2.7141, current_lr: 0.000151
step 844, loss: 2.7120, current_lr: 0.000151
step 845, loss: 2.9734, current_lr: 0.000152
step 846, loss: 2.7036, current_lr: 0.000152
step 847, loss: 2.6565, current_lr: 0.000152
step 848, loss: 2.7692, current_lr: 0.000152
step 849, loss: 2.5352, current_lr: 0.000153
step 850, loss: 2.4935, current_lr: 0.000153
step 851, loss: 2.6415, current_lr: 0.000153
step 852, loss: 2.6053, current_lr: 0.000153
step 853, loss: 2.6744, current_lr: 0.000154
step 854, loss: 2.3678, current_lr: 0.000154
step 855, loss: 2.6234, current_lr: 0.000154
step 856, loss: 2.5640, current_lr: 0.000155
step 857, loss: 2.4497, current_lr: 0.000155
step 858, loss: 2.2859, current_lr: 0.000155
step 859, loss: 2.4964, current_lr: 0.000155
step 860, loss: 2.4167, current_lr: 0.000156
step 861, loss: 2.7014, current_lr: 0.000156
step 862, loss: 2.6904, current_lr: 0.000156
step 863, loss: 2.5876, current_lr: 0.000156
step 864, loss: 2.3448, current_lr: 0.000157
step 865, loss: 2.7203, current_lr: 0.000157
step 866, loss: 2.3936, current_lr: 0.000157
step 867, loss: 2.3124, current_lr: 0.000158
step 868, loss: 2.2596, current_lr: 0.000158
step 869, loss: 2.5822, current_lr: 0.000158
step 870, loss: 2.4919, current_lr: 0.000158
step 871, loss: 2.5633, current_lr: 0.000159
step 872, loss: 2.8684, current_lr: 0.000159
step 873, loss: 2.6738, current_lr: 0.000159
step 874, loss: 2.6676, current_lr: 0.000160
step 875, loss: 2.6895, current_lr: 0.000160
step 876, loss: 2.7605, current_lr: 0.000160
step 877, loss: 2.6317, current_lr: 0.000160
step 878, loss: 2.4642, current_lr: 0.000161
step 879, loss: 2.4912, current_lr: 0.000161
step 880, loss: 2.7154, current_lr: 0.000161
step 881, loss: 2.6305, current_lr: 0.000161
step 882, loss: 2.5839, current_lr: 0.000162
step 883, loss: 2.4920, current_lr: 0.000162
step 884, loss: 2.4110, current_lr: 0.000162
step 885, loss: 2.4380, current_lr: 0.000163
step 886, loss: 2.4869, current_lr: 0.000163
step 887, loss: 2.1436, current_lr: 0.000163
Saved best model with loss: 2.1436 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.1436.pt
step 888, loss: 2.5577, current_lr: 0.000163
step 889, loss: 2.2688, current_lr: 0.000164
step 890, loss: 2.5329, current_lr: 0.000164
step 891, loss: 2.6092, current_lr: 0.000164
step 892, loss: 2.5233, current_lr: 0.000165
step 893, loss: 2.6693, current_lr: 0.000165
step 894, loss: 2.4869, current_lr: 0.000165
step 895, loss: 2.3835, current_lr: 0.000165
step 896, loss: 2.6359, current_lr: 0.000166
step 897, loss: 2.3367, current_lr: 0.000166
step 898, loss: 2.4906, current_lr: 0.000166
step 899, loss: 2.2586, current_lr: 0.000167
step 900, loss: 2.6247, current_lr: 0.000167
step 901, loss: 2.4599, current_lr: 0.000167
step 902, loss: 2.4647, current_lr: 0.000167
step 903, loss: 2.3207, current_lr: 0.000168
step 904, loss: 2.3932, current_lr: 0.000168
step 905, loss: 2.4864, current_lr: 0.000168
step 906, loss: 2.3930, current_lr: 0.000169
step 907, loss: 2.6697, current_lr: 0.000169
step 908, loss: 2.6235, current_lr: 0.000169
step 909, loss: 2.5984, current_lr: 0.000169
step 910, loss: 2.3240, current_lr: 0.000170
step 911, loss: 2.3679, current_lr: 0.000170
step 912, loss: 2.5464, current_lr: 0.000170
step 913, loss: 2.6591, current_lr: 0.000171
step 914, loss: 2.6918, current_lr: 0.000171
step 915, loss: 2.4303, current_lr: 0.000171
step 916, loss: 2.6280, current_lr: 0.000171
step 917, loss: 2.6089, current_lr: 0.000172
step 918, loss: 2.3976, current_lr: 0.000172
step 919, loss: 2.1054, current_lr: 0.000172
Saved best model with loss: 2.1054 to fine_tuned_gpt2\gpt2_finetuned_best_loss_2.1054.pt
step 920, loss: 2.4901, current_lr: 0.000173
step 921, loss: 2.3994, current_lr: 0.000173
step 922, loss: 2.4631, current_lr: 0.000173
step 923, loss: 2.4153, current_lr: 0.000173
step 924, loss: 2.3312, current_lr: 0.000174
step 925, loss: 2.3314, current_lr: 0.000174
step 926, loss: 2.4842, current_lr: 0.000174
step 927, loss: 2.2510, current_lr: 0.000175
step 928, loss: 2.4569, current_lr: 0.000175
step 929, loss: 2.6897, current_lr: 0.000175
step 930, loss: 2.7537, current_lr: 0.000176
step 931, loss: 2.7057, current_lr: 0.000176
step 932, loss: 2.6283, current_lr: 0.000176
step 933, loss: 2.7517, current_lr: 0.000176
step 934, loss: 2.5704, current_lr: 0.000177
step 935, loss: 2.7615, current_lr: 0.000177
step 936, loss: 2.8167, current_lr: 0.000177
step 937, loss: 2.7557, current_lr: 0.000178
step 938, loss: 2.8168, current_lr: 0.000178
step 939, loss: 2.6021, current_lr: 0.000178
step 940, loss: 2.7195, current_lr: 0.000178
step 941, loss: 2.5314, current_lr: 0.000179
step 942, loss: 2.7971, current_lr: 0.000179
step 943, loss: 2.6191, current_lr: 0.000179
step 944, loss: 2.4953, current_lr: 0.000180
step 945, loss: 2.9171, current_lr: 0.000180
step 946, loss: 2.8195, current_lr: 0.000180
step 947, loss: 2.7447, current_lr: 0.000181
step 948, loss: 2.8956, current_lr: 0.000181
step 949, loss: 2.3650, current_lr: 0.000181
step 950, loss: 2.8871, current_lr: 0.000181
step 951, loss: 2.5064, current_lr: 0.000182
step 952, loss: 2.1387, current_lr: 0.000182
step 953, loss: 2.4610, current_lr: 0.000182
step 954, loss: 2.7078, current_lr: 0.000183
step 955, loss: 2.7222, current_lr: 0.000183
step 956, loss: 2.6963, current_lr: 0.000183
step 957, loss: 2.6103, current_lr: 0.000183
step 958, loss: 2.3800, current_lr: 0.000184
step 959, loss: 2.7760, current_lr: 0.000184
step 960, loss: 2.3898, current_lr: 0.000184
step 961, loss: 2.4262, current_lr: 0.000185
step 962, loss: 2.5878, current_lr: 0.000185
step 963, loss: 2.4542, current_lr: 0.000185
step 964, loss: 2.5080, current_lr: 0.000186
step 965, loss: 2.4432, current_lr: 0.000186
step 966, loss: 2.5649, current_lr: 0.000186
step 967, loss: 2.8868, current_lr: 0.000186
step 968, loss: 2.7350, current_lr: 0.000187
step 969, loss: 2.6584, current_lr: 0.000187
step 970, loss: 2.5258, current_lr: 0.000187
step 971, loss: 2.3653, current_lr: 0.000188
step 972, loss: 2.3705, current_lr: 0.000188
step 973, loss: 2.2134, current_lr: 0.000188
step 974, loss: 2.4936, current_lr: 0.000189
step 975, loss: 2.4045, current_lr: 0.000189
step 976, loss: 2.4485, current_lr: 0.000189
step 977, loss: 2.2901, current_lr: 0.000189
step 978, loss: 2.3964, current_lr: 0.000190
step 979, loss: 2.1512, current_lr: 0.000190
step 980, loss: 2.2107, current_lr: 0.000190
step 981, loss: 2.2638, current_lr: 0.000191
step 982, loss: 2.1466, current_lr: 0.000191
step 983, loss: 1.9960, current_lr: 0.000191
Saved best model with loss: 1.9960 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.9960.pt
step 984, loss: 2.4461, current_lr: 0.000192
step 985, loss: 2.7536, current_lr: 0.000192
step 986, loss: 2.6715, current_lr: 0.000192
step 987, loss: 2.7886, current_lr: 0.000192
step 988, loss: 2.3668, current_lr: 0.000193
step 989, loss: 2.4618, current_lr: 0.000193
step 990, loss: 2.7037, current_lr: 0.000193
step 991, loss: 2.6096, current_lr: 0.000194
step 992, loss: 2.3946, current_lr: 0.000194
step 993, loss: 2.6207, current_lr: 0.000194
step 994, loss: 2.6905, current_lr: 0.000195
step 995, loss: 2.4748, current_lr: 0.000195
step 996, loss: 2.4740, current_lr: 0.000195
step 997, loss: 2.5933, current_lr: 0.000195
step 998, loss: 2.3369, current_lr: 0.000196
step 999, loss: 2.4592, current_lr: 0.000196
step 1000, loss: 2.4588, current_lr: 0.000196
step 1001, loss: 2.0413, current_lr: 0.000197
step 1002, loss: 2.4309, current_lr: 0.000197
step 1003, loss: 2.2452, current_lr: 0.000197
step 1004, loss: 2.3962, current_lr: 0.000198
step 1005, loss: 2.1550, current_lr: 0.000198
step 1006, loss: 2.3012, current_lr: 0.000198
step 1007, loss: 2.1202, current_lr: 0.000199
step 1008, loss: 2.4119, current_lr: 0.000199
step 1009, loss: 2.4710, current_lr: 0.000199
step 1010, loss: 2.7205, current_lr: 0.000199
step 1011, loss: 2.3789, current_lr: 0.000200
step 1012, loss: 2.3889, current_lr: 0.000200
step 1013, loss: 2.5134, current_lr: 0.000200
step 1014, loss: 2.2937, current_lr: 0.000201
step 1015, loss: 2.2437, current_lr: 0.000201
step 1016, loss: 2.3846, current_lr: 0.000201
step 1017, loss: 2.3923, current_lr: 0.000202
step 1018, loss: 2.4633, current_lr: 0.000202
step 1019, loss: 2.1611, current_lr: 0.000202
step 1020, loss: 2.3593, current_lr: 0.000203
step 1021, loss: 2.2845, current_lr: 0.000203
step 1022, loss: 2.2113, current_lr: 0.000203
step 1023, loss: 2.0606, current_lr: 0.000203
step 1024, loss: 2.2485, current_lr: 0.000204
step 1025, loss: 2.1658, current_lr: 0.000204
step 1026, loss: 2.4319, current_lr: 0.000204
step 1027, loss: 2.4087, current_lr: 0.000205
step 1028, loss: 2.3149, current_lr: 0.000205
step 1029, loss: 2.1205, current_lr: 0.000205
step 1030, loss: 2.4749, current_lr: 0.000206
step 1031, loss: 2.1866, current_lr: 0.000206
step 1032, loss: 2.1368, current_lr: 0.000206
step 1033, loss: 2.0810, current_lr: 0.000207
step 1034, loss: 2.4024, current_lr: 0.000207
step 1035, loss: 2.2670, current_lr: 0.000207
step 1036, loss: 2.2953, current_lr: 0.000208
step 1037, loss: 2.6206, current_lr: 0.000208
step 1038, loss: 2.4044, current_lr: 0.000208
step 1039, loss: 2.3689, current_lr: 0.000208
step 1040, loss: 2.3592, current_lr: 0.000209
step 1041, loss: 2.4482, current_lr: 0.000209
step 1042, loss: 2.3398, current_lr: 0.000209
step 1043, loss: 2.2670, current_lr: 0.000210
step 1044, loss: 2.3486, current_lr: 0.000210
step 1045, loss: 2.4990, current_lr: 0.000210
step 1046, loss: 2.3739, current_lr: 0.000211
step 1047, loss: 2.3827, current_lr: 0.000211
step 1048, loss: 2.2605, current_lr: 0.000211
step 1049, loss: 2.2308, current_lr: 0.000212
step 1050, loss: 2.2433, current_lr: 0.000212
step 1051, loss: 2.2623, current_lr: 0.000212
step 1052, loss: 1.9413, current_lr: 0.000213
Saved best model with loss: 1.9413 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.9413.pt
step 1053, loss: 2.3452, current_lr: 0.000213
step 1054, loss: 2.0855, current_lr: 0.000213
step 1055, loss: 2.2834, current_lr: 0.000213
step 1056, loss: 2.3754, current_lr: 0.000214
step 1057, loss: 2.3308, current_lr: 0.000214
step 1058, loss: 2.4353, current_lr: 0.000214
step 1059, loss: 2.2794, current_lr: 0.000215
step 1060, loss: 2.1882, current_lr: 0.000215
step 1061, loss: 2.5003, current_lr: 0.000215
step 1062, loss: 2.1273, current_lr: 0.000216
step 1063, loss: 2.3334, current_lr: 0.000216
step 1064, loss: 2.1032, current_lr: 0.000216
step 1065, loss: 2.4485, current_lr: 0.000217
step 1066, loss: 2.2895, current_lr: 0.000217
step 1067, loss: 2.2015, current_lr: 0.000217
step 1068, loss: 2.0737, current_lr: 0.000218
step 1069, loss: 2.1000, current_lr: 0.000218
step 1070, loss: 2.1967, current_lr: 0.000218
step 1071, loss: 2.1003, current_lr: 0.000219
step 1072, loss: 2.3426, current_lr: 0.000219
step 1073, loss: 2.3039, current_lr: 0.000219
step 1074, loss: 2.2559, current_lr: 0.000219
step 1075, loss: 2.0951, current_lr: 0.000220
step 1076, loss: 2.1405, current_lr: 0.000220
step 1077, loss: 2.3553, current_lr: 0.000220
step 1078, loss: 2.4353, current_lr: 0.000221
step 1079, loss: 2.4902, current_lr: 0.000221
step 1080, loss: 2.2225, current_lr: 0.000221
step 1081, loss: 2.4045, current_lr: 0.000222
step 1082, loss: 2.3644, current_lr: 0.000222
step 1083, loss: 2.2000, current_lr: 0.000222
step 1084, loss: 1.9152, current_lr: 0.000223
Saved best model with loss: 1.9152 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.9152.pt
step 1085, loss: 2.2194, current_lr: 0.000223
step 1086, loss: 2.1486, current_lr: 0.000223
step 1087, loss: 2.2297, current_lr: 0.000224
step 1088, loss: 2.1798, current_lr: 0.000224
step 1089, loss: 2.0933, current_lr: 0.000224
step 1090, loss: 2.1158, current_lr: 0.000225
step 1091, loss: 2.2350, current_lr: 0.000225
step 1092, loss: 2.0508, current_lr: 0.000225
step 1093, loss: 2.1785, current_lr: 0.000226
step 1094, loss: 2.4286, current_lr: 0.000226
step 1095, loss: 2.4945, current_lr: 0.000226
step 1096, loss: 2.4124, current_lr: 0.000226
step 1097, loss: 2.3428, current_lr: 0.000227
step 1098, loss: 2.5116, current_lr: 0.000227
step 1099, loss: 2.3561, current_lr: 0.000227
step 1100, loss: 2.4800, current_lr: 0.000228
step 1101, loss: 2.5407, current_lr: 0.000228
step 1102, loss: 2.5615, current_lr: 0.000228
step 1103, loss: 2.5877, current_lr: 0.000229
step 1104, loss: 2.4491, current_lr: 0.000229
step 1105, loss: 2.6004, current_lr: 0.000229
step 1106, loss: 2.3970, current_lr: 0.000230
step 1107, loss: 2.6486, current_lr: 0.000230
step 1108, loss: 2.4326, current_lr: 0.000230
step 1109, loss: 2.3244, current_lr: 0.000231
step 1110, loss: 2.7489, current_lr: 0.000231
step 1111, loss: 2.6771, current_lr: 0.000231
step 1112, loss: 2.5764, current_lr: 0.000232
step 1113, loss: 2.6619, current_lr: 0.000232
step 1114, loss: 2.1505, current_lr: 0.000232
step 1115, loss: 2.6946, current_lr: 0.000233
step 1116, loss: 2.3936, current_lr: 0.000233
step 1117, loss: 1.9632, current_lr: 0.000233
step 1118, loss: 2.2824, current_lr: 0.000234
step 1119, loss: 2.4331, current_lr: 0.000234
step 1120, loss: 2.4540, current_lr: 0.000234
step 1121, loss: 2.5204, current_lr: 0.000235
step 1122, loss: 2.4693, current_lr: 0.000235
step 1123, loss: 2.1636, current_lr: 0.000235
step 1124, loss: 2.4999, current_lr: 0.000236
step 1125, loss: 2.1802, current_lr: 0.000236
step 1126, loss: 2.1927, current_lr: 0.000236
step 1127, loss: 2.3325, current_lr: 0.000237
step 1128, loss: 2.2061, current_lr: 0.000237
step 1129, loss: 2.2729, current_lr: 0.000237
step 1130, loss: 2.2033, current_lr: 0.000237
step 1131, loss: 2.2946, current_lr: 0.000238
step 1132, loss: 2.5648, current_lr: 0.000238
step 1133, loss: 2.4440, current_lr: 0.000238
step 1134, loss: 2.3691, current_lr: 0.000239
step 1135, loss: 2.3091, current_lr: 0.000239
step 1136, loss: 2.1745, current_lr: 0.000239
step 1137, loss: 2.1860, current_lr: 0.000240
step 1138, loss: 2.0420, current_lr: 0.000240
step 1139, loss: 2.2482, current_lr: 0.000240
step 1140, loss: 2.1387, current_lr: 0.000241
step 1141, loss: 2.1799, current_lr: 0.000241
step 1142, loss: 2.0278, current_lr: 0.000241
step 1143, loss: 2.1235, current_lr: 0.000242
step 1144, loss: 1.9555, current_lr: 0.000242
step 1145, loss: 1.9934, current_lr: 0.000242
step 1146, loss: 2.0505, current_lr: 0.000243
step 1147, loss: 1.9838, current_lr: 0.000243
step 1148, loss: 1.7933, current_lr: 0.000243
Saved best model with loss: 1.7933 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.7933.pt
step 1149, loss: 2.1915, current_lr: 0.000244
step 1150, loss: 2.4791, current_lr: 0.000244
step 1151, loss: 2.4082, current_lr: 0.000244
step 1152, loss: 2.5643, current_lr: 0.000245
step 1153, loss: 2.1525, current_lr: 0.000245
step 1154, loss: 2.1498, current_lr: 0.000245
step 1155, loss: 2.4811, current_lr: 0.000246
step 1156, loss: 2.3994, current_lr: 0.000246
step 1157, loss: 2.2066, current_lr: 0.000246
step 1158, loss: 2.3343, current_lr: 0.000247
step 1159, loss: 2.4738, current_lr: 0.000247
step 1160, loss: 2.2682, current_lr: 0.000247
step 1161, loss: 2.2552, current_lr: 0.000248
step 1162, loss: 2.3372, current_lr: 0.000248
step 1163, loss: 2.0754, current_lr: 0.000248
step 1164, loss: 2.2086, current_lr: 0.000249
step 1165, loss: 2.2474, current_lr: 0.000249
step 1166, loss: 1.8943, current_lr: 0.000249
step 1167, loss: 2.1997, current_lr: 0.000250
step 1168, loss: 1.9774, current_lr: 0.000250
step 1169, loss: 2.0570, current_lr: 0.000250
step 1170, loss: 1.8825, current_lr: 0.000251
step 1171, loss: 2.0101, current_lr: 0.000251
step 1172, loss: 1.9042, current_lr: 0.000251
step 1173, loss: 2.0909, current_lr: 0.000252
step 1174, loss: 2.0870, current_lr: 0.000252
step 1175, loss: 2.3071, current_lr: 0.000252
step 1176, loss: 2.1051, current_lr: 0.000253
step 1177, loss: 2.1624, current_lr: 0.000253
step 1178, loss: 2.2090, current_lr: 0.000253
step 1179, loss: 2.0103, current_lr: 0.000254
step 1180, loss: 1.9775, current_lr: 0.000254
step 1181, loss: 2.1035, current_lr: 0.000254
step 1182, loss: 2.1515, current_lr: 0.000255
step 1183, loss: 2.1249, current_lr: 0.000255
step 1184, loss: 1.9150, current_lr: 0.000255
step 1185, loss: 2.1154, current_lr: 0.000256
step 1186, loss: 2.0976, current_lr: 0.000256
step 1187, loss: 2.0583, current_lr: 0.000256
step 1188, loss: 1.8931, current_lr: 0.000257
step 1189, loss: 2.0375, current_lr: 0.000257
step 1190, loss: 2.0456, current_lr: 0.000257
step 1191, loss: 2.2291, current_lr: 0.000258
step 1192, loss: 2.2254, current_lr: 0.000258
step 1193, loss: 2.1283, current_lr: 0.000258
step 1194, loss: 1.8967, current_lr: 0.000259
step 1195, loss: 2.2076, current_lr: 0.000259
step 1196, loss: 2.0276, current_lr: 0.000259
step 1197, loss: 1.9831, current_lr: 0.000260
step 1198, loss: 1.9423, current_lr: 0.000260
step 1199, loss: 2.1478, current_lr: 0.000260
step 1200, loss: 2.0984, current_lr: 0.000261
step 1201, loss: 2.1377, current_lr: 0.000261
step 1202, loss: 2.3257, current_lr: 0.000261
step 1203, loss: 2.1576, current_lr: 0.000262
step 1204, loss: 2.1670, current_lr: 0.000262
step 1205, loss: 2.1414, current_lr: 0.000262
step 1206, loss: 2.2124, current_lr: 0.000263
step 1207, loss: 2.1050, current_lr: 0.000263
step 1208, loss: 2.0024, current_lr: 0.000263
step 1209, loss: 1.9789, current_lr: 0.000264
step 1210, loss: 2.1483, current_lr: 0.000264
step 1211, loss: 2.0392, current_lr: 0.000264
step 1212, loss: 2.0641, current_lr: 0.000265
step 1213, loss: 2.0053, current_lr: 0.000265
step 1214, loss: 1.9681, current_lr: 0.000265
step 1215, loss: 1.9862, current_lr: 0.000266
step 1216, loss: 1.9527, current_lr: 0.000266
step 1217, loss: 1.7481, current_lr: 0.000266
Saved best model with loss: 1.7481 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.7481.pt
step 1218, loss: 2.0901, current_lr: 0.000267
step 1219, loss: 1.8450, current_lr: 0.000267
step 1220, loss: 2.0657, current_lr: 0.000267
step 1221, loss: 2.0302, current_lr: 0.000268
step 1222, loss: 1.9843, current_lr: 0.000268
step 1223, loss: 2.0855, current_lr: 0.000268
step 1224, loss: 1.9874, current_lr: 0.000269
step 1225, loss: 1.9177, current_lr: 0.000269
step 1226, loss: 2.0622, current_lr: 0.000269
step 1227, loss: 1.8645, current_lr: 0.000270
step 1228, loss: 1.9827, current_lr: 0.000270
step 1229, loss: 1.8459, current_lr: 0.000270
step 1230, loss: 2.1154, current_lr: 0.000271
step 1231, loss: 1.9295, current_lr: 0.000271
step 1232, loss: 1.9097, current_lr: 0.000271
step 1233, loss: 1.8692, current_lr: 0.000272
step 1234, loss: 1.9045, current_lr: 0.000272
step 1235, loss: 1.8608, current_lr: 0.000272
step 1236, loss: 1.7426, current_lr: 0.000273
Saved best model with loss: 1.7426 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.7426.pt
step 1237, loss: 1.9233, current_lr: 0.000273
step 1238, loss: 1.9606, current_lr: 0.000273
step 1239, loss: 1.9690, current_lr: 0.000274
step 1240, loss: 1.9123, current_lr: 0.000274
step 1241, loss: 1.8782, current_lr: 0.000274
step 1242, loss: 1.9919, current_lr: 0.000275
step 1243, loss: 2.1051, current_lr: 0.000275
step 1244, loss: 2.2200, current_lr: 0.000275
step 1245, loss: 2.0358, current_lr: 0.000276
step 1246, loss: 2.1725, current_lr: 0.000276
step 1247, loss: 2.0943, current_lr: 0.000276
step 1248, loss: 1.8995, current_lr: 0.000277
step 1249, loss: 1.6767, current_lr: 0.000277
Saved best model with loss: 1.6767 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.6767.pt
step 1250, loss: 1.9649, current_lr: 0.000278
step 1251, loss: 1.9023, current_lr: 0.000278
step 1252, loss: 1.9368, current_lr: 0.000278
step 1253, loss: 1.8687, current_lr: 0.000279
step 1254, loss: 1.8141, current_lr: 0.000279
step 1255, loss: 1.8121, current_lr: 0.000279
step 1256, loss: 1.9697, current_lr: 0.000280
step 1257, loss: 1.7860, current_lr: 0.000280
step 1258, loss: 1.9141, current_lr: 0.000280
step 1259, loss: 2.1426, current_lr: 0.000281
step 1260, loss: 2.2736, current_lr: 0.000281
step 1261, loss: 2.2007, current_lr: 0.000281
step 1262, loss: 2.0727, current_lr: 0.000282
step 1263, loss: 2.2892, current_lr: 0.000282
step 1264, loss: 2.0860, current_lr: 0.000282
step 1265, loss: 2.1877, current_lr: 0.000283
step 1266, loss: 2.1869, current_lr: 0.000283
step 1267, loss: 2.1590, current_lr: 0.000283
step 1268, loss: 2.1975, current_lr: 0.000284
step 1269, loss: 2.0752, current_lr: 0.000284
step 1270, loss: 2.0964, current_lr: 0.000284
step 1271, loss: 2.0688, current_lr: 0.000285
step 1272, loss: 2.3202, current_lr: 0.000285
step 1273, loss: 2.1391, current_lr: 0.000285
step 1274, loss: 2.0288, current_lr: 0.000286
step 1275, loss: 2.4502, current_lr: 0.000286
step 1276, loss: 2.3490, current_lr: 0.000286
step 1277, loss: 2.2345, current_lr: 0.000287
step 1278, loss: 2.3979, current_lr: 0.000287
step 1279, loss: 1.8773, current_lr: 0.000287
step 1280, loss: 2.3715, current_lr: 0.000288
step 1281, loss: 2.1253, current_lr: 0.000288
step 1282, loss: 1.7465, current_lr: 0.000288
step 1283, loss: 2.0024, current_lr: 0.000289
step 1284, loss: 2.1716, current_lr: 0.000289
step 1285, loss: 2.1422, current_lr: 0.000289
step 1286, loss: 2.1248, current_lr: 0.000290
step 1287, loss: 2.0576, current_lr: 0.000290
step 1288, loss: 1.9083, current_lr: 0.000291
step 1289, loss: 2.1469, current_lr: 0.000291
step 1290, loss: 1.8663, current_lr: 0.000291
step 1291, loss: 1.8432, current_lr: 0.000292
step 1292, loss: 2.0180, current_lr: 0.000292
step 1293, loss: 2.0046, current_lr: 0.000292
step 1294, loss: 2.0216, current_lr: 0.000293
step 1295, loss: 1.9160, current_lr: 0.000293
step 1296, loss: 1.9577, current_lr: 0.000293
step 1297, loss: 2.1486, current_lr: 0.000294
step 1298, loss: 2.0843, current_lr: 0.000294
step 1299, loss: 1.9756, current_lr: 0.000294
step 1300, loss: 1.9478, current_lr: 0.000295
step 1301, loss: 1.8372, current_lr: 0.000295
step 1302, loss: 1.8048, current_lr: 0.000295
step 1303, loss: 1.7007, current_lr: 0.000296
step 1304, loss: 1.9256, current_lr: 0.000296
step 1305, loss: 1.8450, current_lr: 0.000296
step 1306, loss: 1.8538, current_lr: 0.000297
step 1307, loss: 1.6764, current_lr: 0.000297
Saved best model with loss: 1.6764 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.6764.pt
step 1308, loss: 1.7892, current_lr: 0.000297
step 1309, loss: 1.7143, current_lr: 0.000298
step 1310, loss: 1.7011, current_lr: 0.000298
step 1311, loss: 1.7500, current_lr: 0.000298
step 1312, loss: 1.6625, current_lr: 0.000299
Saved best model with loss: 1.6625 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.6625.pt
step 1313, loss: 1.5097, current_lr: 0.000299
Saved best model with loss: 1.5097 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.5097.pt
step 1314, loss: 1.8048, current_lr: 0.000299
step 1315, loss: 2.0838, current_lr: 0.000300
step 1316, loss: 2.0515, current_lr: 0.000300
step 1317, loss: 2.1051, current_lr: 0.000301
step 1318, loss: 1.8128, current_lr: 0.000301
step 1319, loss: 1.8298, current_lr: 0.000301
step 1320, loss: 2.0470, current_lr: 0.000302
step 1321, loss: 1.9647, current_lr: 0.000302
step 1322, loss: 1.8552, current_lr: 0.000302
step 1323, loss: 1.9846, current_lr: 0.000303
step 1324, loss: 2.1113, current_lr: 0.000303
step 1325, loss: 1.8750, current_lr: 0.000303
step 1326, loss: 1.8457, current_lr: 0.000304
step 1327, loss: 1.9506, current_lr: 0.000304
step 1328, loss: 1.7663, current_lr: 0.000304
step 1329, loss: 1.8546, current_lr: 0.000305
step 1330, loss: 1.8835, current_lr: 0.000305
step 1331, loss: 1.5473, current_lr: 0.000305
step 1332, loss: 1.7994, current_lr: 0.000306
step 1333, loss: 1.6575, current_lr: 0.000306
step 1334, loss: 1.7563, current_lr: 0.000306
step 1335, loss: 1.6492, current_lr: 0.000307
step 1336, loss: 1.6746, current_lr: 0.000307
step 1337, loss: 1.5221, current_lr: 0.000307
step 1338, loss: 1.6991, current_lr: 0.000308
step 1339, loss: 1.7364, current_lr: 0.000308
step 1340, loss: 1.9562, current_lr: 0.000308
step 1341, loss: 1.6643, current_lr: 0.000309
step 1342, loss: 1.7174, current_lr: 0.000309
step 1343, loss: 1.7859, current_lr: 0.000310
step 1344, loss: 1.6158, current_lr: 0.000310
step 1345, loss: 1.6890, current_lr: 0.000310
step 1346, loss: 1.7517, current_lr: 0.000311
step 1347, loss: 1.7770, current_lr: 0.000311
step 1348, loss: 1.7560, current_lr: 0.000311
step 1349, loss: 1.5798, current_lr: 0.000312
step 1350, loss: 1.7753, current_lr: 0.000312
step 1351, loss: 1.7817, current_lr: 0.000312
step 1352, loss: 1.6960, current_lr: 0.000313
step 1353, loss: 1.5912, current_lr: 0.000313
step 1354, loss: 1.6342, current_lr: 0.000313
step 1355, loss: 1.6008, current_lr: 0.000314
step 1356, loss: 1.7476, current_lr: 0.000314
step 1357, loss: 1.7757, current_lr: 0.000314
step 1358, loss: 1.7559, current_lr: 0.000315
step 1359, loss: 1.6527, current_lr: 0.000315
step 1360, loss: 1.8280, current_lr: 0.000315
step 1361, loss: 1.6512, current_lr: 0.000316
step 1362, loss: 1.6152, current_lr: 0.000316
step 1363, loss: 1.6348, current_lr: 0.000316
step 1364, loss: 1.7888, current_lr: 0.000317
step 1365, loss: 1.8052, current_lr: 0.000317
step 1366, loss: 1.8260, current_lr: 0.000318
step 1367, loss: 2.0710, current_lr: 0.000318
step 1368, loss: 1.8135, current_lr: 0.000318
step 1369, loss: 1.7860, current_lr: 0.000319
step 1370, loss: 1.7320, current_lr: 0.000319
step 1371, loss: 1.7146, current_lr: 0.000319
step 1372, loss: 1.6794, current_lr: 0.000320
step 1373, loss: 1.5915, current_lr: 0.000320
step 1374, loss: 1.5723, current_lr: 0.000320
step 1375, loss: 1.7517, current_lr: 0.000321
step 1376, loss: 1.6508, current_lr: 0.000321
step 1377, loss: 1.6976, current_lr: 0.000321
step 1378, loss: 1.6259, current_lr: 0.000322
step 1379, loss: 1.6564, current_lr: 0.000322
step 1380, loss: 1.6474, current_lr: 0.000322
step 1381, loss: 1.6439, current_lr: 0.000323
step 1382, loss: 1.5248, current_lr: 0.000323
step 1383, loss: 1.7577, current_lr: 0.000323
step 1384, loss: 1.4961, current_lr: 0.000324
Saved best model with loss: 1.4961 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4961.pt
step 1385, loss: 1.6892, current_lr: 0.000324
step 1386, loss: 1.6953, current_lr: 0.000324
step 1387, loss: 1.6547, current_lr: 0.000325
step 1388, loss: 1.7241, current_lr: 0.000325
step 1389, loss: 1.5781, current_lr: 0.000326
step 1390, loss: 1.5396, current_lr: 0.000326
step 1391, loss: 1.5951, current_lr: 0.000326
step 1392, loss: 1.4569, current_lr: 0.000327
Saved best model with loss: 1.4569 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4569.pt
step 1393, loss: 1.5711, current_lr: 0.000327
step 1394, loss: 1.4385, current_lr: 0.000327
Saved best model with loss: 1.4385 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4385.pt
step 1395, loss: 1.7442, current_lr: 0.000328
step 1396, loss: 1.5744, current_lr: 0.000328
step 1397, loss: 1.6108, current_lr: 0.000328
step 1398, loss: 1.5800, current_lr: 0.000329
step 1399, loss: 1.5161, current_lr: 0.000329
step 1400, loss: 1.6232, current_lr: 0.000329
step 1401, loss: 1.5386, current_lr: 0.000330
step 1402, loss: 1.6132, current_lr: 0.000330
step 1403, loss: 1.5957, current_lr: 0.000330
step 1404, loss: 1.5121, current_lr: 0.000331
step 1405, loss: 1.5258, current_lr: 0.000331
step 1406, loss: 1.5898, current_lr: 0.000331
step 1407, loss: 1.6959, current_lr: 0.000332
step 1408, loss: 1.6617, current_lr: 0.000332
step 1409, loss: 1.7265, current_lr: 0.000333
step 1410, loss: 1.5814, current_lr: 0.000333
step 1411, loss: 1.6546, current_lr: 0.000333
step 1412, loss: 1.7253, current_lr: 0.000334
step 1413, loss: 1.6163, current_lr: 0.000334
step 1414, loss: 1.4280, current_lr: 0.000334
Saved best model with loss: 1.4280 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4280.pt
step 1415, loss: 1.6126, current_lr: 0.000335
step 1416, loss: 1.6432, current_lr: 0.000335
step 1417, loss: 1.5834, current_lr: 0.000335
step 1418, loss: 1.5587, current_lr: 0.000336
step 1419, loss: 1.4429, current_lr: 0.000336
step 1420, loss: 1.4259, current_lr: 0.000336
Saved best model with loss: 1.4259 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4259.pt
step 1421, loss: 1.5161, current_lr: 0.000337
step 1422, loss: 1.4629, current_lr: 0.000337
step 1423, loss: 1.5134, current_lr: 0.000337
step 1424, loss: 1.7836, current_lr: 0.000338
step 1425, loss: 1.8358, current_lr: 0.000338
step 1426, loss: 1.7322, current_lr: 0.000339
step 1427, loss: 1.6518, current_lr: 0.000339
step 1428, loss: 1.8307, current_lr: 0.000339
step 1429, loss: 1.7235, current_lr: 0.000340
step 1430, loss: 1.8308, current_lr: 0.000340
step 1431, loss: 1.8071, current_lr: 0.000340
step 1432, loss: 1.7461, current_lr: 0.000341
step 1433, loss: 1.6976, current_lr: 0.000341
step 1434, loss: 1.6649, current_lr: 0.000341
step 1435, loss: 1.6076, current_lr: 0.000342
step 1436, loss: 1.5643, current_lr: 0.000342
step 1437, loss: 1.7906, current_lr: 0.000342
step 1438, loss: 1.6457, current_lr: 0.000343
step 1439, loss: 1.7594, current_lr: 0.000343
step 1440, loss: 2.0495, current_lr: 0.000343
step 1441, loss: 1.8831, current_lr: 0.000344
step 1442, loss: 1.8644, current_lr: 0.000344
step 1443, loss: 2.0333, current_lr: 0.000344
step 1444, loss: 1.6195, current_lr: 0.000345
step 1445, loss: 2.0143, current_lr: 0.000345
step 1446, loss: 1.8070, current_lr: 0.000346
step 1447, loss: 1.4819, current_lr: 0.000346
step 1448, loss: 1.7405, current_lr: 0.000346
step 1449, loss: 1.8570, current_lr: 0.000347
step 1450, loss: 1.8359, current_lr: 0.000347
step 1451, loss: 1.7336, current_lr: 0.000347
step 1452, loss: 1.6782, current_lr: 0.000348
step 1453, loss: 1.5338, current_lr: 0.000348
step 1454, loss: 1.7405, current_lr: 0.000348
step 1455, loss: 1.5160, current_lr: 0.000349
step 1456, loss: 1.4949, current_lr: 0.000349
step 1457, loss: 1.7124, current_lr: 0.000349
step 1458, loss: 1.6373, current_lr: 0.000350
step 1459, loss: 1.6744, current_lr: 0.000350
step 1460, loss: 1.6367, current_lr: 0.000350
step 1461, loss: 1.6551, current_lr: 0.000351
step 1462, loss: 1.7218, current_lr: 0.000351
step 1463, loss: 1.6957, current_lr: 0.000352
step 1464, loss: 1.6952, current_lr: 0.000352
step 1465, loss: 1.6137, current_lr: 0.000352
step 1466, loss: 1.6148, current_lr: 0.000353
step 1467, loss: 1.5715, current_lr: 0.000353
step 1468, loss: 1.4289, current_lr: 0.000353
step 1469, loss: 1.5300, current_lr: 0.000354
step 1470, loss: 1.4078, current_lr: 0.000354
Saved best model with loss: 1.4078 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.4078.pt
step 1471, loss: 1.4599, current_lr: 0.000354
step 1472, loss: 1.3592, current_lr: 0.000355
Saved best model with loss: 1.3592 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.3592.pt
step 1473, loss: 1.5276, current_lr: 0.000355
step 1474, loss: 1.3716, current_lr: 0.000355
step 1475, loss: 1.3735, current_lr: 0.000356
step 1476, loss: 1.4327, current_lr: 0.000356
step 1477, loss: 1.4502, current_lr: 0.000356
step 1478, loss: 1.3241, current_lr: 0.000357
Saved best model with loss: 1.3241 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.3241.pt
step 1479, loss: 1.5470, current_lr: 0.000357
step 1480, loss: 1.7650, current_lr: 0.000357
step 1481, loss: 1.6639, current_lr: 0.000358
step 1482, loss: 1.6697, current_lr: 0.000358
step 1483, loss: 1.5232, current_lr: 0.000359
step 1484, loss: 1.4899, current_lr: 0.000359
step 1485, loss: 1.6660, current_lr: 0.000359
step 1486, loss: 1.6613, current_lr: 0.000360
step 1487, loss: 1.5342, current_lr: 0.000360
step 1488, loss: 1.6097, current_lr: 0.000360
step 1489, loss: 1.6904, current_lr: 0.000361
step 1490, loss: 1.4205, current_lr: 0.000361
step 1491, loss: 1.5161, current_lr: 0.000361
step 1492, loss: 1.6744, current_lr: 0.000362
step 1493, loss: 1.4870, current_lr: 0.000362
step 1494, loss: 1.5506, current_lr: 0.000362
step 1495, loss: 1.5657, current_lr: 0.000363
step 1496, loss: 1.2735, current_lr: 0.000363
Saved best model with loss: 1.2735 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.2735.pt
step 1497, loss: 1.5139, current_lr: 0.000363
step 1498, loss: 1.3573, current_lr: 0.000364
step 1499, loss: 1.3827, current_lr: 0.000364
step 1500, loss: 1.2627, current_lr: 0.000365
Saved best model with loss: 1.2627 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.2627.pt
step 1501, loss: 1.3358, current_lr: 0.000365
step 1502, loss: 1.2691, current_lr: 0.000365
step 1503, loss: 1.4014, current_lr: 0.000366
step 1504, loss: 1.4327, current_lr: 0.000366
step 1505, loss: 1.6070, current_lr: 0.000366
step 1506, loss: 1.4257, current_lr: 0.000367
step 1507, loss: 1.4092, current_lr: 0.000367
step 1508, loss: 1.4689, current_lr: 0.000367
step 1509, loss: 1.3139, current_lr: 0.000368
step 1510, loss: 1.3242, current_lr: 0.000368
step 1511, loss: 1.3762, current_lr: 0.000368
step 1512, loss: 1.3982, current_lr: 0.000369
step 1513, loss: 1.3490, current_lr: 0.000369
step 1514, loss: 1.1722, current_lr: 0.000369
Saved best model with loss: 1.1722 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.1722.pt
step 1515, loss: 1.3892, current_lr: 0.000370
step 1516, loss: 1.3461, current_lr: 0.000370
step 1517, loss: 1.3411, current_lr: 0.000371
step 1518, loss: 1.2208, current_lr: 0.000371
step 1519, loss: 1.2962, current_lr: 0.000371
step 1520, loss: 1.2716, current_lr: 0.000372
step 1521, loss: 1.3068, current_lr: 0.000372
step 1522, loss: 1.2459, current_lr: 0.000372
step 1523, loss: 1.3577, current_lr: 0.000373
step 1524, loss: 1.1878, current_lr: 0.000373
step 1525, loss: 1.3541, current_lr: 0.000373
step 1526, loss: 1.2048, current_lr: 0.000374
step 1527, loss: 1.2470, current_lr: 0.000374
step 1528, loss: 1.2732, current_lr: 0.000374
step 1529, loss: 1.3277, current_lr: 0.000375
step 1530, loss: 1.3530, current_lr: 0.000375
step 1531, loss: 1.3496, current_lr: 0.000375
step 1532, loss: 1.4597, current_lr: 0.000376
step 1533, loss: 1.3922, current_lr: 0.000376
step 1534, loss: 1.3328, current_lr: 0.000376
step 1535, loss: 1.3788, current_lr: 0.000377
step 1536, loss: 1.3517, current_lr: 0.000377
step 1537, loss: 1.3239, current_lr: 0.000378
step 1538, loss: 1.2860, current_lr: 0.000378
step 1539, loss: 1.2989, current_lr: 0.000378
step 1540, loss: 1.2952, current_lr: 0.000379
step 1541, loss: 1.2711, current_lr: 0.000379
step 1542, loss: 1.2765, current_lr: 0.000379
step 1543, loss: 1.2585, current_lr: 0.000380
step 1544, loss: 1.1771, current_lr: 0.000380
step 1545, loss: 1.2585, current_lr: 0.000380
step 1546, loss: 1.2704, current_lr: 0.000381
step 1547, loss: 1.2641, current_lr: 0.000381
step 1548, loss: 1.3821, current_lr: 0.000381
step 1549, loss: 1.3412, current_lr: 0.000382
step 1550, loss: 1.3465, current_lr: 0.000382
step 1551, loss: 1.3640, current_lr: 0.000382
step 1552, loss: 1.3422, current_lr: 0.000383
step 1553, loss: 1.3138, current_lr: 0.000383
step 1554, loss: 1.3440, current_lr: 0.000384
step 1555, loss: 1.2756, current_lr: 0.000384
step 1556, loss: 1.2460, current_lr: 0.000384
step 1557, loss: 1.2398, current_lr: 0.000385
step 1558, loss: 1.3081, current_lr: 0.000385
step 1559, loss: 1.1477, current_lr: 0.000385
Saved best model with loss: 1.1477 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.1477.pt
step 1560, loss: 1.2947, current_lr: 0.000386
step 1561, loss: 1.2238, current_lr: 0.000386
step 1562, loss: 1.2140, current_lr: 0.000386
step 1563, loss: 1.3097, current_lr: 0.000387
step 1564, loss: 1.1966, current_lr: 0.000387
step 1565, loss: 1.2403, current_lr: 0.000387
step 1566, loss: 1.2111, current_lr: 0.000388
step 1567, loss: 1.2035, current_lr: 0.000388
step 1568, loss: 1.2816, current_lr: 0.000388
step 1569, loss: 1.2296, current_lr: 0.000389
step 1570, loss: 1.2174, current_lr: 0.000389
step 1571, loss: 1.1833, current_lr: 0.000389
step 1572, loss: 1.3240, current_lr: 0.000390
step 1573, loss: 1.2807, current_lr: 0.000390
step 1574, loss: 1.3315, current_lr: 0.000391
step 1575, loss: 1.2625, current_lr: 0.000391
step 1576, loss: 1.3248, current_lr: 0.000391
step 1577, loss: 1.3175, current_lr: 0.000392
step 1578, loss: 1.2125, current_lr: 0.000392
step 1579, loss: 1.1210, current_lr: 0.000392
Saved best model with loss: 1.1210 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.1210.pt
step 1580, loss: 1.1918, current_lr: 0.000393
step 1581, loss: 1.2694, current_lr: 0.000393
step 1582, loss: 1.2539, current_lr: 0.000393
step 1583, loss: 1.2486, current_lr: 0.000394
step 1584, loss: 1.2133, current_lr: 0.000394
step 1585, loss: 1.2569, current_lr: 0.000394
step 1586, loss: 1.2166, current_lr: 0.000395
step 1587, loss: 1.1629, current_lr: 0.000395
step 1588, loss: 1.1359, current_lr: 0.000395
step 1589, loss: 1.3578, current_lr: 0.000396
step 1590, loss: 1.3324, current_lr: 0.000396
step 1591, loss: 1.3543, current_lr: 0.000397
step 1592, loss: 1.3725, current_lr: 0.000397
step 1593, loss: 1.4476, current_lr: 0.000397
step 1594, loss: 1.3572, current_lr: 0.000398
step 1595, loss: 1.4328, current_lr: 0.000398
step 1596, loss: 1.4306, current_lr: 0.000398
step 1597, loss: 1.3418, current_lr: 0.000399
step 1598, loss: 1.3187, current_lr: 0.000399
step 1599, loss: 1.3127, current_lr: 0.000399
step 1600, loss: 1.2870, current_lr: 0.000400
step 1601, loss: 1.2503, current_lr: 0.000400
step 1602, loss: 1.4634, current_lr: 0.000400
step 1603, loss: 1.2274, current_lr: 0.000401
step 1604, loss: 1.1920, current_lr: 0.000401
step 1605, loss: 1.5895, current_lr: 0.000401
step 1606, loss: 1.5079, current_lr: 0.000402
step 1607, loss: 1.3909, current_lr: 0.000402
step 1608, loss: 1.5748, current_lr: 0.000402
step 1609, loss: 1.2641, current_lr: 0.000403
step 1610, loss: 1.5957, current_lr: 0.000403
step 1611, loss: 1.4651, current_lr: 0.000404
step 1612, loss: 1.2389, current_lr: 0.000404
step 1613, loss: 1.4992, current_lr: 0.000404
step 1614, loss: 1.4906, current_lr: 0.000405
step 1615, loss: 1.4894, current_lr: 0.000405
step 1616, loss: 1.4272, current_lr: 0.000405
step 1617, loss: 1.4801, current_lr: 0.000406
step 1618, loss: 1.2549, current_lr: 0.000406
step 1619, loss: 1.4000, current_lr: 0.000406
step 1620, loss: 1.2527, current_lr: 0.000407
step 1621, loss: 1.1699, current_lr: 0.000407
step 1622, loss: 1.2931, current_lr: 0.000407
step 1623, loss: 1.2785, current_lr: 0.000408
step 1624, loss: 1.3394, current_lr: 0.000408
step 1625, loss: 1.2625, current_lr: 0.000408
step 1626, loss: 1.3471, current_lr: 0.000409
step 1627, loss: 1.3596, current_lr: 0.000409
step 1628, loss: 1.3012, current_lr: 0.000409
step 1629, loss: 1.3441, current_lr: 0.000410
step 1630, loss: 1.3042, current_lr: 0.000410
step 1631, loss: 1.2777, current_lr: 0.000410
step 1632, loss: 1.2742, current_lr: 0.000411
step 1633, loss: 1.1814, current_lr: 0.000411
step 1634, loss: 1.2434, current_lr: 0.000412
step 1635, loss: 1.2187, current_lr: 0.000412
step 1636, loss: 1.1489, current_lr: 0.000412
step 1637, loss: 1.1151, current_lr: 0.000413
Saved best model with loss: 1.1151 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.1151.pt
step 1638, loss: 1.1879, current_lr: 0.000413
step 1639, loss: 1.1199, current_lr: 0.000413
step 1640, loss: 1.1377, current_lr: 0.000414
step 1641, loss: 1.2193, current_lr: 0.000414
step 1642, loss: 1.1403, current_lr: 0.000414
step 1643, loss: 1.0084, current_lr: 0.000415
Saved best model with loss: 1.0084 to fine_tuned_gpt2\gpt2_finetuned_best_loss_1.0084.pt
step 1644, loss: 1.2522, current_lr: 0.000415
step 1645, loss: 1.3706, current_lr: 0.000415
step 1646, loss: 1.2919, current_lr: 0.000416
step 1647, loss: 1.3448, current_lr: 0.000416
step 1648, loss: 1.2461, current_lr: 0.000416
step 1649, loss: 1.2728, current_lr: 0.000417
step 1650, loss: 1.3394, current_lr: 0.000417
step 1651, loss: 1.2469, current_lr: 0.000417
step 1652, loss: 1.2250, current_lr: 0.000418
step 1653, loss: 1.2338, current_lr: 0.000418
step 1654, loss: 1.3149, current_lr: 0.000418
step 1655, loss: 1.1281, current_lr: 0.000419
step 1656, loss: 1.1740, current_lr: 0.000419
step 1657, loss: 1.2915, current_lr: 0.000420
step 1658, loss: 1.1644, current_lr: 0.000420
step 1659, loss: 1.2035, current_lr: 0.000420
step 1660, loss: 1.1326, current_lr: 0.000421
step 1661, loss: 1.0206, current_lr: 0.000421
step 1662, loss: 1.1496, current_lr: 0.000421
step 1663, loss: 1.0469, current_lr: 0.000422
step 1664, loss: 1.1657, current_lr: 0.000422
step 1665, loss: 1.0091, current_lr: 0.000422
step 1666, loss: 1.0598, current_lr: 0.000423
step 1667, loss: 0.9016, current_lr: 0.000423
Saved best model with loss: 0.9016 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.9016.pt
step 1668, loss: 1.0397, current_lr: 0.000423
step 1669, loss: 1.0938, current_lr: 0.000424
step 1670, loss: 1.1014, current_lr: 0.000424
step 1671, loss: 0.9935, current_lr: 0.000424
step 1672, loss: 0.9948, current_lr: 0.000425
step 1673, loss: 1.1152, current_lr: 0.000425
step 1674, loss: 1.0529, current_lr: 0.000425
step 1675, loss: 1.0548, current_lr: 0.000426
step 1676, loss: 1.1061, current_lr: 0.000426
step 1677, loss: 1.1438, current_lr: 0.000426
step 1678, loss: 1.1043, current_lr: 0.000427
step 1679, loss: 0.9677, current_lr: 0.000427
step 1680, loss: 1.0449, current_lr: 0.000427
step 1681, loss: 1.0447, current_lr: 0.000428
step 1682, loss: 1.0259, current_lr: 0.000428
step 1683, loss: 0.9826, current_lr: 0.000429
step 1684, loss: 0.9749, current_lr: 0.000429
step 1685, loss: 0.9574, current_lr: 0.000429
step 1686, loss: 0.9647, current_lr: 0.000430
step 1687, loss: 0.9892, current_lr: 0.000430
step 1688, loss: 0.9904, current_lr: 0.000430
step 1689, loss: 0.9373, current_lr: 0.000431
step 1690, loss: 0.9806, current_lr: 0.000431
step 1691, loss: 0.8917, current_lr: 0.000431
Saved best model with loss: 0.8917 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.8917.pt
step 1692, loss: 0.8629, current_lr: 0.000432
Saved best model with loss: 0.8629 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.8629.pt
step 1693, loss: 0.9241, current_lr: 0.000432
step 1694, loss: 1.0079, current_lr: 0.000432
step 1695, loss: 1.0164, current_lr: 0.000433
step 1696, loss: 0.9770, current_lr: 0.000433
step 1697, loss: 1.1527, current_lr: 0.000433
step 1698, loss: 1.0865, current_lr: 0.000434
step 1699, loss: 1.0257, current_lr: 0.000434
step 1700, loss: 0.9964, current_lr: 0.000434
step 1701, loss: 1.0308, current_lr: 0.000435
step 1702, loss: 0.9772, current_lr: 0.000435
step 1703, loss: 0.9444, current_lr: 0.000435
step 1704, loss: 0.9642, current_lr: 0.000436
step 1705, loss: 0.9925, current_lr: 0.000436
step 1706, loss: 0.9827, current_lr: 0.000436
step 1707, loss: 0.9212, current_lr: 0.000437
step 1708, loss: 0.9317, current_lr: 0.000437
step 1709, loss: 0.8725, current_lr: 0.000437
step 1710, loss: 0.9715, current_lr: 0.000438
step 1711, loss: 0.9968, current_lr: 0.000438
step 1712, loss: 0.9873, current_lr: 0.000439
step 1713, loss: 1.0200, current_lr: 0.000439
step 1714, loss: 0.9620, current_lr: 0.000439
step 1715, loss: 1.0350, current_lr: 0.000440
step 1716, loss: 1.1062, current_lr: 0.000440
step 1717, loss: 1.0159, current_lr: 0.000440
step 1718, loss: 0.9983, current_lr: 0.000441
step 1719, loss: 1.0274, current_lr: 0.000441
step 1720, loss: 1.0310, current_lr: 0.000441
step 1721, loss: 0.9992, current_lr: 0.000442
step 1722, loss: 0.9661, current_lr: 0.000442
step 1723, loss: 1.0409, current_lr: 0.000442
step 1724, loss: 0.9634, current_lr: 0.000443
step 1725, loss: 0.9912, current_lr: 0.000443
step 1726, loss: 0.9693, current_lr: 0.000443
step 1727, loss: 1.0102, current_lr: 0.000444
step 1728, loss: 1.0294, current_lr: 0.000444
step 1729, loss: 1.0040, current_lr: 0.000444
step 1730, loss: 0.9805, current_lr: 0.000445
step 1731, loss: 0.9804, current_lr: 0.000445
step 1732, loss: 0.9630, current_lr: 0.000445
step 1733, loss: 1.0005, current_lr: 0.000446
step 1734, loss: 0.9699, current_lr: 0.000446
step 1735, loss: 1.0057, current_lr: 0.000446
step 1736, loss: 0.9474, current_lr: 0.000447
step 1737, loss: 0.9960, current_lr: 0.000447
step 1738, loss: 1.0510, current_lr: 0.000447
step 1739, loss: 0.9880, current_lr: 0.000448
step 1740, loss: 1.0070, current_lr: 0.000448
step 1741, loss: 0.9747, current_lr: 0.000448
step 1742, loss: 1.0290, current_lr: 0.000449
step 1743, loss: 0.9750, current_lr: 0.000449
step 1744, loss: 0.8617, current_lr: 0.000449
Saved best model with loss: 0.8617 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.8617.pt
step 1745, loss: 0.9078, current_lr: 0.000450
step 1746, loss: 0.9454, current_lr: 0.000450
step 1747, loss: 0.9817, current_lr: 0.000450
step 1748, loss: 0.9759, current_lr: 0.000451
step 1749, loss: 0.8842, current_lr: 0.000451
step 1750, loss: 0.9195, current_lr: 0.000452
step 1751, loss: 0.9338, current_lr: 0.000452
step 1752, loss: 0.9448, current_lr: 0.000452
step 1753, loss: 0.9257, current_lr: 0.000453
step 1754, loss: 1.0464, current_lr: 0.000453
step 1755, loss: 1.0321, current_lr: 0.000453
step 1756, loss: 0.9503, current_lr: 0.000454
step 1757, loss: 1.0038, current_lr: 0.000454
step 1758, loss: 1.0803, current_lr: 0.000454
step 1759, loss: 1.0371, current_lr: 0.000455
step 1760, loss: 1.1717, current_lr: 0.000455
step 1761, loss: 1.1109, current_lr: 0.000455
step 1762, loss: 1.0633, current_lr: 0.000456
step 1763, loss: 0.9700, current_lr: 0.000456
step 1764, loss: 0.9702, current_lr: 0.000456
step 1765, loss: 0.9107, current_lr: 0.000457
step 1766, loss: 0.9452, current_lr: 0.000457
step 1767, loss: 1.1430, current_lr: 0.000457
step 1768, loss: 0.9323, current_lr: 0.000458
step 1769, loss: 0.9490, current_lr: 0.000458
step 1770, loss: 1.1511, current_lr: 0.000458
step 1771, loss: 1.0758, current_lr: 0.000459
step 1772, loss: 1.0744, current_lr: 0.000459
step 1773, loss: 1.1365, current_lr: 0.000459
step 1774, loss: 0.9459, current_lr: 0.000460
step 1775, loss: 1.1542, current_lr: 0.000460
step 1776, loss: 1.0481, current_lr: 0.000460
step 1777, loss: 0.9297, current_lr: 0.000461
step 1778, loss: 1.1500, current_lr: 0.000461
step 1779, loss: 1.0799, current_lr: 0.000461
step 1780, loss: 1.1131, current_lr: 0.000462
step 1781, loss: 1.0736, current_lr: 0.000462
step 1782, loss: 1.1606, current_lr: 0.000462
step 1783, loss: 1.0724, current_lr: 0.000463
step 1784, loss: 1.0512, current_lr: 0.000463
step 1785, loss: 0.9937, current_lr: 0.000463
step 1786, loss: 0.9603, current_lr: 0.000464
step 1787, loss: 1.0189, current_lr: 0.000464
step 1788, loss: 0.9979, current_lr: 0.000464
step 1789, loss: 1.0438, current_lr: 0.000465
step 1790, loss: 0.9421, current_lr: 0.000465
step 1791, loss: 1.0867, current_lr: 0.000465
step 1792, loss: 0.9662, current_lr: 0.000466
step 1793, loss: 1.0470, current_lr: 0.000466
step 1794, loss: 1.0473, current_lr: 0.000466
step 1795, loss: 0.9572, current_lr: 0.000467
step 1796, loss: 0.9979, current_lr: 0.000467
step 1797, loss: 1.0291, current_lr: 0.000467
step 1798, loss: 0.9063, current_lr: 0.000468
step 1799, loss: 0.9603, current_lr: 0.000468
step 1800, loss: 0.9466, current_lr: 0.000468
step 1801, loss: 0.9267, current_lr: 0.000469
step 1802, loss: 0.8887, current_lr: 0.000469
step 1803, loss: 0.9183, current_lr: 0.000469
step 1804, loss: 0.8432, current_lr: 0.000470
Saved best model with loss: 0.8432 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.8432.pt
step 1805, loss: 0.9297, current_lr: 0.000470
step 1806, loss: 1.0054, current_lr: 0.000470
step 1807, loss: 0.9445, current_lr: 0.000471
step 1808, loss: 0.8770, current_lr: 0.000471
step 1809, loss: 0.9164, current_lr: 0.000471
step 1810, loss: 1.1142, current_lr: 0.000472
step 1811, loss: 0.9262, current_lr: 0.000472
step 1812, loss: 0.9627, current_lr: 0.000472
step 1813, loss: 0.9197, current_lr: 0.000473
step 1814, loss: 0.9346, current_lr: 0.000473
step 1815, loss: 0.9942, current_lr: 0.000473
step 1816, loss: 1.0329, current_lr: 0.000474
step 1817, loss: 0.9734, current_lr: 0.000474
step 1818, loss: 0.9813, current_lr: 0.000474
step 1819, loss: 0.9638, current_lr: 0.000475
step 1820, loss: 0.8235, current_lr: 0.000475
Saved best model with loss: 0.8235 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.8235.pt
step 1821, loss: 0.8910, current_lr: 0.000475
step 1822, loss: 0.9138, current_lr: 0.000476
step 1823, loss: 0.8755, current_lr: 0.000476
step 1824, loss: 0.8879, current_lr: 0.000476
step 1825, loss: 0.9443, current_lr: 0.000477
step 1826, loss: 0.8738, current_lr: 0.000477
step 1827, loss: 0.8724, current_lr: 0.000477
step 1828, loss: 0.7980, current_lr: 0.000478
Saved best model with loss: 0.7980 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7980.pt
step 1829, loss: 0.8449, current_lr: 0.000478
step 1830, loss: 0.7610, current_lr: 0.000478
Saved best model with loss: 0.7610 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7610.pt
step 1831, loss: 0.8034, current_lr: 0.000479
step 1832, loss: 0.7423, current_lr: 0.000479
Saved best model with loss: 0.7423 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7423.pt
step 1833, loss: 0.7811, current_lr: 0.000479
step 1834, loss: 0.7827, current_lr: 0.000480
step 1835, loss: 0.8363, current_lr: 0.000480
step 1836, loss: 0.7523, current_lr: 0.000480
step 1837, loss: 0.7818, current_lr: 0.000481
step 1838, loss: 0.8597, current_lr: 0.000481
step 1839, loss: 0.7308, current_lr: 0.000481
Saved best model with loss: 0.7308 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7308.pt
step 1840, loss: 0.7561, current_lr: 0.000482
step 1841, loss: 0.8797, current_lr: 0.000482
step 1842, loss: 0.8267, current_lr: 0.000482
step 1843, loss: 0.8380, current_lr: 0.000483
step 1844, loss: 0.8649, current_lr: 0.000483
step 1845, loss: 0.8471, current_lr: 0.000483
step 1846, loss: 0.8693, current_lr: 0.000484
step 1847, loss: 0.7253, current_lr: 0.000484
Saved best model with loss: 0.7253 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7253.pt
step 1848, loss: 0.8186, current_lr: 0.000484
step 1849, loss: 0.7369, current_lr: 0.000485
step 1850, loss: 0.7478, current_lr: 0.000485
step 1851, loss: 0.7300, current_lr: 0.000485
step 1852, loss: 0.7007, current_lr: 0.000486
Saved best model with loss: 0.7007 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.7007.pt
step 1853, loss: 0.7147, current_lr: 0.000486
step 1854, loss: 0.7324, current_lr: 0.000486
step 1855, loss: 0.7429, current_lr: 0.000487
step 1856, loss: 0.6554, current_lr: 0.000487
Saved best model with loss: 0.6554 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6554.pt
step 1857, loss: 0.6259, current_lr: 0.000487
Saved best model with loss: 0.6259 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6259.pt
step 1858, loss: 0.6558, current_lr: 0.000488
step 1859, loss: 0.6700, current_lr: 0.000488
step 1860, loss: 0.7225, current_lr: 0.000488
step 1861, loss: 0.7010, current_lr: 0.000489
step 1862, loss: 0.7897, current_lr: 0.000489
step 1863, loss: 0.8063, current_lr: 0.000489
step 1864, loss: 0.7452, current_lr: 0.000490
step 1865, loss: 0.7931, current_lr: 0.000490
step 1866, loss: 0.7585, current_lr: 0.000490
step 1867, loss: 0.7583, current_lr: 0.000491
step 1868, loss: 0.7600, current_lr: 0.000491
step 1869, loss: 0.6873, current_lr: 0.000491
step 1870, loss: 0.7912, current_lr: 0.000491
step 1871, loss: 0.6826, current_lr: 0.000492
step 1872, loss: 0.7015, current_lr: 0.000492
step 1873, loss: 0.7068, current_lr: 0.000492
step 1874, loss: 0.6805, current_lr: 0.000493
step 1875, loss: 0.7034, current_lr: 0.000493
step 1876, loss: 0.7298, current_lr: 0.000493
step 1877, loss: 0.7084, current_lr: 0.000494
step 1878, loss: 0.9095, current_lr: 0.000494
step 1879, loss: 0.7520, current_lr: 0.000494
step 1880, loss: 0.7692, current_lr: 0.000495
step 1881, loss: 0.7781, current_lr: 0.000495
step 1882, loss: 0.7536, current_lr: 0.000495
step 1883, loss: 0.7880, current_lr: 0.000496
step 1884, loss: 0.7494, current_lr: 0.000496
step 1885, loss: 0.7043, current_lr: 0.000496
step 1886, loss: 0.6953, current_lr: 0.000497
step 1887, loss: 0.6867, current_lr: 0.000497
step 1888, loss: 0.7481, current_lr: 0.000497
step 1889, loss: 0.7563, current_lr: 0.000498
step 1890, loss: 0.7985, current_lr: 0.000498
step 1891, loss: 0.7269, current_lr: 0.000498
step 1892, loss: 0.7557, current_lr: 0.000499
step 1893, loss: 0.7642, current_lr: 0.000499
step 1894, loss: 0.6840, current_lr: 0.000499
step 1895, loss: 0.7461, current_lr: 0.000500
step 1896, loss: 0.7206, current_lr: 0.000500
step 1897, loss: 0.7194, current_lr: 0.000500
step 1898, loss: 0.7711, current_lr: 0.000501
step 1899, loss: 0.7440, current_lr: 0.000501
step 1900, loss: 0.7881, current_lr: 0.000501
step 1901, loss: 0.7461, current_lr: 0.000502
step 1902, loss: 0.7183, current_lr: 0.000502
step 1903, loss: 0.7537, current_lr: 0.000502
step 1904, loss: 0.7278, current_lr: 0.000502
step 1905, loss: 0.7241, current_lr: 0.000503
step 1906, loss: 0.8019, current_lr: 0.000503
step 1907, loss: 0.8421, current_lr: 0.000503
step 1908, loss: 0.7444, current_lr: 0.000504
step 1909, loss: 0.7123, current_lr: 0.000504
step 1910, loss: 0.6953, current_lr: 0.000504
step 1911, loss: 0.7073, current_lr: 0.000505
step 1912, loss: 0.7323, current_lr: 0.000505
step 1913, loss: 0.7355, current_lr: 0.000505
step 1914, loss: 0.7070, current_lr: 0.000506
step 1915, loss: 0.6596, current_lr: 0.000506
step 1916, loss: 0.7088, current_lr: 0.000506
step 1917, loss: 0.7523, current_lr: 0.000507
step 1918, loss: 0.7191, current_lr: 0.000507
step 1919, loss: 0.8715, current_lr: 0.000507
step 1920, loss: 0.7625, current_lr: 0.000508
step 1921, loss: 0.7046, current_lr: 0.000508
step 1922, loss: 0.7686, current_lr: 0.000508
step 1923, loss: 0.8394, current_lr: 0.000509
step 1924, loss: 0.7939, current_lr: 0.000509
step 1925, loss: 0.8176, current_lr: 0.000509
step 1926, loss: 0.7472, current_lr: 0.000509
step 1927, loss: 0.7829, current_lr: 0.000510
step 1928, loss: 0.7192, current_lr: 0.000510
step 1929, loss: 0.7210, current_lr: 0.000510
step 1930, loss: 0.6867, current_lr: 0.000511
step 1931, loss: 0.7109, current_lr: 0.000511
step 1932, loss: 0.7998, current_lr: 0.000511
step 1933, loss: 0.7078, current_lr: 0.000512
step 1934, loss: 0.7722, current_lr: 0.000512
step 1935, loss: 0.7763, current_lr: 0.000512
step 1936, loss: 0.7398, current_lr: 0.000513
step 1937, loss: 0.7637, current_lr: 0.000513
step 1938, loss: 0.8656, current_lr: 0.000513
step 1939, loss: 0.7042, current_lr: 0.000514
step 1940, loss: 0.8450, current_lr: 0.000514
step 1941, loss: 0.7472, current_lr: 0.000514
step 1942, loss: 0.7834, current_lr: 0.000515
step 1943, loss: 0.8375, current_lr: 0.000515
step 1944, loss: 0.7755, current_lr: 0.000515
step 1945, loss: 0.7407, current_lr: 0.000515
step 1946, loss: 0.7730, current_lr: 0.000516
step 1947, loss: 0.7685, current_lr: 0.000516
step 1948, loss: 0.8486, current_lr: 0.000516
step 1949, loss: 0.7876, current_lr: 0.000517
step 1950, loss: 0.7723, current_lr: 0.000517
step 1951, loss: 0.7164, current_lr: 0.000517
step 1952, loss: 0.7472, current_lr: 0.000518
step 1953, loss: 0.7301, current_lr: 0.000518
step 1954, loss: 0.7491, current_lr: 0.000518
step 1955, loss: 0.7367, current_lr: 0.000519
step 1956, loss: 0.8068, current_lr: 0.000519
step 1957, loss: 0.7540, current_lr: 0.000519
step 1958, loss: 0.7845, current_lr: 0.000520
step 1959, loss: 0.8068, current_lr: 0.000520
step 1960, loss: 0.6791, current_lr: 0.000520
step 1961, loss: 0.7371, current_lr: 0.000520
step 1962, loss: 0.7221, current_lr: 0.000521
step 1963, loss: 0.6264, current_lr: 0.000521
step 1964, loss: 0.6742, current_lr: 0.000521
step 1965, loss: 0.6614, current_lr: 0.000522
step 1966, loss: 0.6563, current_lr: 0.000522
step 1967, loss: 0.7181, current_lr: 0.000522
step 1968, loss: 0.7090, current_lr: 0.000523
step 1969, loss: 0.6432, current_lr: 0.000523
step 1970, loss: 0.6911, current_lr: 0.000523
step 1971, loss: 0.6578, current_lr: 0.000524
step 1972, loss: 0.6653, current_lr: 0.000524
step 1973, loss: 0.6176, current_lr: 0.000524
Saved best model with loss: 0.6176 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6176.pt
step 1974, loss: 0.6503, current_lr: 0.000525
step 1975, loss: 0.8795, current_lr: 0.000525
step 1976, loss: 0.7608, current_lr: 0.000525
step 1977, loss: 0.6994, current_lr: 0.000525
step 1978, loss: 0.7101, current_lr: 0.000526
step 1979, loss: 0.6711, current_lr: 0.000526
step 1980, loss: 0.6953, current_lr: 0.000526
step 1981, loss: 0.7044, current_lr: 0.000527
step 1982, loss: 0.7337, current_lr: 0.000527
step 1983, loss: 0.7387, current_lr: 0.000527
step 1984, loss: 0.7419, current_lr: 0.000528
step 1985, loss: 0.6811, current_lr: 0.000528
step 1986, loss: 0.7277, current_lr: 0.000528
step 1987, loss: 0.6770, current_lr: 0.000529
step 1988, loss: 0.6555, current_lr: 0.000529
step 1989, loss: 0.6704, current_lr: 0.000529
step 1990, loss: 0.6172, current_lr: 0.000529
Saved best model with loss: 0.6172 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.6172.pt
step 1991, loss: 0.6712, current_lr: 0.000530
step 1992, loss: 0.6469, current_lr: 0.000530
step 1993, loss: 0.6506, current_lr: 0.000530
step 1994, loss: 0.7013, current_lr: 0.000531
step 1995, loss: 0.5914, current_lr: 0.000531
Saved best model with loss: 0.5914 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5914.pt
step 1996, loss: 0.6129, current_lr: 0.000531
step 1997, loss: 0.5709, current_lr: 0.000532
Saved best model with loss: 0.5709 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5709.pt
step 1998, loss: 0.5935, current_lr: 0.000532
step 1999, loss: 0.5876, current_lr: 0.000532
step 2000, loss: 0.6412, current_lr: 0.000533
step 2001, loss: 0.5714, current_lr: 0.000533
step 2002, loss: 0.6007, current_lr: 0.000533
step 2003, loss: 0.6159, current_lr: 0.000533
step 2004, loss: 0.5592, current_lr: 0.000534
Saved best model with loss: 0.5592 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5592.pt
step 2005, loss: 0.5734, current_lr: 0.000534
step 2006, loss: 0.6753, current_lr: 0.000534
step 2007, loss: 0.6161, current_lr: 0.000535
step 2008, loss: 0.5803, current_lr: 0.000535
step 2009, loss: 0.5170, current_lr: 0.000535
Saved best model with loss: 0.5170 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.5170.pt
step 2010, loss: 0.6544, current_lr: 0.000536
step 2011, loss: 0.6755, current_lr: 0.000536
step 2012, loss: 0.6126, current_lr: 0.000536
step 2013, loss: 0.6275, current_lr: 0.000536
step 2014, loss: 0.6171, current_lr: 0.000537
step 2015, loss: 0.6117, current_lr: 0.000537
step 2016, loss: 0.5656, current_lr: 0.000537
step 2017, loss: 0.5440, current_lr: 0.000538
step 2018, loss: 0.5975, current_lr: 0.000538
step 2019, loss: 0.5350, current_lr: 0.000538
step 2020, loss: 0.5391, current_lr: 0.000539
step 2021, loss: 0.5277, current_lr: 0.000539
step 2022, loss: 0.4828, current_lr: 0.000539
Saved best model with loss: 0.4828 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4828.pt
step 2023, loss: 0.5564, current_lr: 0.000539
step 2024, loss: 0.5208, current_lr: 0.000540
step 2025, loss: 0.5919, current_lr: 0.000540
step 2026, loss: 0.5103, current_lr: 0.000540
step 2027, loss: 0.5192, current_lr: 0.000541
step 2028, loss: 0.5494, current_lr: 0.000541
step 2029, loss: 0.5181, current_lr: 0.000541
step 2030, loss: 0.5977, current_lr: 0.000542
step 2031, loss: 0.6122, current_lr: 0.000542
step 2032, loss: 0.5652, current_lr: 0.000542
step 2033, loss: 0.6363, current_lr: 0.000542
step 2034, loss: 0.5799, current_lr: 0.000543
step 2035, loss: 0.5799, current_lr: 0.000543
step 2036, loss: 0.6103, current_lr: 0.000543
step 2037, loss: 0.5691, current_lr: 0.000544
step 2038, loss: 0.5878, current_lr: 0.000544
step 2039, loss: 0.5365, current_lr: 0.000544
step 2040, loss: 0.5831, current_lr: 0.000545
step 2041, loss: 0.5988, current_lr: 0.000545
step 2042, loss: 0.5966, current_lr: 0.000545
step 2043, loss: 0.8905, current_lr: 0.000545
step 2044, loss: 0.5179, current_lr: 0.000546
step 2045, loss: 0.5856, current_lr: 0.000546
step 2046, loss: 0.5883, current_lr: 0.000546
step 2047, loss: 0.5573, current_lr: 0.000547
step 2048, loss: 0.5534, current_lr: 0.000547
step 2049, loss: 0.5357, current_lr: 0.000547
step 2050, loss: 0.5904, current_lr: 0.000547
step 2051, loss: 0.5376, current_lr: 0.000548
step 2052, loss: 0.5407, current_lr: 0.000548
step 2053, loss: 0.5839, current_lr: 0.000548
step 2054, loss: 0.5235, current_lr: 0.000549
step 2055, loss: 0.5541, current_lr: 0.000549
step 2056, loss: 0.5231, current_lr: 0.000549
step 2057, loss: 0.6093, current_lr: 0.000550
step 2058, loss: 0.5825, current_lr: 0.000550
step 2059, loss: 0.5643, current_lr: 0.000550
step 2060, loss: 0.5673, current_lr: 0.000550
step 2061, loss: 0.6158, current_lr: 0.000551
step 2062, loss: 0.5390, current_lr: 0.000551
step 2063, loss: 0.5930, current_lr: 0.000551
step 2064, loss: 0.5531, current_lr: 0.000552
step 2065, loss: 0.6035, current_lr: 0.000552
step 2066, loss: 0.5375, current_lr: 0.000552
step 2067, loss: 0.5850, current_lr: 0.000552
step 2068, loss: 0.5640, current_lr: 0.000553
step 2069, loss: 0.5233, current_lr: 0.000553
step 2070, loss: 0.5269, current_lr: 0.000553
step 2071, loss: 0.5439, current_lr: 0.000554
step 2072, loss: 0.5529, current_lr: 0.000554
step 2073, loss: 0.5336, current_lr: 0.000554
step 2074, loss: 0.5005, current_lr: 0.000555
step 2075, loss: 0.5818, current_lr: 0.000555
step 2076, loss: 0.5240, current_lr: 0.000555
step 2077, loss: 0.5768, current_lr: 0.000555
step 2078, loss: 0.5846, current_lr: 0.000556
step 2079, loss: 0.5274, current_lr: 0.000556
step 2080, loss: 0.5364, current_lr: 0.000556
step 2081, loss: 0.5622, current_lr: 0.000557
step 2082, loss: 0.5498, current_lr: 0.000557
step 2083, loss: 0.5054, current_lr: 0.000557
step 2084, loss: 0.5931, current_lr: 0.000557
step 2085, loss: 0.5514, current_lr: 0.000558
step 2086, loss: 0.5193, current_lr: 0.000558
step 2087, loss: 0.5336, current_lr: 0.000558
step 2088, loss: 0.6395, current_lr: 0.000559
step 2089, loss: 0.5889, current_lr: 0.000559
step 2090, loss: 0.5779, current_lr: 0.000559
step 2091, loss: 0.5917, current_lr: 0.000559
step 2092, loss: 0.5383, current_lr: 0.000560
step 2093, loss: 0.5415, current_lr: 0.000560
step 2094, loss: 0.6007, current_lr: 0.000560
step 2095, loss: 0.4798, current_lr: 0.000561
Saved best model with loss: 0.4798 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4798.pt
step 2096, loss: 0.5085, current_lr: 0.000561
step 2097, loss: 0.5792, current_lr: 0.000561
step 2098, loss: 0.5646, current_lr: 0.000561
step 2099, loss: 0.5955, current_lr: 0.000562
step 2100, loss: 0.6636, current_lr: 0.000562
step 2101, loss: 0.5336, current_lr: 0.000562
step 2102, loss: 0.5342, current_lr: 0.000563
step 2103, loss: 0.5555, current_lr: 0.000563
step 2104, loss: 0.5396, current_lr: 0.000563
step 2105, loss: 0.6093, current_lr: 0.000563
step 2106, loss: 0.5476, current_lr: 0.000564
step 2107, loss: 0.5653, current_lr: 0.000564
step 2108, loss: 0.6317, current_lr: 0.000564
step 2109, loss: 0.5968, current_lr: 0.000565
step 2110, loss: 0.5671, current_lr: 0.000565
step 2111, loss: 0.5877, current_lr: 0.000565
step 2112, loss: 0.5540, current_lr: 0.000565
step 2113, loss: 0.5700, current_lr: 0.000566
step 2114, loss: 0.6228, current_lr: 0.000566
step 2115, loss: 0.5713, current_lr: 0.000566
step 2116, loss: 0.5243, current_lr: 0.000567
step 2117, loss: 0.5289, current_lr: 0.000567
step 2118, loss: 0.5151, current_lr: 0.000567
step 2119, loss: 0.5599, current_lr: 0.000567
step 2120, loss: 0.5498, current_lr: 0.000568
step 2121, loss: 0.5915, current_lr: 0.000568
step 2122, loss: 0.5354, current_lr: 0.000568
step 2123, loss: 0.5793, current_lr: 0.000568
step 2124, loss: 0.5368, current_lr: 0.000569
step 2125, loss: 0.5512, current_lr: 0.000569
step 2126, loss: 0.5737, current_lr: 0.000569
step 2127, loss: 0.5556, current_lr: 0.000570
step 2128, loss: 0.4919, current_lr: 0.000570
step 2129, loss: 0.4860, current_lr: 0.000570
step 2130, loss: 0.5468, current_lr: 0.000570
step 2131, loss: 0.5166, current_lr: 0.000571
step 2132, loss: 0.4897, current_lr: 0.000571
step 2133, loss: 0.5534, current_lr: 0.000571
step 2134, loss: 0.5169, current_lr: 0.000572
step 2135, loss: 0.5724, current_lr: 0.000572
step 2136, loss: 0.4949, current_lr: 0.000572
step 2137, loss: 0.4881, current_lr: 0.000572
step 2138, loss: 0.4786, current_lr: 0.000573
Saved best model with loss: 0.4786 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4786.pt
step 2139, loss: 0.5570, current_lr: 0.000573
step 2140, loss: 0.6244, current_lr: 0.000573
step 2141, loss: 0.5839, current_lr: 0.000573
step 2142, loss: 0.5238, current_lr: 0.000574
step 2143, loss: 0.5658, current_lr: 0.000574
step 2144, loss: 0.5293, current_lr: 0.000574
step 2145, loss: 0.5245, current_lr: 0.000575
step 2146, loss: 0.5564, current_lr: 0.000575
step 2147, loss: 0.6175, current_lr: 0.000575
step 2148, loss: 0.5217, current_lr: 0.000575
step 2149, loss: 0.5641, current_lr: 0.000576
step 2150, loss: 0.5069, current_lr: 0.000576
step 2151, loss: 0.5351, current_lr: 0.000576
step 2152, loss: 0.4923, current_lr: 0.000576
step 2153, loss: 0.5838, current_lr: 0.000577
step 2154, loss: 0.5547, current_lr: 0.000577
step 2155, loss: 0.4614, current_lr: 0.000577
Saved best model with loss: 0.4614 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4614.pt
step 2156, loss: 0.5294, current_lr: 0.000578
step 2157, loss: 0.4927, current_lr: 0.000578
step 2158, loss: 0.5056, current_lr: 0.000578
step 2159, loss: 0.5105, current_lr: 0.000578
step 2160, loss: 0.4398, current_lr: 0.000579
Saved best model with loss: 0.4398 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4398.pt
step 2161, loss: 0.4224, current_lr: 0.000579
Saved best model with loss: 0.4224 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4224.pt
step 2162, loss: 0.4405, current_lr: 0.000579
step 2163, loss: 0.4678, current_lr: 0.000579
step 2164, loss: 0.4527, current_lr: 0.000580
step 2165, loss: 0.4810, current_lr: 0.000580
step 2166, loss: 0.4358, current_lr: 0.000580
step 2167, loss: 0.4215, current_lr: 0.000581
Saved best model with loss: 0.4215 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4215.pt
step 2168, loss: 0.4851, current_lr: 0.000581
step 2169, loss: 0.4408, current_lr: 0.000581
step 2170, loss: 0.4264, current_lr: 0.000581
step 2171, loss: 0.4953, current_lr: 0.000582
step 2172, loss: 0.4381, current_lr: 0.000582
step 2173, loss: 0.4105, current_lr: 0.000582
Saved best model with loss: 0.4105 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.4105.pt
step 2174, loss: 0.3866, current_lr: 0.000582
Saved best model with loss: 0.3866 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3866.pt
step 2175, loss: 0.4438, current_lr: 0.000583
step 2176, loss: 0.4599, current_lr: 0.000583
step 2177, loss: 0.4233, current_lr: 0.000583
step 2178, loss: 0.4592, current_lr: 0.000583
step 2179, loss: 0.4307, current_lr: 0.000584
step 2180, loss: 0.4556, current_lr: 0.000584
step 2181, loss: 0.4568, current_lr: 0.000584
step 2182, loss: 0.4535, current_lr: 0.000585
step 2183, loss: 0.4534, current_lr: 0.000585
step 2184, loss: 0.4017, current_lr: 0.000585
step 2185, loss: 0.4268, current_lr: 0.000585
step 2186, loss: 0.4155, current_lr: 0.000586
step 2187, loss: 0.3921, current_lr: 0.000586
step 2188, loss: 0.4436, current_lr: 0.000586
step 2189, loss: 0.4531, current_lr: 0.000586
step 2190, loss: 0.4224, current_lr: 0.000587
step 2191, loss: 0.3477, current_lr: 0.000587
Saved best model with loss: 0.3477 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3477.pt
step 2192, loss: 0.4449, current_lr: 0.000587
step 2193, loss: 0.4319, current_lr: 0.000587
step 2194, loss: 0.3425, current_lr: 0.000588
Saved best model with loss: 0.3425 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3425.pt
step 2195, loss: 0.4047, current_lr: 0.000588
step 2196, loss: 0.4982, current_lr: 0.000588
step 2197, loss: 0.4615, current_lr: 0.000588
step 2198, loss: 0.4498, current_lr: 0.000589
step 2199, loss: 0.3894, current_lr: 0.000589
step 2200, loss: 0.3993, current_lr: 0.000589
step 2201, loss: 0.4157, current_lr: 0.000590
step 2202, loss: 0.4368, current_lr: 0.000590
step 2203, loss: 0.4464, current_lr: 0.000590
step 2204, loss: 0.4018, current_lr: 0.000590
step 2205, loss: 0.4006, current_lr: 0.000591
step 2206, loss: 0.4202, current_lr: 0.000591
step 2207, loss: 0.4395, current_lr: 0.000591
step 2208, loss: 0.5846, current_lr: 0.000591
step 2209, loss: 0.4052, current_lr: 0.000592
step 2210, loss: 0.4471, current_lr: 0.000592
step 2211, loss: 0.4805, current_lr: 0.000592
step 2212, loss: 0.4005, current_lr: 0.000592
step 2213, loss: 0.3896, current_lr: 0.000593
step 2214, loss: 0.4270, current_lr: 0.000593
step 2215, loss: 0.3978, current_lr: 0.000593
step 2216, loss: 0.3623, current_lr: 0.000593
step 2217, loss: 0.3845, current_lr: 0.000594
step 2218, loss: 0.4598, current_lr: 0.000594
step 2219, loss: 0.4150, current_lr: 0.000594
step 2220, loss: 0.4093, current_lr: 0.000594
step 2221, loss: 0.3854, current_lr: 0.000595
step 2222, loss: 0.4018, current_lr: 0.000595
step 2223, loss: 0.3980, current_lr: 0.000595
step 2224, loss: 0.3866, current_lr: 0.000595
step 2225, loss: 0.3629, current_lr: 0.000596
step 2226, loss: 0.4622, current_lr: 0.000596
step 2227, loss: 0.4022, current_lr: 0.000596
step 2228, loss: 0.4451, current_lr: 0.000596
step 2229, loss: 0.4113, current_lr: 0.000597
step 2230, loss: 0.4001, current_lr: 0.000597
step 2231, loss: 0.3848, current_lr: 0.000597
step 2232, loss: 0.4362, current_lr: 0.000598
step 2233, loss: 0.4304, current_lr: 0.000598
step 2234, loss: 0.3874, current_lr: 0.000598
step 2235, loss: 0.3759, current_lr: 0.000598
step 2236, loss: 0.3870, current_lr: 0.000599
step 2237, loss: 0.4252, current_lr: 0.000599
step 2238, loss: 0.3599, current_lr: 0.000599
step 2239, loss: 0.3854, current_lr: 0.000599
step 2240, loss: 0.3496, current_lr: 0.000600
step 2241, loss: 0.4258, current_lr: 0.000600
step 2242, loss: 0.4090, current_lr: 0.000600
step 2243, loss: 0.4399, current_lr: 0.000600
step 2244, loss: 0.3597, current_lr: 0.000601
step 2245, loss: 0.3521, current_lr: 0.000601
step 2246, loss: 0.4318, current_lr: 0.000601
step 2247, loss: 0.4269, current_lr: 0.000601
step 2248, loss: 0.3879, current_lr: 0.000602
step 2249, loss: 0.4024, current_lr: 0.000602
step 2250, loss: 0.4450, current_lr: 0.000602
step 2251, loss: 0.4209, current_lr: 0.000602
step 2252, loss: 0.4102, current_lr: 0.000603
step 2253, loss: 0.3844, current_lr: 0.000603
step 2254, loss: 0.4692, current_lr: 0.000603
step 2255, loss: 0.4015, current_lr: 0.000603
step 2256, loss: 0.4396, current_lr: 0.000604
step 2257, loss: 0.4427, current_lr: 0.000604
step 2258, loss: 0.4014, current_lr: 0.000604
step 2259, loss: 0.4108, current_lr: 0.000604
step 2260, loss: 0.3505, current_lr: 0.000604
step 2261, loss: 0.4023, current_lr: 0.000605
step 2262, loss: 0.4492, current_lr: 0.000605
step 2263, loss: 0.4126, current_lr: 0.000605
step 2264, loss: 0.4060, current_lr: 0.000605
step 2265, loss: 0.4634, current_lr: 0.000606
step 2266, loss: 0.4085, current_lr: 0.000606
step 2267, loss: 0.4236, current_lr: 0.000606
step 2268, loss: 0.3955, current_lr: 0.000606
step 2269, loss: 0.3444, current_lr: 0.000607
step 2270, loss: 0.4378, current_lr: 0.000607
step 2271, loss: 0.4623, current_lr: 0.000607
step 2272, loss: 0.3966, current_lr: 0.000607
step 2273, loss: 0.4778, current_lr: 0.000608
step 2274, loss: 0.4504, current_lr: 0.000608
step 2275, loss: 0.4166, current_lr: 0.000608
step 2276, loss: 0.4290, current_lr: 0.000608
step 2277, loss: 0.4424, current_lr: 0.000609
step 2278, loss: 0.4218, current_lr: 0.000609
step 2279, loss: 0.4541, current_lr: 0.000609
step 2280, loss: 0.4507, current_lr: 0.000609
step 2281, loss: 0.4087, current_lr: 0.000610
step 2282, loss: 0.4117, current_lr: 0.000610
step 2283, loss: 0.4215, current_lr: 0.000610
step 2284, loss: 0.4229, current_lr: 0.000610
step 2285, loss: 0.4327, current_lr: 0.000611
step 2286, loss: 0.4427, current_lr: 0.000611
step 2287, loss: 0.4106, current_lr: 0.000611
step 2288, loss: 0.4187, current_lr: 0.000611
step 2289, loss: 0.4232, current_lr: 0.000612
step 2290, loss: 0.4335, current_lr: 0.000612
step 2291, loss: 0.3958, current_lr: 0.000612
step 2292, loss: 0.4459, current_lr: 0.000612
step 2293, loss: 0.3864, current_lr: 0.000612
step 2294, loss: 0.4453, current_lr: 0.000613
step 2295, loss: 0.4170, current_lr: 0.000613
step 2296, loss: 0.4064, current_lr: 0.000613
step 2297, loss: 0.4175, current_lr: 0.000613
step 2298, loss: 0.4104, current_lr: 0.000614
step 2299, loss: 0.3926, current_lr: 0.000614
step 2300, loss: 0.3814, current_lr: 0.000614
step 2301, loss: 0.3894, current_lr: 0.000614
step 2302, loss: 0.3929, current_lr: 0.000615
step 2303, loss: 0.3767, current_lr: 0.000615
step 2304, loss: 0.3783, current_lr: 0.000615
step 2305, loss: 0.4493, current_lr: 0.000615
step 2306, loss: 0.4628, current_lr: 0.000616
step 2307, loss: 0.4362, current_lr: 0.000616
step 2308, loss: 0.4359, current_lr: 0.000616
step 2309, loss: 0.4097, current_lr: 0.000616
step 2310, loss: 0.4107, current_lr: 0.000616
step 2311, loss: 0.4399, current_lr: 0.000617
step 2312, loss: 0.4643, current_lr: 0.000617
step 2313, loss: 0.3891, current_lr: 0.000617
step 2314, loss: 0.3960, current_lr: 0.000617
step 2315, loss: 0.3506, current_lr: 0.000618
step 2316, loss: 0.4453, current_lr: 0.000618
step 2317, loss: 0.4043, current_lr: 0.000618
step 2318, loss: 0.4195, current_lr: 0.000618
step 2319, loss: 0.3945, current_lr: 0.000619
step 2320, loss: 0.4083, current_lr: 0.000619
step 2321, loss: 0.3777, current_lr: 0.000619
step 2322, loss: 0.3951, current_lr: 0.000619
step 2323, loss: 0.3481, current_lr: 0.000619
step 2324, loss: 0.4063, current_lr: 0.000620
step 2325, loss: 0.3555, current_lr: 0.000620
step 2326, loss: 0.3290, current_lr: 0.000620
Saved best model with loss: 0.3290 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.3290.pt
step 2327, loss: 0.3302, current_lr: 0.000620
step 2328, loss: 0.4274, current_lr: 0.000621
step 2329, loss: 0.2930, current_lr: 0.000621
Saved best model with loss: 0.2930 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2930.pt
step 2330, loss: 0.3273, current_lr: 0.000621
step 2331, loss: 0.3559, current_lr: 0.000621
step 2332, loss: 0.3643, current_lr: 0.000621
step 2333, loss: 0.3812, current_lr: 0.000622
step 2334, loss: 0.3362, current_lr: 0.000622
step 2335, loss: 0.3207, current_lr: 0.000622
step 2336, loss: 0.3645, current_lr: 0.000622
step 2337, loss: 0.3495, current_lr: 0.000623
step 2338, loss: 0.3626, current_lr: 0.000623
step 2339, loss: 0.3508, current_lr: 0.000623
step 2340, loss: 0.3574, current_lr: 0.000623
step 2341, loss: 0.5929, current_lr: 0.000624
step 2342, loss: 0.3474, current_lr: 0.000624
step 2343, loss: 0.3283, current_lr: 0.000624
step 2344, loss: 0.3149, current_lr: 0.000624
step 2345, loss: 0.3236, current_lr: 0.000624
step 2346, loss: 0.3654, current_lr: 0.000625
step 2347, loss: 0.3374, current_lr: 0.000625
step 2348, loss: 0.3381, current_lr: 0.000625
step 2349, loss: 0.3593, current_lr: 0.000625
step 2350, loss: 0.3429, current_lr: 0.000626
step 2351, loss: 0.3206, current_lr: 0.000626
step 2352, loss: 0.2964, current_lr: 0.000626
step 2353, loss: 0.3286, current_lr: 0.000626
step 2354, loss: 0.3512, current_lr: 0.000626
step 2355, loss: 0.4079, current_lr: 0.000627
step 2356, loss: 0.3090, current_lr: 0.000627
step 2357, loss: 0.3665, current_lr: 0.000627
step 2358, loss: 0.3378, current_lr: 0.000627
step 2359, loss: 0.3206, current_lr: 0.000627
step 2360, loss: 0.3731, current_lr: 0.000628
step 2361, loss: 0.3291, current_lr: 0.000628
step 2362, loss: 0.3625, current_lr: 0.000628
step 2363, loss: 0.3180, current_lr: 0.000628
step 2364, loss: 0.3153, current_lr: 0.000629
step 2365, loss: 0.3598, current_lr: 0.000629
step 2366, loss: 0.3011, current_lr: 0.000629
step 2367, loss: 0.3433, current_lr: 0.000629
step 2368, loss: 0.3317, current_lr: 0.000629
step 2369, loss: 0.3053, current_lr: 0.000630
step 2370, loss: 0.3063, current_lr: 0.000630
step 2371, loss: 0.3300, current_lr: 0.000630
step 2372, loss: 0.3588, current_lr: 0.000630
step 2373, loss: 0.4423, current_lr: 0.000631
step 2374, loss: 0.3104, current_lr: 0.000631
step 2375, loss: 0.3235, current_lr: 0.000631
step 2376, loss: 0.3133, current_lr: 0.000631
step 2377, loss: 0.3248, current_lr: 0.000631
step 2378, loss: 0.3538, current_lr: 0.000632
step 2379, loss: 0.3550, current_lr: 0.000632
step 2380, loss: 0.3170, current_lr: 0.000632
step 2381, loss: 0.3092, current_lr: 0.000632
step 2382, loss: 0.3028, current_lr: 0.000632
step 2383, loss: 0.2944, current_lr: 0.000633
step 2384, loss: 0.3197, current_lr: 0.000633
step 2385, loss: 0.3108, current_lr: 0.000633
step 2386, loss: 0.3025, current_lr: 0.000633
step 2387, loss: 0.3519, current_lr: 0.000633
step 2388, loss: 0.3205, current_lr: 0.000634
step 2389, loss: 0.3054, current_lr: 0.000634
step 2390, loss: 0.2983, current_lr: 0.000634
step 2391, loss: 0.3513, current_lr: 0.000634
step 2392, loss: 0.3298, current_lr: 0.000635
step 2393, loss: 0.3310, current_lr: 0.000635
step 2394, loss: 0.3421, current_lr: 0.000635
step 2395, loss: 0.3139, current_lr: 0.000635
step 2396, loss: 0.3221, current_lr: 0.000635
step 2397, loss: 0.3319, current_lr: 0.000636
step 2398, loss: 0.3718, current_lr: 0.000636
step 2399, loss: 0.3268, current_lr: 0.000636
step 2400, loss: 0.3285, current_lr: 0.000636
step 2401, loss: 0.3322, current_lr: 0.000636
step 2402, loss: 0.3272, current_lr: 0.000637
step 2403, loss: 0.2901, current_lr: 0.000637
Saved best model with loss: 0.2901 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2901.pt
step 2404, loss: 0.3176, current_lr: 0.000637
step 2405, loss: 0.3127, current_lr: 0.000637
step 2406, loss: 0.3058, current_lr: 0.000637
step 2407, loss: 0.2930, current_lr: 0.000638
step 2408, loss: 0.3517, current_lr: 0.000638
step 2409, loss: 0.3465, current_lr: 0.000638
step 2410, loss: 0.2852, current_lr: 0.000638
Saved best model with loss: 0.2852 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2852.pt
step 2411, loss: 0.3033, current_lr: 0.000638
step 2412, loss: 0.2914, current_lr: 0.000639
step 2413, loss: 0.3276, current_lr: 0.000639
step 2414, loss: 0.3119, current_lr: 0.000639
step 2415, loss: 0.3613, current_lr: 0.000639
step 2416, loss: 0.2988, current_lr: 0.000639
step 2417, loss: 0.3075, current_lr: 0.000640
step 2418, loss: 0.3931, current_lr: 0.000640
step 2419, loss: 0.3555, current_lr: 0.000640
step 2420, loss: 0.3392, current_lr: 0.000640
step 2421, loss: 0.3385, current_lr: 0.000640
step 2422, loss: 0.3494, current_lr: 0.000641
step 2423, loss: 0.3264, current_lr: 0.000641
step 2424, loss: 0.3873, current_lr: 0.000641
step 2425, loss: 0.3385, current_lr: 0.000641
step 2426, loss: 0.3172, current_lr: 0.000641
step 2427, loss: 0.3087, current_lr: 0.000642
step 2428, loss: 0.3225, current_lr: 0.000642
step 2429, loss: 0.2804, current_lr: 0.000642
Saved best model with loss: 0.2804 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2804.pt
step 2430, loss: 0.3179, current_lr: 0.000642
step 2431, loss: 0.3028, current_lr: 0.000642
step 2432, loss: 0.3453, current_lr: 0.000643
step 2433, loss: 0.3309, current_lr: 0.000643
step 2434, loss: 0.3115, current_lr: 0.000643
step 2435, loss: 0.3128, current_lr: 0.000643
step 2436, loss: 0.2888, current_lr: 0.000643
step 2437, loss: 0.3013, current_lr: 0.000644
step 2438, loss: 0.3007, current_lr: 0.000644
step 2439, loss: 0.3271, current_lr: 0.000644
step 2440, loss: 0.3431, current_lr: 0.000644
step 2441, loss: 0.3147, current_lr: 0.000644
step 2442, loss: 0.3063, current_lr: 0.000645
step 2443, loss: 0.3123, current_lr: 0.000645
step 2444, loss: 0.3291, current_lr: 0.000645
step 2445, loss: 0.3225, current_lr: 0.000645
step 2446, loss: 0.2559, current_lr: 0.000645
Saved best model with loss: 0.2559 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2559.pt
step 2447, loss: 0.3349, current_lr: 0.000646
step 2448, loss: 0.3306, current_lr: 0.000646
step 2449, loss: 0.3105, current_lr: 0.000646
step 2450, loss: 0.3087, current_lr: 0.000646
step 2451, loss: 0.3309, current_lr: 0.000646
step 2452, loss: 0.3283, current_lr: 0.000647
step 2453, loss: 0.3325, current_lr: 0.000647
step 2454, loss: 0.2973, current_lr: 0.000647
step 2455, loss: 0.3192, current_lr: 0.000647
step 2456, loss: 0.3222, current_lr: 0.000647
step 2457, loss: 0.3454, current_lr: 0.000647
step 2458, loss: 0.2790, current_lr: 0.000648
step 2459, loss: 0.2932, current_lr: 0.000648
step 2460, loss: 0.2912, current_lr: 0.000648
step 2461, loss: 0.3102, current_lr: 0.000648
step 2462, loss: 0.3265, current_lr: 0.000648
step 2463, loss: 0.3378, current_lr: 0.000649
step 2464, loss: 0.3373, current_lr: 0.000649
step 2465, loss: 0.2986, current_lr: 0.000649
step 2466, loss: 0.2679, current_lr: 0.000649
step 2467, loss: 0.3199, current_lr: 0.000649
step 2468, loss: 0.2953, current_lr: 0.000650
step 2469, loss: 0.3373, current_lr: 0.000650
step 2470, loss: 0.3018, current_lr: 0.000650
step 2471, loss: 0.2427, current_lr: 0.000650
Saved best model with loss: 0.2427 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2427.pt
step 2472, loss: 0.2946, current_lr: 0.000650
step 2473, loss: 0.3307, current_lr: 0.000650
step 2474, loss: 0.2911, current_lr: 0.000651
step 2475, loss: 0.3254, current_lr: 0.000651
step 2476, loss: 0.2605, current_lr: 0.000651
step 2477, loss: 0.3559, current_lr: 0.000651
step 2478, loss: 0.2990, current_lr: 0.000651
step 2479, loss: 0.3481, current_lr: 0.000652
step 2480, loss: 0.3203, current_lr: 0.000652
step 2481, loss: 0.2975, current_lr: 0.000652
step 2482, loss: 0.2959, current_lr: 0.000652
step 2483, loss: 0.3285, current_lr: 0.000652
step 2484, loss: 0.2876, current_lr: 0.000652
step 2485, loss: 0.2808, current_lr: 0.000653
step 2486, loss: 0.2930, current_lr: 0.000653
step 2487, loss: 0.3525, current_lr: 0.000653
step 2488, loss: 0.3029, current_lr: 0.000653
step 2489, loss: 0.3386, current_lr: 0.000653
step 2490, loss: 0.2941, current_lr: 0.000654
step 2491, loss: 0.2638, current_lr: 0.000654
step 2492, loss: 0.2534, current_lr: 0.000654
step 2493, loss: 0.2708, current_lr: 0.000654
step 2494, loss: 0.3193, current_lr: 0.000654
step 2495, loss: 0.3393, current_lr: 0.000654
step 2496, loss: 0.2581, current_lr: 0.000655
step 2497, loss: 0.2689, current_lr: 0.000655
step 2498, loss: 0.2794, current_lr: 0.000655
step 2499, loss: 0.2508, current_lr: 0.000655
step 2500, loss: 0.2885, current_lr: 0.000655
step 2501, loss: 0.2828, current_lr: 0.000655
step 2502, loss: 0.2548, current_lr: 0.000656
step 2503, loss: 0.2764, current_lr: 0.000656
step 2504, loss: 0.2680, current_lr: 0.000656
step 2505, loss: 0.3455, current_lr: 0.000656
step 2506, loss: 0.2857, current_lr: 0.000656
step 2507, loss: 0.2573, current_lr: 0.000657
step 2508, loss: 0.2657, current_lr: 0.000657
step 2509, loss: 0.2746, current_lr: 0.000657
step 2510, loss: 0.2886, current_lr: 0.000657
step 2511, loss: 0.2740, current_lr: 0.000657
step 2512, loss: 0.2490, current_lr: 0.000657
step 2513, loss: 0.2943, current_lr: 0.000658
step 2514, loss: 0.3049, current_lr: 0.000658
step 2515, loss: 0.3167, current_lr: 0.000658
step 2516, loss: 0.2532, current_lr: 0.000658
step 2517, loss: 0.2346, current_lr: 0.000658
Saved best model with loss: 0.2346 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2346.pt
step 2518, loss: 0.2302, current_lr: 0.000658
Saved best model with loss: 0.2302 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2302.pt
step 2519, loss: 0.2546, current_lr: 0.000659
step 2520, loss: 0.2740, current_lr: 0.000659
step 2521, loss: 0.2583, current_lr: 0.000659
step 2522, loss: 0.3160, current_lr: 0.000659
step 2523, loss: 0.2699, current_lr: 0.000659
step 2524, loss: 0.2292, current_lr: 0.000659
Saved best model with loss: 0.2292 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2292.pt
step 2525, loss: 0.2615, current_lr: 0.000660
step 2526, loss: 0.2572, current_lr: 0.000660
step 2527, loss: 0.2968, current_lr: 0.000660
step 2528, loss: 0.3072, current_lr: 0.000660
step 2529, loss: 0.2568, current_lr: 0.000660
step 2530, loss: 0.2228, current_lr: 0.000660
Saved best model with loss: 0.2228 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2228.pt
step 2531, loss: 0.2399, current_lr: 0.000661
step 2532, loss: 0.2718, current_lr: 0.000661
step 2533, loss: 0.2419, current_lr: 0.000661
step 2534, loss: 0.2689, current_lr: 0.000661
step 2535, loss: 0.2650, current_lr: 0.000661
step 2536, loss: 0.2566, current_lr: 0.000661
step 2537, loss: 0.2486, current_lr: 0.000662
step 2538, loss: 0.3557, current_lr: 0.000662
step 2539, loss: 0.2384, current_lr: 0.000662
step 2540, loss: 0.4140, current_lr: 0.000662
step 2541, loss: 0.2587, current_lr: 0.000662
step 2542, loss: 0.2124, current_lr: 0.000662
Saved best model with loss: 0.2124 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2124.pt
step 2543, loss: 0.2757, current_lr: 0.000663
step 2544, loss: 0.3233, current_lr: 0.000663
step 2545, loss: 0.2772, current_lr: 0.000663
step 2546, loss: 0.2372, current_lr: 0.000663
step 2547, loss: 0.2741, current_lr: 0.000663
step 2548, loss: 0.2517, current_lr: 0.000663
step 2549, loss: 0.2495, current_lr: 0.000664
step 2550, loss: 0.2590, current_lr: 0.000664
step 2551, loss: 0.2623, current_lr: 0.000664
step 2552, loss: 0.2896, current_lr: 0.000664
step 2553, loss: 0.2647, current_lr: 0.000664
step 2554, loss: 0.2434, current_lr: 0.000664
step 2555, loss: 0.2621, current_lr: 0.000664
step 2556, loss: 0.3129, current_lr: 0.000665
step 2557, loss: 0.2685, current_lr: 0.000665
step 2558, loss: 0.2706, current_lr: 0.000665
step 2559, loss: 0.2399, current_lr: 0.000665
step 2560, loss: 0.2980, current_lr: 0.000665
step 2561, loss: 0.2533, current_lr: 0.000665
step 2562, loss: 0.2729, current_lr: 0.000666
step 2563, loss: 0.2469, current_lr: 0.000666
step 2564, loss: 0.2773, current_lr: 0.000666
step 2565, loss: 0.2653, current_lr: 0.000666
step 2566, loss: 0.2460, current_lr: 0.000666
step 2567, loss: 0.3153, current_lr: 0.000666
step 2568, loss: 0.2504, current_lr: 0.000666
step 2569, loss: 0.2387, current_lr: 0.000667
step 2570, loss: 0.2514, current_lr: 0.000667
step 2571, loss: 0.2618, current_lr: 0.000667
step 2572, loss: 0.2479, current_lr: 0.000667
step 2573, loss: 0.2407, current_lr: 0.000667
step 2574, loss: 0.2710, current_lr: 0.000667
step 2575, loss: 0.2860, current_lr: 0.000668
step 2576, loss: 0.2437, current_lr: 0.000668
step 2577, loss: 0.2583, current_lr: 0.000668
step 2578, loss: 0.2674, current_lr: 0.000668
step 2579, loss: 0.2689, current_lr: 0.000668
step 2580, loss: 0.2564, current_lr: 0.000668
step 2581, loss: 0.2653, current_lr: 0.000668
step 2582, loss: 0.2361, current_lr: 0.000669
step 2583, loss: 0.2619, current_lr: 0.000669
step 2584, loss: 0.2686, current_lr: 0.000669
step 2585, loss: 0.2999, current_lr: 0.000669
step 2586, loss: 0.2748, current_lr: 0.000669
step 2587, loss: 0.2669, current_lr: 0.000669
step 2588, loss: 0.2423, current_lr: 0.000669
step 2589, loss: 0.2630, current_lr: 0.000670
step 2590, loss: 0.2853, current_lr: 0.000670
step 2591, loss: 0.2945, current_lr: 0.000670
step 2592, loss: 0.2384, current_lr: 0.000670
step 2593, loss: 0.2531, current_lr: 0.000670
step 2594, loss: 0.2431, current_lr: 0.000670
step 2595, loss: 0.2734, current_lr: 0.000671
step 2596, loss: 0.2404, current_lr: 0.000671
step 2597, loss: 0.3044, current_lr: 0.000671
step 2598, loss: 0.2903, current_lr: 0.000671
step 2599, loss: 0.2522, current_lr: 0.000671
step 2600, loss: 0.2516, current_lr: 0.000671
step 2601, loss: 0.2604, current_lr: 0.000671
step 2602, loss: 0.2637, current_lr: 0.000672
step 2603, loss: 0.2743, current_lr: 0.000672
step 2604, loss: 0.2711, current_lr: 0.000672
step 2605, loss: 0.2477, current_lr: 0.000672
step 2606, loss: 0.2367, current_lr: 0.000672
step 2607, loss: 0.2511, current_lr: 0.000672
step 2608, loss: 0.2516, current_lr: 0.000672
step 2609, loss: 0.2153, current_lr: 0.000672
step 2610, loss: 0.2396, current_lr: 0.000673
step 2611, loss: 0.2584, current_lr: 0.000673
step 2612, loss: 0.2200, current_lr: 0.000673
step 2613, loss: 0.2262, current_lr: 0.000673
step 2614, loss: 0.2933, current_lr: 0.000673
step 2615, loss: 0.2577, current_lr: 0.000673
step 2616, loss: 0.2739, current_lr: 0.000673
step 2617, loss: 0.2508, current_lr: 0.000674
step 2618, loss: 0.2698, current_lr: 0.000674
step 2619, loss: 0.2685, current_lr: 0.000674
step 2620, loss: 0.2564, current_lr: 0.000674
step 2621, loss: 0.2527, current_lr: 0.000674
step 2622, loss: 0.2285, current_lr: 0.000674
step 2623, loss: 0.2196, current_lr: 0.000674
step 2624, loss: 0.2090, current_lr: 0.000675
Saved best model with loss: 0.2090 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2090.pt
step 2625, loss: 0.2574, current_lr: 0.000675
step 2626, loss: 0.2378, current_lr: 0.000675
step 2627, loss: 0.2319, current_lr: 0.000675
step 2628, loss: 0.2586, current_lr: 0.000675
step 2629, loss: 0.2571, current_lr: 0.000675
step 2630, loss: 0.2311, current_lr: 0.000675
step 2631, loss: 0.2333, current_lr: 0.000675
step 2632, loss: 0.2229, current_lr: 0.000676
step 2633, loss: 0.2360, current_lr: 0.000676
step 2634, loss: 0.2594, current_lr: 0.000676
step 2635, loss: 0.2619, current_lr: 0.000676
step 2636, loss: 0.2486, current_lr: 0.000676
step 2637, loss: 0.2700, current_lr: 0.000676
step 2638, loss: 0.2813, current_lr: 0.000676
step 2639, loss: 0.2542, current_lr: 0.000677
step 2640, loss: 0.2639, current_lr: 0.000677
step 2641, loss: 0.2532, current_lr: 0.000677
step 2642, loss: 0.3459, current_lr: 0.000677
step 2643, loss: 0.2435, current_lr: 0.000677
step 2644, loss: 0.2340, current_lr: 0.000677
step 2645, loss: 0.2477, current_lr: 0.000677
step 2646, loss: 0.2322, current_lr: 0.000677
step 2647, loss: 0.2474, current_lr: 0.000678
step 2648, loss: 0.2216, current_lr: 0.000678
step 2649, loss: 0.2580, current_lr: 0.000678
step 2650, loss: 0.2397, current_lr: 0.000678
step 2651, loss: 0.2356, current_lr: 0.000678
step 2652, loss: 0.2276, current_lr: 0.000678
step 2653, loss: 0.2241, current_lr: 0.000678
step 2654, loss: 0.3009, current_lr: 0.000678
step 2655, loss: 0.2587, current_lr: 0.000679
step 2656, loss: 0.2178, current_lr: 0.000679
step 2657, loss: 0.2363, current_lr: 0.000679
step 2658, loss: 0.2332, current_lr: 0.000679
step 2659, loss: 0.2192, current_lr: 0.000679
step 2660, loss: 0.2576, current_lr: 0.000679
step 2661, loss: 0.2647, current_lr: 0.000679
step 2662, loss: 0.2394, current_lr: 0.000679
step 2663, loss: 0.2451, current_lr: 0.000680
step 2664, loss: 0.2289, current_lr: 0.000680
step 2665, loss: 0.2081, current_lr: 0.000680
Saved best model with loss: 0.2081 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2081.pt
step 2666, loss: 0.3139, current_lr: 0.000680
step 2667, loss: 0.2735, current_lr: 0.000680
step 2668, loss: 0.2058, current_lr: 0.000680
Saved best model with loss: 0.2058 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2058.pt
step 2669, loss: 0.2150, current_lr: 0.000680
step 2670, loss: 0.2595, current_lr: 0.000680
step 2671, loss: 0.2689, current_lr: 0.000680
step 2672, loss: 0.2682, current_lr: 0.000681
step 2673, loss: 0.2051, current_lr: 0.000681
Saved best model with loss: 0.2051 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.2051.pt
step 2674, loss: 0.2143, current_lr: 0.000681
step 2675, loss: 0.2700, current_lr: 0.000681
step 2676, loss: 0.2593, current_lr: 0.000681
step 2677, loss: 0.2081, current_lr: 0.000681
step 2678, loss: 0.2100, current_lr: 0.000681
step 2679, loss: 0.2344, current_lr: 0.000681
step 2680, loss: 0.2065, current_lr: 0.000682
step 2681, loss: 0.2207, current_lr: 0.000682
step 2682, loss: 0.2171, current_lr: 0.000682
step 2683, loss: 0.2377, current_lr: 0.000682
step 2684, loss: 0.2418, current_lr: 0.000682
step 2685, loss: 0.2067, current_lr: 0.000682
step 2686, loss: 0.2078, current_lr: 0.000682
step 2687, loss: 0.2491, current_lr: 0.000682
step 2688, loss: 0.2375, current_lr: 0.000682
step 2689, loss: 0.2139, current_lr: 0.000683
step 2690, loss: 0.1998, current_lr: 0.000683
Saved best model with loss: 0.1998 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1998.pt
step 2691, loss: 0.2111, current_lr: 0.000683
step 2692, loss: 0.2584, current_lr: 0.000683
step 2693, loss: 0.2341, current_lr: 0.000683
step 2694, loss: 0.1952, current_lr: 0.000683
Saved best model with loss: 0.1952 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1952.pt
step 2695, loss: 0.2144, current_lr: 0.000683
step 2696, loss: 0.2625, current_lr: 0.000683
step 2697, loss: 0.2064, current_lr: 0.000683
step 2698, loss: 0.2161, current_lr: 0.000684
step 2699, loss: 0.1858, current_lr: 0.000684
Saved best model with loss: 0.1858 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1858.pt
step 2700, loss: 0.2124, current_lr: 0.000684
step 2701, loss: 0.1989, current_lr: 0.000684
step 2702, loss: 0.2059, current_lr: 0.000684
step 2703, loss: 0.2595, current_lr: 0.000684
step 2704, loss: 0.2129, current_lr: 0.000684
step 2705, loss: 0.2754, current_lr: 0.000684
step 2706, loss: 0.2373, current_lr: 0.000684
step 2707, loss: 0.2035, current_lr: 0.000685
step 2708, loss: 0.2418, current_lr: 0.000685
step 2709, loss: 0.2224, current_lr: 0.000685
step 2710, loss: 0.2115, current_lr: 0.000685
step 2711, loss: 0.2098, current_lr: 0.000685
step 2712, loss: 0.2028, current_lr: 0.000685
step 2713, loss: 0.2125, current_lr: 0.000685
step 2714, loss: 0.1812, current_lr: 0.000685
Saved best model with loss: 0.1812 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1812.pt
step 2715, loss: 0.2150, current_lr: 0.000685
step 2716, loss: 0.2178, current_lr: 0.000685
step 2717, loss: 0.2119, current_lr: 0.000686
step 2718, loss: 0.2406, current_lr: 0.000686
step 2719, loss: 0.2157, current_lr: 0.000686
step 2720, loss: 0.2201, current_lr: 0.000686
step 2721, loss: 0.2477, current_lr: 0.000686
step 2722, loss: 0.2228, current_lr: 0.000686
step 2723, loss: 0.2162, current_lr: 0.000686
step 2724, loss: 0.2208, current_lr: 0.000686
step 2725, loss: 0.2308, current_lr: 0.000686
step 2726, loss: 0.2137, current_lr: 0.000686
step 2727, loss: 0.1968, current_lr: 0.000687
step 2728, loss: 0.2239, current_lr: 0.000687
step 2729, loss: 0.2350, current_lr: 0.000687
step 2730, loss: 0.2323, current_lr: 0.000687
step 2731, loss: 0.2126, current_lr: 0.000687
step 2732, loss: 0.2311, current_lr: 0.000687
step 2733, loss: 0.1998, current_lr: 0.000687
step 2734, loss: 0.2035, current_lr: 0.000687
step 2735, loss: 0.2268, current_lr: 0.000687
step 2736, loss: 0.2299, current_lr: 0.000687
step 2737, loss: 0.2100, current_lr: 0.000688
step 2738, loss: 0.2301, current_lr: 0.000688
step 2739, loss: 0.2073, current_lr: 0.000688
step 2740, loss: 0.1908, current_lr: 0.000688
step 2741, loss: 0.2558, current_lr: 0.000688
step 2742, loss: 0.2365, current_lr: 0.000688
step 2743, loss: 0.1980, current_lr: 0.000688
step 2744, loss: 0.2053, current_lr: 0.000688
step 2745, loss: 0.2120, current_lr: 0.000688
step 2746, loss: 0.2138, current_lr: 0.000688
step 2747, loss: 0.2300, current_lr: 0.000688
step 2748, loss: 0.2364, current_lr: 0.000689
step 2749, loss: 0.2052, current_lr: 0.000689
step 2750, loss: 0.2278, current_lr: 0.000689
step 2751, loss: 0.2571, current_lr: 0.000689
step 2752, loss: 0.2302, current_lr: 0.000689
step 2753, loss: 0.2223, current_lr: 0.000689
step 2754, loss: 0.2004, current_lr: 0.000689
step 2755, loss: 0.2015, current_lr: 0.000689
step 2756, loss: 0.2105, current_lr: 0.000689
step 2757, loss: 0.2244, current_lr: 0.000689
step 2758, loss: 0.2133, current_lr: 0.000689
step 2759, loss: 0.1914, current_lr: 0.000690
step 2760, loss: 0.2328, current_lr: 0.000690
step 2761, loss: 0.1750, current_lr: 0.000690
Saved best model with loss: 0.1750 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1750.pt
step 2762, loss: 0.1996, current_lr: 0.000690
step 2763, loss: 0.2208, current_lr: 0.000690
step 2764, loss: 0.2197, current_lr: 0.000690
step 2765, loss: 0.2121, current_lr: 0.000690
step 2766, loss: 0.1863, current_lr: 0.000690
step 2767, loss: 0.1873, current_lr: 0.000690
step 2768, loss: 0.2170, current_lr: 0.000690
step 2769, loss: 0.2238, current_lr: 0.000690
step 2770, loss: 0.2134, current_lr: 0.000690
step 2771, loss: 0.2026, current_lr: 0.000691
step 2772, loss: 0.2310, current_lr: 0.000691
step 2773, loss: 0.2160, current_lr: 0.000691
step 2774, loss: 0.2214, current_lr: 0.000691
step 2775, loss: 0.1844, current_lr: 0.000691
step 2776, loss: 0.1907, current_lr: 0.000691
step 2777, loss: 0.1862, current_lr: 0.000691
step 2778, loss: 0.1814, current_lr: 0.000691
step 2779, loss: 0.1933, current_lr: 0.000691
step 2780, loss: 0.1587, current_lr: 0.000691
Saved best model with loss: 0.1587 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1587.pt
step 2781, loss: 0.2486, current_lr: 0.000691
step 2782, loss: 0.2413, current_lr: 0.000691
step 2783, loss: 0.2476, current_lr: 0.000692
step 2784, loss: 0.1837, current_lr: 0.000692
step 2785, loss: 0.1989, current_lr: 0.000692
step 2786, loss: 0.2409, current_lr: 0.000692
step 2787, loss: 0.2064, current_lr: 0.000692
step 2788, loss: 0.1703, current_lr: 0.000692
step 2789, loss: 0.1786, current_lr: 0.000692
step 2790, loss: 0.1734, current_lr: 0.000692
step 2791, loss: 0.2099, current_lr: 0.000692
step 2792, loss: 0.2260, current_lr: 0.000692
step 2793, loss: 0.2060, current_lr: 0.000692
step 2794, loss: 0.1887, current_lr: 0.000692
step 2795, loss: 0.1809, current_lr: 0.000692
step 2796, loss: 0.2003, current_lr: 0.000693
step 2797, loss: 0.1772, current_lr: 0.000693
step 2798, loss: 0.1668, current_lr: 0.000693
step 2799, loss: 0.1777, current_lr: 0.000693
step 2800, loss: 0.2060, current_lr: 0.000693
step 2801, loss: 0.1878, current_lr: 0.000693
step 2802, loss: 0.2392, current_lr: 0.000693
step 2803, loss: 0.2060, current_lr: 0.000693
step 2804, loss: 0.1784, current_lr: 0.000693
step 2805, loss: 0.2001, current_lr: 0.000693
step 2806, loss: 0.2017, current_lr: 0.000693
step 2807, loss: 0.2549, current_lr: 0.000693
step 2808, loss: 0.1743, current_lr: 0.000693
step 2809, loss: 0.1756, current_lr: 0.000693
step 2810, loss: 0.1916, current_lr: 0.000694
step 2811, loss: 0.1740, current_lr: 0.000694
step 2812, loss: 0.1779, current_lr: 0.000694
step 2813, loss: 0.1904, current_lr: 0.000694
step 2814, loss: 0.1839, current_lr: 0.000694
step 2815, loss: 0.1547, current_lr: 0.000694
Saved best model with loss: 0.1547 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1547.pt
step 2816, loss: 0.2188, current_lr: 0.000694
step 2817, loss: 0.2220, current_lr: 0.000694
step 2818, loss: 0.1840, current_lr: 0.000694
step 2819, loss: 0.1820, current_lr: 0.000694
step 2820, loss: 0.2374, current_lr: 0.000694
step 2821, loss: 0.1948, current_lr: 0.000694
step 2822, loss: 0.1908, current_lr: 0.000694
step 2823, loss: 0.1883, current_lr: 0.000694
step 2824, loss: 0.2111, current_lr: 0.000694
step 2825, loss: 0.2008, current_lr: 0.000694
step 2826, loss: 0.2053, current_lr: 0.000695
step 2827, loss: 0.1873, current_lr: 0.000695
step 2828, loss: 0.1960, current_lr: 0.000695
step 2829, loss: 0.2219, current_lr: 0.000695
step 2830, loss: 0.1804, current_lr: 0.000695
step 2831, loss: 0.2339, current_lr: 0.000695
step 2832, loss: 0.1914, current_lr: 0.000695
step 2833, loss: 0.1710, current_lr: 0.000695
step 2834, loss: 0.1802, current_lr: 0.000695
step 2835, loss: 0.2182, current_lr: 0.000695
step 2836, loss: 0.2113, current_lr: 0.000695
step 2837, loss: 0.2056, current_lr: 0.000695
step 2838, loss: 0.1844, current_lr: 0.000695
step 2839, loss: 0.2052, current_lr: 0.000695
step 2840, loss: 0.2232, current_lr: 0.000695
step 2841, loss: 0.1900, current_lr: 0.000695
step 2842, loss: 0.1893, current_lr: 0.000696
step 2843, loss: 0.1799, current_lr: 0.000696
step 2844, loss: 0.2017, current_lr: 0.000696
step 2845, loss: 0.1932, current_lr: 0.000696
step 2846, loss: 0.1800, current_lr: 0.000696
step 2847, loss: 0.1793, current_lr: 0.000696
step 2848, loss: 0.1670, current_lr: 0.000696
step 2849, loss: 0.1962, current_lr: 0.000696
step 2850, loss: 0.2119, current_lr: 0.000696
step 2851, loss: 0.1606, current_lr: 0.000696
step 2852, loss: 0.2247, current_lr: 0.000696
step 2853, loss: 0.1664, current_lr: 0.000696
step 2854, loss: 0.1673, current_lr: 0.000696
step 2855, loss: 0.1684, current_lr: 0.000696
step 2856, loss: 0.2005, current_lr: 0.000696
step 2857, loss: 0.1825, current_lr: 0.000696
step 2858, loss: 0.1540, current_lr: 0.000696
Saved best model with loss: 0.1540 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1540.pt
step 2859, loss: 0.1663, current_lr: 0.000696
step 2860, loss: 0.1702, current_lr: 0.000696
step 2861, loss: 0.1872, current_lr: 0.000697
step 2862, loss: 0.1905, current_lr: 0.000697
step 2863, loss: 0.1770, current_lr: 0.000697
step 2864, loss: 0.1693, current_lr: 0.000697
step 2865, loss: 0.1688, current_lr: 0.000697
step 2866, loss: 0.1703, current_lr: 0.000697
step 2867, loss: 0.1846, current_lr: 0.000697
step 2868, loss: 0.2343, current_lr: 0.000697
step 2869, loss: 0.2131, current_lr: 0.000697
step 2870, loss: 0.1900, current_lr: 0.000697
step 2871, loss: 0.1902, current_lr: 0.000697
step 2872, loss: 0.1954, current_lr: 0.000697
step 2873, loss: 0.2051, current_lr: 0.000697
step 2874, loss: 0.1586, current_lr: 0.000697
step 2875, loss: 0.1610, current_lr: 0.000697
step 2876, loss: 0.1672, current_lr: 0.000697
step 2877, loss: 0.1701, current_lr: 0.000697
step 2878, loss: 0.1706, current_lr: 0.000697
step 2879, loss: 0.1625, current_lr: 0.000697
step 2880, loss: 0.1817, current_lr: 0.000697
step 2881, loss: 0.1671, current_lr: 0.000697
step 2882, loss: 0.1350, current_lr: 0.000698
Saved best model with loss: 0.1350 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1350.pt
step 2883, loss: 0.1897, current_lr: 0.000698
step 2884, loss: 0.1690, current_lr: 0.000698
step 2885, loss: 0.1504, current_lr: 0.000698
step 2886, loss: 0.2046, current_lr: 0.000698
step 2887, loss: 0.2092, current_lr: 0.000698
step 2888, loss: 0.1661, current_lr: 0.000698
step 2889, loss: 0.1725, current_lr: 0.000698
step 2890, loss: 0.1856, current_lr: 0.000698
step 2891, loss: 0.2078, current_lr: 0.000698
step 2892, loss: 0.1841, current_lr: 0.000698
step 2893, loss: 0.2016, current_lr: 0.000698
step 2894, loss: 0.1960, current_lr: 0.000698
step 2895, loss: 0.1520, current_lr: 0.000698
step 2896, loss: 0.2144, current_lr: 0.000698
step 2897, loss: 0.2116, current_lr: 0.000698
step 2898, loss: 0.2307, current_lr: 0.000698
step 2899, loss: 0.1394, current_lr: 0.000698
step 2900, loss: 0.1955, current_lr: 0.000698
step 2901, loss: 0.1805, current_lr: 0.000698
step 2902, loss: 0.1759, current_lr: 0.000698
step 2903, loss: 0.2038, current_lr: 0.000698
step 2904, loss: 0.2007, current_lr: 0.000698
step 2905, loss: 0.1647, current_lr: 0.000698
step 2906, loss: 0.1672, current_lr: 0.000698
step 2907, loss: 0.1881, current_lr: 0.000698
step 2908, loss: 0.1634, current_lr: 0.000699
step 2909, loss: 0.1937, current_lr: 0.000699
step 2910, loss: 0.1777, current_lr: 0.000699
step 2911, loss: 0.1854, current_lr: 0.000699
step 2912, loss: 0.1778, current_lr: 0.000699
step 2913, loss: 0.1816, current_lr: 0.000699
step 2914, loss: 0.1762, current_lr: 0.000699
step 2915, loss: 0.1855, current_lr: 0.000699
step 2916, loss: 0.2094, current_lr: 0.000699
step 2917, loss: 0.1985, current_lr: 0.000699
step 2918, loss: 0.1759, current_lr: 0.000699
step 2919, loss: 0.1850, current_lr: 0.000699
step 2920, loss: 0.1849, current_lr: 0.000699
step 2921, loss: 0.1831, current_lr: 0.000699
step 2922, loss: 0.1636, current_lr: 0.000699
step 2923, loss: 0.2007, current_lr: 0.000699
step 2924, loss: 0.1688, current_lr: 0.000699
step 2925, loss: 0.1315, current_lr: 0.000699
Saved best model with loss: 0.1315 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1315.pt
step 2926, loss: 0.1656, current_lr: 0.000699
step 2927, loss: 0.2135, current_lr: 0.000699
step 2928, loss: 0.1660, current_lr: 0.000699
step 2929, loss: 0.1820, current_lr: 0.000699
step 2930, loss: 0.1724, current_lr: 0.000699
step 2931, loss: 0.1809, current_lr: 0.000699
step 2932, loss: 0.1767, current_lr: 0.000699
step 2933, loss: 0.1586, current_lr: 0.000699
step 2934, loss: 0.1457, current_lr: 0.000699
step 2935, loss: 0.1620, current_lr: 0.000699
step 2936, loss: 0.1596, current_lr: 0.000699
step 2937, loss: 0.1760, current_lr: 0.000699
step 2938, loss: 0.2073, current_lr: 0.000699
step 2939, loss: 0.1803, current_lr: 0.000699
step 2940, loss: 0.1724, current_lr: 0.000699
step 2941, loss: 0.1640, current_lr: 0.000699
step 2942, loss: 0.1578, current_lr: 0.000699
step 2943, loss: 0.1726, current_lr: 0.000699
step 2944, loss: 0.2167, current_lr: 0.000699
step 2945, loss: 0.1644, current_lr: 0.000699
step 2946, loss: 0.1593, current_lr: 0.000700
step 2947, loss: 0.2055, current_lr: 0.000700
step 2948, loss: 0.1892, current_lr: 0.000700
step 2949, loss: 0.1780, current_lr: 0.000700
step 2950, loss: 0.1923, current_lr: 0.000700
step 2951, loss: 0.2088, current_lr: 0.000700
step 2952, loss: 0.1985, current_lr: 0.000700
step 2953, loss: 0.1438, current_lr: 0.000700
step 2954, loss: 0.1550, current_lr: 0.000700
step 2955, loss: 0.1671, current_lr: 0.000700
step 2956, loss: 0.1717, current_lr: 0.000700
step 2957, loss: 0.1822, current_lr: 0.000700
step 2958, loss: 0.1811, current_lr: 0.000700
step 2959, loss: 0.1585, current_lr: 0.000700
step 2960, loss: 0.1854, current_lr: 0.000700
step 2961, loss: 0.1641, current_lr: 0.000700
step 2962, loss: 0.1567, current_lr: 0.000700
step 2963, loss: 0.1584, current_lr: 0.000700
step 2964, loss: 0.1635, current_lr: 0.000700
step 2965, loss: 0.1638, current_lr: 0.000700
step 2966, loss: 0.1847, current_lr: 0.000700
step 2967, loss: 0.1731, current_lr: 0.000700
step 2968, loss: 0.1712, current_lr: 0.000700
step 2969, loss: 0.1449, current_lr: 0.000700
step 2970, loss: 0.1741, current_lr: 0.000700
step 2971, loss: 0.1690, current_lr: 0.000700
step 2972, loss: 0.2029, current_lr: 0.000700
step 2973, loss: 0.1645, current_lr: 0.000700
step 2974, loss: 0.1905, current_lr: 0.000700
step 2975, loss: 0.1714, current_lr: 0.000700
step 2976, loss: 0.1460, current_lr: 0.000700
step 2977, loss: 0.1398, current_lr: 0.000700
step 2978, loss: 0.1497, current_lr: 0.000700
step 2979, loss: 0.1722, current_lr: 0.000700
step 2980, loss: 0.1514, current_lr: 0.000700
step 2981, loss: 0.1732, current_lr: 0.000700
step 2982, loss: 0.1760, current_lr: 0.000700
step 2983, loss: 0.1621, current_lr: 0.000700
step 2984, loss: 0.1736, current_lr: 0.000700
step 2985, loss: 0.1765, current_lr: 0.000700
step 2986, loss: 0.2012, current_lr: 0.000700
step 2987, loss: 0.1874, current_lr: 0.000700
step 2988, loss: 0.1782, current_lr: 0.000700
step 2989, loss: 0.1518, current_lr: 0.000700
step 2990, loss: 0.1759, current_lr: 0.000700
step 2991, loss: 0.1686, current_lr: 0.000700
step 2992, loss: 0.1775, current_lr: 0.000700
step 2993, loss: 0.1802, current_lr: 0.000700
step 2994, loss: 0.1623, current_lr: 0.000700
step 2995, loss: 0.1729, current_lr: 0.000700
step 2996, loss: 0.1824, current_lr: 0.000700
step 2997, loss: 0.1511, current_lr: 0.000700
step 2998, loss: 0.1881, current_lr: 0.000700
step 2999, loss: 0.1444, current_lr: 0.000700
step 3000, loss: 0.1565, current_lr: 0.000700
step 3001, loss: 0.1854, current_lr: 0.000700
step 3002, loss: 0.1616, current_lr: 0.000700
step 3003, loss: 0.1447, current_lr: 0.000700
step 3004, loss: 0.1483, current_lr: 0.000700
step 3005, loss: 0.1583, current_lr: 0.000700
step 3006, loss: 0.1593, current_lr: 0.000700
step 3007, loss: 0.1446, current_lr: 0.000700
step 3008, loss: 0.1427, current_lr: 0.000700
step 3009, loss: 0.1642, current_lr: 0.000700
step 3010, loss: 0.1595, current_lr: 0.000700
step 3011, loss: 0.1566, current_lr: 0.000700
step 3012, loss: 0.1556, current_lr: 0.000700
step 3013, loss: 0.1609, current_lr: 0.000700
step 3014, loss: 0.1745, current_lr: 0.000700
step 3015, loss: 0.1674, current_lr: 0.000700
step 3016, loss: 0.1566, current_lr: 0.000700
step 3017, loss: 0.2044, current_lr: 0.000700
step 3018, loss: 0.1788, current_lr: 0.000700
step 3019, loss: 0.1569, current_lr: 0.000700
step 3020, loss: 0.1574, current_lr: 0.000700
step 3021, loss: 0.1507, current_lr: 0.000700
step 3022, loss: 0.1763, current_lr: 0.000700
step 3023, loss: 0.1573, current_lr: 0.000700
step 3024, loss: 0.1693, current_lr: 0.000700
step 3025, loss: 0.1355, current_lr: 0.000700
step 3026, loss: 0.1567, current_lr: 0.000700
step 3027, loss: 0.1602, current_lr: 0.000700
step 3028, loss: 0.1743, current_lr: 0.000700
step 3029, loss: 0.1444, current_lr: 0.000700
step 3030, loss: 0.1520, current_lr: 0.000700
step 3031, loss: 0.1559, current_lr: 0.000700
step 3032, loss: 0.1593, current_lr: 0.000700
step 3033, loss: 0.1781, current_lr: 0.000700
step 3034, loss: 0.1410, current_lr: 0.000700
step 3035, loss: 0.1777, current_lr: 0.000700
step 3036, loss: 0.1578, current_lr: 0.000700
step 3037, loss: 0.1773, current_lr: 0.000700
step 3038, loss: 0.1633, current_lr: 0.000700
step 3039, loss: 0.1600, current_lr: 0.000700
step 3040, loss: 0.1391, current_lr: 0.000700
step 3041, loss: 0.1404, current_lr: 0.000700
step 3042, loss: 0.1697, current_lr: 0.000700
step 3043, loss: 0.1419, current_lr: 0.000700
step 3044, loss: 0.1416, current_lr: 0.000700
step 3045, loss: 0.1520, current_lr: 0.000700
step 3046, loss: 0.1333, current_lr: 0.000700
step 3047, loss: 0.1403, current_lr: 0.000700
step 3048, loss: 0.1824, current_lr: 0.000700
step 3049, loss: 0.1475, current_lr: 0.000700
step 3050, loss: 0.1436, current_lr: 0.000700
step 3051, loss: 0.1726, current_lr: 0.000700
step 3052, loss: 0.1585, current_lr: 0.000700
step 3053, loss: 0.1342, current_lr: 0.000700
step 3054, loss: 0.1292, current_lr: 0.000700
Saved best model with loss: 0.1292 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1292.pt
step 3055, loss: 0.1555, current_lr: 0.000700
step 3056, loss: 0.1599, current_lr: 0.000700
step 3057, loss: 0.1339, current_lr: 0.000700
step 3058, loss: 0.1606, current_lr: 0.000700
step 3059, loss: 0.1678, current_lr: 0.000700
step 3060, loss: 0.1097, current_lr: 0.000700
Saved best model with loss: 0.1097 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1097.pt
step 3061, loss: 0.1576, current_lr: 0.000700
step 3062, loss: 0.1608, current_lr: 0.000700
step 3063, loss: 0.1823, current_lr: 0.000700
step 3064, loss: 0.1553, current_lr: 0.000700
step 3065, loss: 0.1339, current_lr: 0.000700
step 3066, loss: 0.1770, current_lr: 0.000700
step 3067, loss: 0.1803, current_lr: 0.000700
step 3068, loss: 0.1507, current_lr: 0.000700
step 3069, loss: 0.1632, current_lr: 0.000700
step 3070, loss: 0.1393, current_lr: 0.000700
step 3071, loss: 0.1653, current_lr: 0.000700
step 3072, loss: 0.1386, current_lr: 0.000700
step 3073, loss: 0.1624, current_lr: 0.000700
step 3074, loss: 0.1600, current_lr: 0.000700
step 3075, loss: 0.1639, current_lr: 0.000700
step 3076, loss: 0.2056, current_lr: 0.000700
step 3077, loss: 0.1582, current_lr: 0.000700
step 3078, loss: 0.1341, current_lr: 0.000700
step 3079, loss: 0.1582, current_lr: 0.000700
step 3080, loss: 0.1832, current_lr: 0.000700
step 3081, loss: 0.1728, current_lr: 0.000700
step 3082, loss: 0.1816, current_lr: 0.000700
step 3083, loss: 0.1615, current_lr: 0.000700
step 3084, loss: 0.1834, current_lr: 0.000700
step 3085, loss: 0.1756, current_lr: 0.000700
step 3086, loss: 0.1897, current_lr: 0.000700
step 3087, loss: 0.1664, current_lr: 0.000700
step 3088, loss: 0.1443, current_lr: 0.000700
step 3089, loss: 0.1704, current_lr: 0.000700
step 3090, loss: 0.1679, current_lr: 0.000700
step 3091, loss: 0.1362, current_lr: 0.000700
step 3092, loss: 0.1606, current_lr: 0.000700
step 3093, loss: 0.1346, current_lr: 0.000700
step 3094, loss: 0.1577, current_lr: 0.000700
step 3095, loss: 0.1499, current_lr: 0.000700
step 3096, loss: 0.1511, current_lr: 0.000700
step 3097, loss: 0.1628, current_lr: 0.000700
step 3098, loss: 0.1473, current_lr: 0.000700
step 3099, loss: 0.1570, current_lr: 0.000700
step 3100, loss: 0.1280, current_lr: 0.000700
step 3101, loss: 0.1521, current_lr: 0.000700
step 3102, loss: 0.1594, current_lr: 0.000700
step 3103, loss: 0.1648, current_lr: 0.000700
step 3104, loss: 0.1485, current_lr: 0.000700
step 3105, loss: 0.1725, current_lr: 0.000700
step 3106, loss: 0.1591, current_lr: 0.000700
step 3107, loss: 0.1469, current_lr: 0.000700
step 3108, loss: 0.1700, current_lr: 0.000700
step 3109, loss: 0.1407, current_lr: 0.000700
step 3110, loss: 0.1399, current_lr: 0.000700
step 3111, loss: 0.1388, current_lr: 0.000700
step 3112, loss: 0.1399, current_lr: 0.000700
step 3113, loss: 0.1574, current_lr: 0.000700
step 3114, loss: 0.1330, current_lr: 0.000700
step 3115, loss: 0.1586, current_lr: 0.000700
step 3116, loss: 0.1743, current_lr: 0.000700
step 3117, loss: 0.1478, current_lr: 0.000700
step 3118, loss: 0.1398, current_lr: 0.000699
step 3119, loss: 0.1548, current_lr: 0.000699
step 3120, loss: 0.1864, current_lr: 0.000699
step 3121, loss: 0.1275, current_lr: 0.000699
step 3122, loss: 0.1519, current_lr: 0.000699
step 3123, loss: 0.1445, current_lr: 0.000699
step 3124, loss: 0.1492, current_lr: 0.000699
step 3125, loss: 0.1385, current_lr: 0.000699
step 3126, loss: 0.1350, current_lr: 0.000699
step 3127, loss: 0.1303, current_lr: 0.000699
step 3128, loss: 0.1140, current_lr: 0.000699
step 3129, loss: 0.1318, current_lr: 0.000699
step 3130, loss: 0.1438, current_lr: 0.000699
step 3131, loss: 0.1308, current_lr: 0.000699
step 3132, loss: 0.1639, current_lr: 0.000699
step 3133, loss: 0.1615, current_lr: 0.000699
step 3134, loss: 0.1539, current_lr: 0.000699
step 3135, loss: 0.1369, current_lr: 0.000699
step 3136, loss: 0.1270, current_lr: 0.000699
step 3137, loss: 0.1685, current_lr: 0.000699
step 3138, loss: 0.1357, current_lr: 0.000699
step 3139, loss: 0.1623, current_lr: 0.000699
step 3140, loss: 0.1424, current_lr: 0.000699
step 3141, loss: 0.1582, current_lr: 0.000699
step 3142, loss: 0.1443, current_lr: 0.000699
step 3143, loss: 0.1419, current_lr: 0.000699
step 3144, loss: 0.1498, current_lr: 0.000699
step 3145, loss: 0.1611, current_lr: 0.000699
step 3146, loss: 0.1257, current_lr: 0.000699
step 3147, loss: 0.1313, current_lr: 0.000699
step 3148, loss: 0.1560, current_lr: 0.000699
step 3149, loss: 0.1636, current_lr: 0.000699
step 3150, loss: 0.1369, current_lr: 0.000699
step 3151, loss: 0.1663, current_lr: 0.000699
step 3152, loss: 0.1308, current_lr: 0.000699
step 3153, loss: 0.1481, current_lr: 0.000699
step 3154, loss: 0.1588, current_lr: 0.000699
step 3155, loss: 0.1491, current_lr: 0.000699
step 3156, loss: 0.1309, current_lr: 0.000699
step 3157, loss: 0.1756, current_lr: 0.000699
step 3158, loss: 0.1549, current_lr: 0.000699
step 3159, loss: 0.1186, current_lr: 0.000699
step 3160, loss: 0.1520, current_lr: 0.000699
step 3161, loss: 0.1390, current_lr: 0.000699
step 3162, loss: 0.1399, current_lr: 0.000699
step 3163, loss: 0.1258, current_lr: 0.000699
step 3164, loss: 0.1423, current_lr: 0.000699
step 3165, loss: 0.1735, current_lr: 0.000699
step 3166, loss: 0.1560, current_lr: 0.000699
step 3167, loss: 0.1560, current_lr: 0.000699
step 3168, loss: 0.1189, current_lr: 0.000699
step 3169, loss: 0.1485, current_lr: 0.000699
step 3170, loss: 0.1143, current_lr: 0.000699
step 3171, loss: 0.1182, current_lr: 0.000699
step 3172, loss: 0.1312, current_lr: 0.000699
step 3173, loss: 0.1423, current_lr: 0.000699
step 3174, loss: 0.1439, current_lr: 0.000699
step 3175, loss: 0.1356, current_lr: 0.000699
step 3176, loss: 0.1352, current_lr: 0.000699
step 3177, loss: 0.1173, current_lr: 0.000699
step 3178, loss: 0.1267, current_lr: 0.000699
step 3179, loss: 0.1262, current_lr: 0.000699
step 3180, loss: 0.1918, current_lr: 0.000699
step 3181, loss: 0.1426, current_lr: 0.000699
step 3182, loss: 0.1831, current_lr: 0.000699
step 3183, loss: 0.1405, current_lr: 0.000699
step 3184, loss: 0.1211, current_lr: 0.000699
step 3185, loss: 0.1551, current_lr: 0.000699
step 3186, loss: 0.1779, current_lr: 0.000699
step 3187, loss: 0.1575, current_lr: 0.000699
step 3188, loss: 0.1613, current_lr: 0.000699
step 3189, loss: 0.1418, current_lr: 0.000699
step 3190, loss: 0.1279, current_lr: 0.000699
step 3191, loss: 0.1251, current_lr: 0.000699
step 3192, loss: 0.1429, current_lr: 0.000699
step 3193, loss: 0.1660, current_lr: 0.000699
step 3194, loss: 0.1413, current_lr: 0.000699
step 3195, loss: 0.1232, current_lr: 0.000699
step 3196, loss: 0.1197, current_lr: 0.000699
step 3197, loss: 0.1368, current_lr: 0.000699
step 3198, loss: 0.1605, current_lr: 0.000699
step 3199, loss: 0.1391, current_lr: 0.000699
step 3200, loss: 0.1328, current_lr: 0.000699
step 3201, loss: 0.1833, current_lr: 0.000699
step 3202, loss: 0.1494, current_lr: 0.000699
step 3203, loss: 0.1175, current_lr: 0.000699
step 3204, loss: 0.1519, current_lr: 0.000699
step 3205, loss: 0.1525, current_lr: 0.000698
step 3206, loss: 0.1266, current_lr: 0.000698
step 3207, loss: 0.1123, current_lr: 0.000698
step 3208, loss: 0.1472, current_lr: 0.000698
step 3209, loss: 0.1251, current_lr: 0.000698
step 3210, loss: 0.1432, current_lr: 0.000698
step 3211, loss: 0.1297, current_lr: 0.000698
step 3212, loss: 0.1446, current_lr: 0.000698
step 3213, loss: 0.1224, current_lr: 0.000698
step 3214, loss: 0.1488, current_lr: 0.000698
step 3215, loss: 0.1059, current_lr: 0.000698
Saved best model with loss: 0.1059 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.1059.pt
step 3216, loss: 0.1624, current_lr: 0.000698
step 3217, loss: 0.1170, current_lr: 0.000698
step 3218, loss: 0.1241, current_lr: 0.000698
step 3219, loss: 0.1413, current_lr: 0.000698
step 3220, loss: 0.1650, current_lr: 0.000698
step 3221, loss: 0.1132, current_lr: 0.000698
step 3222, loss: 0.1239, current_lr: 0.000698
step 3223, loss: 0.1390, current_lr: 0.000698
step 3224, loss: 0.1355, current_lr: 0.000698
step 3225, loss: 0.1087, current_lr: 0.000698
step 3226, loss: 0.1447, current_lr: 0.000698
step 3227, loss: 0.1399, current_lr: 0.000698
step 3228, loss: 0.1394, current_lr: 0.000698
step 3229, loss: 0.1244, current_lr: 0.000698
step 3230, loss: 0.1187, current_lr: 0.000698
step 3231, loss: 0.1418, current_lr: 0.000698
step 3232, loss: 0.1432, current_lr: 0.000698
step 3233, loss: 0.1406, current_lr: 0.000698
step 3234, loss: 0.1409, current_lr: 0.000698
step 3235, loss: 0.1180, current_lr: 0.000698
step 3236, loss: 0.1251, current_lr: 0.000698
step 3237, loss: 0.1585, current_lr: 0.000698
step 3238, loss: 0.1370, current_lr: 0.000698
step 3239, loss: 0.1752, current_lr: 0.000698
step 3240, loss: 0.1514, current_lr: 0.000698
step 3241, loss: 0.1425, current_lr: 0.000698
step 3242, loss: 0.1637, current_lr: 0.000698
step 3243, loss: 0.1668, current_lr: 0.000698
step 3244, loss: 0.1439, current_lr: 0.000698
step 3245, loss: 0.1391, current_lr: 0.000698
step 3246, loss: 0.1331, current_lr: 0.000698
step 3247, loss: 0.1680, current_lr: 0.000698
step 3248, loss: 0.1333, current_lr: 0.000698
step 3249, loss: 0.1553, current_lr: 0.000698
step 3250, loss: 0.1489, current_lr: 0.000698
step 3251, loss: 0.1441, current_lr: 0.000698
step 3252, loss: 0.1603, current_lr: 0.000698
step 3253, loss: 0.1500, current_lr: 0.000698
step 3254, loss: 0.1532, current_lr: 0.000698
step 3255, loss: 0.1686, current_lr: 0.000698
step 3256, loss: 0.1302, current_lr: 0.000698
step 3257, loss: 0.1302, current_lr: 0.000698
step 3258, loss: 0.1276, current_lr: 0.000698
step 3259, loss: 0.1484, current_lr: 0.000698
step 3260, loss: 0.1270, current_lr: 0.000698
step 3261, loss: 0.1543, current_lr: 0.000698
step 3262, loss: 0.1322, current_lr: 0.000698
step 3263, loss: 0.1424, current_lr: 0.000698
step 3264, loss: 0.1228, current_lr: 0.000698
step 3265, loss: 0.1188, current_lr: 0.000697
step 3266, loss: 0.1344, current_lr: 0.000697
step 3267, loss: 0.1438, current_lr: 0.000697
step 3268, loss: 0.1186, current_lr: 0.000697
step 3269, loss: 0.1368, current_lr: 0.000697
step 3270, loss: 0.1450, current_lr: 0.000697
step 3271, loss: 0.1187, current_lr: 0.000697
step 3272, loss: 0.1163, current_lr: 0.000697
step 3273, loss: 0.1305, current_lr: 0.000697
step 3274, loss: 0.1246, current_lr: 0.000697
step 3275, loss: 0.1343, current_lr: 0.000697
step 3276, loss: 0.1319, current_lr: 0.000697
step 3277, loss: 0.1112, current_lr: 0.000697
step 3278, loss: 0.1287, current_lr: 0.000697
step 3279, loss: 0.1111, current_lr: 0.000697
step 3280, loss: 0.1231, current_lr: 0.000697
step 3281, loss: 0.1383, current_lr: 0.000697
step 3282, loss: 0.1348, current_lr: 0.000697
step 3283, loss: 0.1173, current_lr: 0.000697
step 3284, loss: 0.1139, current_lr: 0.000697
step 3285, loss: 0.1176, current_lr: 0.000697
step 3286, loss: 0.1220, current_lr: 0.000697
step 3287, loss: 0.1599, current_lr: 0.000697
step 3288, loss: 0.1645, current_lr: 0.000697
step 3289, loss: 0.1316, current_lr: 0.000697
step 3290, loss: 0.1355, current_lr: 0.000697
step 3291, loss: 0.1091, current_lr: 0.000697
step 3292, loss: 0.0998, current_lr: 0.000697
Saved best model with loss: 0.0998 to fine_tuned_gpt2\gpt2_finetuned_best_loss_0.0998.pt
Achieved target loss of 0.09999! Stopping training.
Final loss: 0.09980350732803345

==================================================
Starting text generation...
==================================================
GENERATED TEXT 1:
> The quick brown foxes stand to their ministers:
What, is their oath come to pass?

BAPTISTA:
No
--------------------------------------------------
GENERATED TEXT 2:
> The quick brown foxes, smiling on their knees at the
Fair face of the statue,
Although they frown upon 't.

TR
--------------------------------------------------
GENERATED TEXT 3:
> The quick brown foxes are upon you.

KATHARINA:
Why, master, look not big: I prithee,
--------------------------------------------------
GENERATED TEXT 4:
> The quick brown foxes stand to their charge:
Why, what of a stocking?

KATHARINA:
They serve in
--------------------------------------------------
GENERATED TEXT 5:
> The quick brown fox and he we are,
Shall act as he will stand to us.

PETRUCHIO:
Sir
--------------------------------------------------
